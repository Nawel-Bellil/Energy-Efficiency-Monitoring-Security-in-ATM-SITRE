{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI Surveillance System - Google Colab Version\n",
        "# This combines all  modules into a single notebook-friendly implementation\n"
      ],
      "metadata": {
        "id": "y382cRpuejSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import cv2\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime\n",
        "import argparse\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import ipywidgets as widgets\n",
        "from threading import Thread\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "aCLNdbTPeg5U"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ====================================================================\n",
        "# UTILS MODULE\n",
        "# ======================================================================\n"
      ],
      "metadata": {
        "id": "bC5-51TSerwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_env_file(file_path):\n",
        "    \"\"\"Load environment variables from a file.\"\"\"\n",
        "    config = {}\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line or line.startswith('#'):\n",
        "                    continue\n",
        "                key, value = line.split('=', 1)\n",
        "                config[key] = value\n",
        "        return config\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading config: {e}\")\n",
        "        return {}\n"
      ],
      "metadata": {
        "id": "M3jd8YuSe26Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_coordinates(coord_str):\n",
        "    \"\"\"Parse coordinates from string to tuple.\"\"\"\n",
        "    if not coord_str or coord_str.lower() == 'none':\n",
        "        return None\n",
        "    try:\n",
        "        coords = list(map(int, coord_str.split(',')))\n",
        "        if len(coords) == 2:\n",
        "            return tuple(coords)\n",
        "        elif len(coords) == 4:\n",
        "            return (tuple(coords[:2]), tuple(coords[2:]))\n",
        "        else:\n",
        "            return None\n",
        "    except (ValueError, TypeError):\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "TLJBD7t0e5sA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ensure_directory(directory):\n",
        "    \"\"\"Ensure a directory exists, create if it doesn't.\"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "        print(f\"Created directory: {directory}\")\n",
        "    return directory\n"
      ],
      "metadata": {
        "id": "28LDRkXFe8Ch"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_timestamp(format_str=\"%Y-%m-%d %H:%M:%S\"):\n",
        "    \"\"\"Get current timestamp as string.\"\"\"\n",
        "    return datetime.now().strftime(format_str)\n"
      ],
      "metadata": {
        "id": "6Ynj_UCAfAx5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_file_timestamp():\n",
        "    \"\"\"Get timestamp format suitable for filenames.\"\"\"\n",
        "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n"
      ],
      "metadata": {
        "id": "VBlm1ETQfCeI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def draw_timestamp(frame, timestamp=None):\n",
        "    \"\"\"Draw timestamp on the frame.\"\"\"\n",
        "    if timestamp is None:\n",
        "        timestamp = get_timestamp()\n",
        "    cv2.putText(frame, timestamp, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "    return frame\n"
      ],
      "metadata": {
        "id": "-Z4KxCD8fEUI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def draw_roi(frame, roi_start, roi_end):\n",
        "    \"\"\"Draw ROI rectangle on the frame.\"\"\"\n",
        "    if roi_start is not None and roi_end is not None:\n",
        "        cv2.rectangle(frame, roi_start, roi_end, (0, 255, 0), 2)\n",
        "    return frame\n"
      ],
      "metadata": {
        "id": "bunFMHW8fF3Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_roi(frame, roi_start, roi_end):\n",
        "    \"\"\"Extract the region of interest from frame.\"\"\"\n",
        "    if roi_start is None or roi_end is None:\n",
        "        return frame\n",
        "    x1, y1 = roi_start\n",
        "    x2, y2 = roi_end\n",
        "    h, w = frame.shape[:2]\n",
        "    x1 = max(0, min(x1, w-1))\n",
        "    y1 = max(0, min(y1, h-1))\n",
        "    x2 = max(0, min(x2, w-1))\n",
        "    y2 = max(0, min(y2, h-1))\n",
        "    return frame[y1:y2, x1:x2]\n"
      ],
      "metadata": {
        "id": "t0mpI96EfIKf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def save_frame(frame, directory, prefix=\"detection\"):\n",
        "    \"\"\"Save a frame to disk with timestamp in filename.\"\"\"\n",
        "    timestamp = get_file_timestamp()\n",
        "    filename = f\"{prefix}_{timestamp}.jpg\"\n",
        "    filepath = os.path.join(directory, filename)\n",
        "    ensure_directory(directory)\n",
        "    cv2.imwrite(filepath, frame)\n",
        "    print(f\"Saved: {filepath}\")\n",
        "    return filepath\n"
      ],
      "metadata": {
        "id": "msrKwlpBfKZV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def str_to_bool(value):\n",
        "    \"\"\"Convert string value to boolean.\"\"\"\n",
        "    if isinstance(value, bool):\n",
        "        return value\n",
        "    if value.lower() in ('true', 'yes', '1', 'y'):\n",
        "        return True\n",
        "    elif value.lower() in ('false', 'no', '0', 'n'):\n",
        "        return False\n",
        "    else:\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "Pl-Aj_blfMMI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ====================================================================\n",
        "# LOGGER MODULE\n",
        "# ======================================================================\n"
      ],
      "metadata": {
        "id": "9JUgjNQefN95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Logger:\n",
        "    \"\"\"Simple logging utility.\"\"\"\n",
        "    def __init__(self, log_file=None, output_dir='surveillance_footage'):\n",
        "        self.output_dir = ensure_directory(output_dir)\n",
        "        if log_file is None:\n",
        "            log_file = os.path.join(self.output_dir, 'security_log.txt')\n",
        "        self.log_file = log_file\n",
        "\n",
        "    def log(self, event_type, details=None):\n",
        "        \"\"\"Log an event to file and console.\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        details_str = f\" - {details}\" if details else \"\"\n",
        "        log_entry = f\"[{timestamp}] {event_type}{details_str}\\n\"\n",
        "        print(log_entry, end=\"\")\n",
        "        try:\n",
        "            with open(self.log_file, 'a') as f:\n",
        "                f.write(log_entry)\n",
        "        except Exception as e:\n",
        "            print(f\"Error writing to log file: {e}\")\n"
      ],
      "metadata": {
        "id": "0ueJ6f8efUGA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Global logger"
      ],
      "metadata": {
        "id": "FNEj2l8lfVt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "security_logger = None\n",
        "\n",
        "def init_logger(log_file=None, output_dir='surveillance_footage'):\n",
        "    \"\"\"Initialize the global logger.\"\"\"\n",
        "    global security_logger\n",
        "    security_logger = Logger(log_file, output_dir)\n",
        "    return security_logger\n"
      ],
      "metadata": {
        "id": "Codp_SXgfYEo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def log_event(event_type, details=None):\n",
        "    \"\"\"Log an event using the global logger.\"\"\"\n",
        "    global security_logger\n",
        "    if security_logger is None:\n",
        "        security_logger = Logger()\n",
        "    security_logger.log(event_type, details)\n"
      ],
      "metadata": {
        "id": "EA-7TSH_fbLP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ===================================================================\n",
        "# MOTION DETECTION MODULE\n",
        "# =====================================================================\n"
      ],
      "metadata": {
        "id": "BGb6doNtfc13"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z_tLBPNVfkHk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bg_subtractor = None\n",
        "\n",
        "def init_motion_detector():\n",
        "    \"\"\"Initialize the motion detector.\"\"\"\n",
        "    global bg_subtractor\n",
        "    bg_subtractor = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=16, detectShadows=True)\n",
        "    log_event(\"INIT\", \"Motion detector initialized\")\n",
        "    return bg_subtractor\n"
      ],
      "metadata": {
        "id": "GaGuZh9Dfj4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def detect_motion(frame, roi_start=None, roi_end=None, min_area=5000):\n",
        "    \"\"\"Detect motion in the frame or region of interest.\"\"\"\n",
        "    global bg_subtractor\n",
        "    if bg_subtractor is None:\n",
        "        bg_subtractor = init_motion_detector()\n",
        "\n",
        "    if roi_start is not None and roi_end is not None:\n",
        "        roi_frame = extract_roi(frame, roi_start, roi_end)\n",
        "    else:\n",
        "        roi_frame = frame\n",
        "\n",
        "    fg_mask = bg_subtractor.apply(roi_frame)\n",
        "    _, thresh = cv2.threshold(fg_mask, 25, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
        "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
        "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
        "    thresh = cv2.dilate(thresh, kernel, iterations=2)\n",
        "\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    motion_detected = False\n",
        "    motion_contours = []\n",
        "\n",
        "    for contour in contours:\n",
        "        area = cv2.contourArea(contour)\n",
        "        if area > min_area:\n",
        "            motion_detected = True\n",
        "            motion_contours.append(contour)\n",
        "\n",
        "    return motion_detected, thresh, motion_contours\n"
      ],
      "metadata": {
        "id": "4cACgYk1fmW4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def draw_motion_contours(frame, contours, roi_start=None, roi_end=None):\n",
        "    \"\"\"Draw motion contours on the frame.\"\"\"\n",
        "    if not contours:\n",
        "        return frame\n",
        "\n",
        "    result = frame.copy()\n",
        "\n",
        "    if roi_start is not None:\n",
        "        x_offset, y_offset = roi_start\n",
        "        for contour in contours:\n",
        "            shifted_contour = contour + np.array([x_offset, y_offset])\n",
        "            cv2.drawContours(result, [shifted_contour], -1, (0, 255, 255), 2)\n",
        "    else:\n",
        "        cv2.drawContours(result, contours, -1, (0, 255, 255), 2)\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "DX7pU-knfoN5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ====================================================================\n",
        "# FACE DETECTION MODULE\n",
        "# ======================================================================\n"
      ],
      "metadata": {
        "id": "3HeqgHWpfq7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "face_detector = None\n",
        "face_recognition_loaded = False\n",
        "known_face_encodings = []\n",
        "known_face_names = []\n"
      ],
      "metadata": {
        "id": "WO9deOSXfvPf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def init_face_detector():\n",
        "    \"\"\"Initialize the face detector.\"\"\"\n",
        "    global face_detector\n",
        "    face_cascade_path = os.path.join(cv2.data.haarcascades, 'haarcascade_frontalface_default.xml')\n",
        "    face_detector = cv2.CascadeClassifier(face_cascade_path)\n",
        "\n",
        "    if face_detector.empty():\n",
        "        log_event(\"ERROR\", \"Failed to load face detector\")\n",
        "        return None\n",
        "\n",
        "    log_event(\"INIT\", \"Face detector initialized\")\n",
        "    return face_detector\n"
      ],
      "metadata": {
        "id": "rKFf5KvEfxZf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_face_recognition():\n",
        "    \"\"\"Load face recognition library if available.\"\"\"\n",
        "    global face_recognition_loaded\n",
        "    try:\n",
        "        global face_recognition\n",
        "        import face_recognition\n",
        "        face_recognition_loaded = True\n",
        "        log_event(\"INIT\", \"Face recognition library loaded\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        log_event(\"WARNING\", \"face_recognition library not available. Installing...\")\n",
        "        # Install face_recognition in Colab\n",
        "        os.system(\"pip install face_recognition\")\n",
        "        try:\n",
        "            import face_recognition\n",
        "            face_recognition_loaded = True\n",
        "            log_event(\"INIT\", \"Face recognition library installed and loaded\")\n",
        "            return True\n",
        "        except ImportError:\n",
        "            log_event(\"WARNING\", \"face_recognition library installation failed. Using basic detection only.\")\n",
        "            return False\n"
      ],
      "metadata": {
        "id": "HBbejO8Jfzr3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_known_faces(known_faces_path='known_faces'):\n",
        "    \"\"\"Load known faces from the database.\"\"\"\n",
        "    global known_face_encodings, known_face_names, face_recognition_loaded\n",
        "\n",
        "    ensure_directory(known_faces_path)\n",
        "\n",
        "    if not face_recognition_loaded and not load_face_recognition():\n",
        "        log_event(\"WARNING\", \"Face recognition not available. Cannot load known faces.\")\n",
        "        return False\n",
        "\n",
        "    encodings_file = os.path.join(known_faces_path, \"face_encodings.pickle\")\n",
        "\n",
        "    if os.path.exists(encodings_file):\n",
        "        log_event(\"INFO\", \"Loading pre-computed face encodings...\")\n",
        "        try:\n",
        "            with open(encodings_file, \"rb\") as f:\n",
        "                data = pickle.load(f)\n",
        "                known_face_encodings = data[\"encodings\"]\n",
        "                known_face_names = data[\"names\"]\n",
        "            log_event(\"INFO\", f\"Loaded {len(known_face_names)} known faces\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            log_event(\"ERROR\", f\"Failed to load face encodings: {e}\")\n",
        "\n",
        "    log_event(\"INFO\", \"No pre-computed encodings found. Processing image files...\")\n",
        "    known_face_encodings = []\n",
        "    known_face_names = []\n",
        "\n",
        "    for filename in os.listdir(known_faces_path):\n",
        "        if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
        "            name = os.path.splitext(filename)[0].replace('_', ' ').title()\n",
        "            image_path = os.path.join(known_faces_path, filename)\n",
        "\n",
        "            try:\n",
        "                image = face_recognition.load_image_file(image_path)\n",
        "                face_locations = face_recognition.face_locations(image)\n",
        "\n",
        "                if face_locations:\n",
        "                    face_encoding = face_recognition.face_encodings(image, [face_locations[0]])[0]\n",
        "                    known_face_encodings.append(face_encoding)\n",
        "                    known_face_names.append(name)\n",
        "                    log_event(\"INFO\", f\"Loaded face: {name}\")\n",
        "                else:\n",
        "                    log_event(\"WARNING\", f\"No face detected in {filename}\")\n",
        "            except Exception as e:\n",
        "                log_event(\"ERROR\", f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    if known_face_encodings:\n",
        "        try:\n",
        "            log_event(\"INFO\", \"Saving face encodings to file...\")\n",
        "            data = {\"encodings\": known_face_encodings, \"names\": known_face_names}\n",
        "            with open(encodings_file, \"wb\") as f:\n",
        "                pickle.dump(data, f)\n",
        "        except Exception as e:\n",
        "            log_event(\"ERROR\", f\"Failed to save face encodings: {e}\")\n",
        "\n",
        "    log_event(\"INFO\", f\"Loaded {len(known_face_names)} known faces\")\n",
        "    return len(known_face_names) > 0\n"
      ],
      "metadata": {
        "id": "zCzDdnXGf3YO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_faces(frame):\n",
        "    \"\"\"Basic face detection using Haar cascade.\"\"\"\n",
        "    global face_detector\n",
        "    if face_detector is None:\n",
        "        face_detector = init_face_detector()\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_detector.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "    return faces\n"
      ],
      "metadata": {
        "id": "V4HtLYb0f5Xf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def recognize_faces(frame, threshold=0.6):\n",
        "    \"\"\"Detect and recognize faces in the frame.\"\"\"\n",
        "    global face_recognition_loaded, known_face_encodings, known_face_names\n",
        "\n",
        "    display_frame = frame.copy()\n",
        "    detected_faces = []\n",
        "\n",
        "    if not face_recognition_loaded:\n",
        "        if not load_face_recognition():\n",
        "            faces = detect_faces(frame)\n",
        "            for (x, y, w, h) in faces:\n",
        "                cv2.rectangle(display_frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
        "                name = \"Unknown\"\n",
        "                cv2.putText(display_frame, name, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
        "                detected_faces.append({'name': name, 'location': (x, y, x+w, y+h), 'recognized': False})\n",
        "            return display_frame, detected_faces\n",
        "\n",
        "    if not known_face_encodings:\n",
        "        load_known_faces()\n",
        "\n",
        "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
        "    rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    face_locations = face_recognition.face_locations(rgb_small_frame)\n",
        "    face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
        "\n",
        "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
        "        top *= 4\n",
        "        right *= 4\n",
        "        bottom *= 4\n",
        "        left *= 4\n",
        "\n",
        "        matches = []\n",
        "        name = \"Unknown\"\n",
        "\n",
        "        if known_face_encodings:\n",
        "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding, tolerance=threshold)\n",
        "            if True in matches:\n",
        "                first_match_index = matches.index(True)\n",
        "                name = known_face_names[first_match_index]\n",
        "\n",
        "        cv2.rectangle(display_frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
        "        cv2.rectangle(display_frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
        "        cv2.putText(display_frame, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_DUPLEX, 1.0, (255, 255, 255), 1)\n",
        "\n",
        "        detected_faces.append({'name': name, 'location': (left, top, right, bottom), 'recognized': name != \"Unknown\"})\n",
        "\n",
        "    return display_frame, detected_faces\n",
        "\n"
      ],
      "metadata": {
        "id": "a88KcoNof8mP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ====================================================================\n",
        "# MAIN SURVEILLANCE SYSTEM\n",
        "# ======================================================================\n"
      ],
      "metadata": {
        "id": "EtJVKiVkf-xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SurveillanceSystem:\n",
        "    def __init__(self):\n",
        "        self.config = self.load_default_config()\n",
        "        self.running = False\n",
        "        self.motion_counter = 0\n",
        "        self.last_alert_time = 0\n",
        "\n",
        "        # Initialize components\n",
        "        init_logger()\n",
        "        log_event(\"SYSTEM\", \"Surveillance System Initialized\")\n",
        "\n",
        "    def load_default_config(self):\n",
        "        \"\"\"Load default configuration for Colab environment.\"\"\"\n",
        "        return {\n",
        "            'camera_source': 0,  # Use webcam\n",
        "            'roi_start_point': None,\n",
        "            'roi_end_point': None,\n",
        "            'detection_mode': 'both',\n",
        "            'min_contour_area': 5000,\n",
        "            'persistence_threshold': 5,\n",
        "            'alert_interval': 10,\n",
        "            'face_recognition_enabled': True,\n",
        "            'face_recognition_threshold': 0.6,\n",
        "            'output_dir': 'surveillance_footage'\n",
        "        }\n",
        "\n",
        "    def process_frame(self, frame):\n",
        "        \"\"\"Process a single frame for motion and face detection.\"\"\"\n",
        "        result_frame = frame.copy()\n",
        "        alerts = []\n",
        "\n",
        "        # Add timestamp\n",
        "        result_frame = draw_timestamp(result_frame)\n",
        "\n",
        "        # Draw ROI if specified\n",
        "        if self.config['roi_start_point'] and self.config['roi_end_point']:\n",
        "            result_frame = draw_roi(result_frame, self.config['roi_start_point'], self.config['roi_end_point'])\n",
        "\n",
        "        # Motion detection\n",
        "        if self.config['detection_mode'] in ['motion', 'both']:\n",
        "            motion_detected, motion_mask, motion_contours = detect_motion(\n",
        "                frame,\n",
        "                self.config['roi_start_point'],\n",
        "                self.config['roi_end_point'],\n",
        "                self.config['min_contour_area']\n",
        "            )\n",
        "\n",
        "            if motion_detected:\n",
        "                self.motion_counter += 1\n",
        "                result_frame = draw_motion_contours(result_frame, motion_contours,\n",
        "                                                 self.config['roi_start_point'], self.config['roi_end_point'])\n",
        "\n",
        "                if self.motion_counter >= self.config['persistence_threshold']:\n",
        "                    current_time = time.time()\n",
        "                    if current_time - self.last_alert_time > self.config['alert_interval']:\n",
        "                        alerts.append(\"MOTION DETECTED\")\n",
        "                        log_event(\"ALERT\", \"Motion detected\")\n",
        "                        save_frame(result_frame, self.config['output_dir'], \"motion\")\n",
        "                        self.last_alert_time = current_time\n",
        "            else:\n",
        "                self.motion_counter = 0\n",
        "\n",
        "        # Face detection/recognition\n",
        "        if self.config['detection_mode'] in ['face', 'both'] and self.config['face_recognition_enabled']:\n",
        "            result_frame, detected_faces = recognize_faces(result_frame, self.config['face_recognition_threshold'])\n",
        "\n",
        "            for face in detected_faces:\n",
        "                if not face['recognized']:\n",
        "                    alerts.append(f\"UNKNOWN FACE DETECTED\")\n",
        "                    log_event(\"ALERT\", f\"Unknown face detected\")\n",
        "                    save_frame(result_frame, self.config['output_dir'], \"unknown_face\")\n",
        "                else:\n",
        "                    log_event(\"INFO\", f\"Known face detected: {face['name']}\")\n",
        "\n",
        "        return result_frame, alerts\n",
        "\n",
        "    def run_on_image(self, image_path):\n",
        "        \"\"\"Process a single image.\"\"\"\n",
        "        frame = cv2.imread(image_path)\n",
        "        if frame is None:\n",
        "            print(f\"Could not load image: {image_path}\")\n",
        "            return None\n",
        "\n",
        "        processed_frame, alerts = self.process_frame(frame)\n",
        "\n",
        "        print(f\"Alerts: {alerts}\")\n",
        "        return processed_frame\n",
        "\n",
        "    def setup_dataset(self, dataset_path):\n",
        "        \"\"\"Setup known faces from dataset.\"\"\"\n",
        "        if not os.path.exists(dataset_path):\n",
        "            print(f\"Dataset path {dataset_path} does not exist\")\n",
        "            return False\n",
        "\n",
        "        known_faces_dir = ensure_directory('known_faces')\n",
        "\n",
        "        # Copy some sample images from dataset to known_faces\n",
        "        count = 0\n",
        "        for person_dir in os.listdir(dataset_path):\n",
        "            person_path = os.path.join(dataset_path, person_dir)\n",
        "            if os.path.isdir(person_path) and count < 5:  # Limit to 5 people for demo\n",
        "                images = [f for f in os.listdir(person_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "                if images:\n",
        "                    # Take first image for each person\n",
        "                    src_image = os.path.join(person_path, images[0])\n",
        "                    dst_image = os.path.join(known_faces_dir, f\"{person_dir}.jpg\")\n",
        "\n",
        "                    # Copy the image\n",
        "                    import shutil\n",
        "                    shutil.copy2(src_image, dst_image)\n",
        "                    print(f\"Added {person_dir} to known faces\")\n",
        "                    count += 1\n",
        "\n",
        "        # Load the known faces\n",
        "        load_known_faces()\n",
        "        return True\n"
      ],
      "metadata": {
        "id": "zWv4sJUdgGHn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ====================================================================\n",
        "# COLAB INTERFACE FUNCTIONS\n",
        "# ======================================================================\n"
      ],
      "metadata": {
        "id": "TgObJznLgJDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def demo_image_processing():\n",
        "    \"\"\"Demo function for processing uploaded images.\"\"\"\n",
        "    print(\"Upload an image to test the surveillance system:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded\")\n",
        "        return\n",
        "\n",
        "    # Initialize surveillance system\n",
        "    system = SurveillanceSystem()\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        print(f\"Processing {filename}...\")\n",
        "\n",
        "        # Save uploaded file\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(uploaded[filename])\n",
        "\n",
        "        # Process the image\n",
        "        processed_frame = system.run_on_image(filename)\n",
        "\n",
        "        if processed_frame is not None:\n",
        "            # Display results\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            original = cv2.imread(filename)\n",
        "            plt.imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))\n",
        "            plt.title('Original Image')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.imshow(cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB))\n",
        "            plt.title('Processed Image')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n"
      ],
      "metadata": {
        "id": "uTS0t2XVgO1O"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def setup_lfw_dataset():\n",
        "    \"\"\"Setup Labeled Faces in the Wild dataset.\"\"\"\n",
        "    print(\"Setting up LFW dataset...\")\n",
        "    print(\"Please upload the LFW dataset or run this cell after downloading it:\")\n",
        "    print(\"!wget http://vis-www.cs.umass.edu/lfw/lfw.tgz\")\n",
        "    print(\"!tar -xzf lfw.tgz\")\n",
        "\n",
        "    # Check if dataset exists\n",
        "    if os.path.exists('lfw'):\n",
        "        system = SurveillanceSystem()\n",
        "        system.setup_dataset('lfw')\n",
        "        print(\"Dataset setup complete!\")\n",
        "    else:\n",
        "        print(\"LFW dataset not found. Please download it first.\")\n"
      ],
      "metadata": {
        "id": "qSM0HBalgQr2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ====================================================================\n",
        "# USAGE INSTRUCTIONS\n",
        "# ======================================================================\n"
      ],
      "metadata": {
        "id": "4cYLAQFbgTnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def print_usage_instructions():\n",
        "    \"\"\"Print usage instructions for Colab.\"\"\"\n",
        "    print(\"\"\"\n",
        "    🚀 AI SURVEILLANCE SYSTEM - COLAB VERSION\n",
        "    ==========================================\n",
        "\n",
        "    Your surveillance system includes:\n",
        "    ✅ Motion Detection using background subtraction\n",
        "    ✅ Face Detection using Haar cascades\n",
        "    ✅ Face Recognition using face_recognition library\n",
        "    ✅ Configurable ROI (Region of Interest)\n",
        "    ✅ Event logging and frame saving\n",
        "    ✅ Modular architecture maintained\n",
        "\n",
        "    QUICK START GUIDE:\n",
        "    ------------------\n",
        "\n",
        "    1. Test with an image:\n",
        "       demo_image_processing()\n",
        "\n",
        "    2. Setup LFW dataset for face recognition:\n",
        "       setup_lfw_dataset()\n",
        "\n",
        "    3. Create surveillance system instance:\n",
        "       system = SurveillanceSystem()\n",
        "\n",
        "    4. Process an image:\n",
        "       frame = cv2.imread('your_image.jpg')\n",
        "       processed, alerts = system.process_frame(frame)\n",
        "\n",
        "    5. Check logs:\n",
        "       !cat surveillance_footage/security_log.txt\n",
        "\n",
        "    FILES CREATED:\n",
        "    --------------\n",
        "    📁 surveillance_footage/ - Saved detection frames and logs\n",
        "    📁 known_faces/ - Database of known faces\n",
        "    📄 security_log.txt - Event logs\n",
        "\n",
        "    \"\"\")\n"
      ],
      "metadata": {
        "id": "SvfpHDH1gY4v"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize and show instructions\n"
      ],
      "metadata": {
        "id": "iZynu9-wgbDH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SularLwMV0g4",
        "outputId": "8bb8a42b-90e0-474a-a206-0d5767e9c2bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    🚀 AI SURVEILLANCE SYSTEM - COLAB VERSION\n",
            "    ==========================================\n",
            "    \n",
            "    Your surveillance system includes:\n",
            "    ✅ Motion Detection using background subtraction\n",
            "    ✅ Face Detection using Haar cascades  \n",
            "    ✅ Face Recognition using face_recognition library\n",
            "    ✅ Configurable ROI (Region of Interest)\n",
            "    ✅ Event logging and frame saving\n",
            "    ✅ Modular architecture maintained\n",
            "    \n",
            "    QUICK START GUIDE:\n",
            "    ------------------\n",
            "    \n",
            "    1. Test with an image:\n",
            "       demo_image_processing()\n",
            "    \n",
            "    2. Setup LFW dataset for face recognition:\n",
            "       setup_lfw_dataset()\n",
            "    \n",
            "    3. Create surveillance system instance:\n",
            "       system = SurveillanceSystem()\n",
            "       \n",
            "    4. Process an image:\n",
            "       frame = cv2.imread('your_image.jpg')\n",
            "       processed, alerts = system.process_frame(frame)\n",
            "    \n",
            "    5. Check logs:\n",
            "       !cat surveillance_footage/security_log.txt\n",
            "    \n",
            "    FILES CREATED:\n",
            "    --------------\n",
            "    📁 surveillance_footage/ - Saved detection frames and logs\n",
            "    📁 known_faces/ - Database of known faces\n",
            "    📄 security_log.txt - Event logs\n",
            "    \n",
            "    \n"
          ]
        }
      ],
      "source": [
        "print_usage_instructions()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "       demo_image_processing()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w4TBBJ1ee0W5",
        "outputId": "f6edfe15-bc26-41f0-9e99-5d043be69304"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload an image to test the surveillance system:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8f6eefe4-b954-4653-8930-3b31d8a69f5e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8f6eefe4-b954-4653-8930-3b31d8a69f5e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 3face1.jpg to 3face1.jpg\n",
            "Created directory: surveillance_footage\n",
            "[2025-05-30 14:04:33] SYSTEM - Surveillance System Initialized\n",
            "Processing 3face1.jpg...\n",
            "[2025-05-30 14:04:33] INIT - Motion detector initialized\n",
            "[2025-05-30 14:04:33] WARNING - face_recognition library not available. Installing...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error while calling cudaGetDevice(&the_device_id) in file /tmp/.tmpYAwLzy/sdists-v9/pypi/dlib/19.24.6/b_ZuI215-z7cRkD5Gta0t/src/dlib/cuda/gpu_data.cpp:204. code: 35, reason: CUDA driver version is insufficient for CUDA runtime version",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-20688372eeed>\u001b[0m in \u001b[0;36mload_face_recognition\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mface_recognition_loaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'face_recognition'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-3a6346146dc1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdemo_image_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-80a500c50c60>\u001b[0m in \u001b[0;36mdemo_image_processing\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Process the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mprocessed_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_on_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprocessed_frame\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-1901d4f71b70>\u001b[0m in \u001b[0;36mrun_on_image\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mprocessed_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malerts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Alerts: {alerts}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-1901d4f71b70>\u001b[0m in \u001b[0;36mprocess_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Face detection/recognition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'detection_mode'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'face'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'both'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'face_recognition_enabled'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mresult_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetected_faces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecognize_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'face_recognition_threshold'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdetected_faces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-efba114195ed>\u001b[0m in \u001b[0;36mrecognize_faces\u001b[0;34m(frame, threshold)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mface_recognition_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mload_face_recognition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-20688372eeed>\u001b[0m in \u001b[0;36mload_face_recognition\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pip install face_recognition\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mface_recognition_loaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mlog_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"INIT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Face recognition library installed and loaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/face_recognition/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1.2.3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_image_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_face_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_landmarks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompare_faces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/face_recognition/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mcnn_face_detection_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_face_detector_model_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mcnn_face_detector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_face_detection_model_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_face_detection_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mface_recognition_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_recognition_model_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error while calling cudaGetDevice(&the_device_id) in file /tmp/.tmpYAwLzy/sdists-v9/pypi/dlib/19.24.6/b_ZuI215-z7cRkD5Gta0t/src/dlib/cuda/gpu_data.cpp:204. code: 35, reason: CUDA driver version is insufficient for CUDA runtime version"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "       setup_lfw_dataset()\n"
      ],
      "metadata": {
        "id": "2TFhl8JsgeSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "       system = SurveillanceSystem()\n"
      ],
      "metadata": {
        "id": "hpFPEoQTgrfT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
