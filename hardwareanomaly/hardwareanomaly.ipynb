{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nawel-Bellil/ENERGY-EFFICIENCY-MONITORING-SECURITY-IN-ATM-SITRE/blob/main/NESTHACK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the original CSV file for BMC data\n",
        "# Skip the first 3 rows and read the next 1290 rows\n",
        "dBMC = pd.read_csv('BMC.csv', skiprows=3, nrows=1290)\n",
        "\n",
        "# Read the original CSV file for host data\n",
        "# Skip the first 1308 rows and read the next 1718 rows\n",
        "dhost = pd.read_csv('BMC.csv', skiprows=1308, nrows=1718)\n",
        "\n",
        "# Drop columns from BMC dataset\n",
        "# Keep only time, device_id, and sensor measurements\n",
        "dBMC_reduced = dBMC.drop(columns=['result', 'table', 'Unnamed: 0', '_start', '_stop'])\n",
        "\n",
        "# Drop columns from host dataset\n",
        "# Keeping only essential performance metrics and dropping individual core metrics\n",
        "columns_to_drop_host = ['result', 'table', 'Unnamed: 0', '_start', '_stop', '_measurement']\n",
        "\n",
        "# Add core metrics to drop list (we'll keep aggregated metrics instead of individual cores)\n",
        "for i in range(0, 128):\n",
        "    columns_to_drop_host.append(f'core{i}')\n",
        "\n",
        "# Drop the identified columns\n",
        "dhost_reduced = dhost.drop(columns=columns_to_drop_host, errors='ignore')\n",
        "\n",
        "# Save to new CSV files\n",
        "dBMC_reduced.to_csv('BMCsensor_reduced.csv', index=False)\n",
        "dhost_reduced.to_csv('host_reduced.csv', index=False)\n",
        "\n",
        "# Display the headers of the new files\n",
        "print(\"Header of the BMC sensor file after dropping columns:\")\n",
        "print(dBMC_reduced.head())\n",
        "\n",
        "print(\"\\nHeader of the host file after dropping columns:\")\n",
        "print(dhost_reduced.head())\n",
        "\n",
        "# Display columns kept in each dataset\n",
        "print(\"\\nColumns kept in BMC dataset:\")\n",
        "print(dBMC_reduced.columns.tolist())\n",
        "\n",
        "print(\"\\nColumns kept in host dataset:\")\n",
        "print(dhost_reduced.columns.tolist())\n",
        "\n",
        "# Load the reduced datasets\n",
        "dfBMC = pd.read_csv('BMCsensor_reduced.csv')\n",
        "dfhost = pd.read_csv('host_reduced.csv')\n",
        "\n",
        "# Display dataset information\n",
        "dfBMC.info()\n",
        "dfhost.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3GegDtAOMqX",
        "outputId": "f9cf0d89-c4a3-4caf-994f-d3f8ffcb410d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Header of the BMC sensor file after dropping columns:\n",
            "                  _time _measurement device_id  Chipset2_Temp  Chipset_Temp  \\\n",
            "0  2024-11-01T10:01:08Z         sdgp       bmc           53.0          49.5   \n",
            "1  2024-11-01T10:01:20Z         sdgp       bmc           53.0          49.0   \n",
            "2  2024-11-01T10:01:30Z         sdgp       bmc           53.0          49.0   \n",
            "3  2024-11-01T10:01:41Z         sdgp       bmc           53.0          49.5   \n",
            "4  2024-11-01T10:01:53Z         sdgp       bmc           53.0          49.0   \n",
            "\n",
            "   Cpu1_Temp  Cpu2_Temp  FAN1  FAN2  FAN3  ...  PSU1_CIN  PSU1_FAN  \\\n",
            "0       49.0       43.5  1706  1702  1682  ...      0.53      6264   \n",
            "1       48.5       43.5  1674  1692  1701  ...      0.53      6264   \n",
            "2       48.5       43.5  1695  1684  1698  ...      0.53      6264   \n",
            "3       48.5       43.5  1696  1692  1691  ...      0.53      6264   \n",
            "4       48.5       43.5  1701  1684  1697  ...      0.52      6264   \n",
            "\n",
            "   PSU1_Inlet  PSU1_Total_Power  PSU1_VIN  PSU2_CIN  PSU2_FAN  PSU2_Inlet  \\\n",
            "0          31               120     230.5      0.56      4920          24   \n",
            "1          31               120     230.0      0.56      4920          24   \n",
            "2          31               120     230.0      0.56      4920          24   \n",
            "3          31               120     230.5      0.56      4888          24   \n",
            "4          31               119     230.5      0.57      4888          24   \n",
            "\n",
            "   PSU2_Total_Power  PSU2_VIN  \n",
            "0               126    230.25  \n",
            "1               126    230.25  \n",
            "2               126    230.00  \n",
            "3               127    230.75  \n",
            "4               128    230.75  \n",
            "\n",
            "[5 rows x 24 columns]\n",
            "\n",
            "Header of the host file after dropping columns:\n",
            "                  _time device_id L1_dcache_load_misses L1_dcache_loads  \\\n",
            "0  2024-11-01T10:01:08Z      host               6148582       131532464   \n",
            "1  2024-11-01T10:01:15Z      host               5567586       128338497   \n",
            "2  2024-11-01T10:01:22Z      host              25500334       705701972   \n",
            "3  2024-11-01T10:01:29Z      host               4796724       103290280   \n",
            "4  2024-11-01T10:01:36Z      host               6815037       154150807   \n",
            "\n",
            "  L1_dcache_store_misses L1_dcache_stores L1_icache_load_misses  \\\n",
            "0                4982528        108385704               3823946   \n",
            "1                6579029        173691566               5894640   \n",
            "2               27288695        734316864              35784141   \n",
            "3                3920736         97063966               3260699   \n",
            "4                6296245        143639750               4093086   \n",
            "\n",
            "  L1_icache_loads branch_misses  bus_cycles  ... temp2 temp3  temp4  temp5  \\\n",
            "0       134540963       1904839  1261213451  ...  57.4  59.8   57.4   59.2   \n",
            "1       189556586       1691909  1645591668  ...  57.3  59.8   57.4   59.0   \n",
            "2      1036933082      16030986  4716820185  ...  57.2  59.5   57.3   58.8   \n",
            "3       131283052       1450661  1094396064  ...  56.9  59.4   57.2   58.8   \n",
            "4       142953526       1501766  1416467078  ...  57.0  59.3   57.1   58.7   \n",
            "\n",
            "   temp6  temp7  temp8  temp9  total_vcpu  total_vm  \n",
            "0   57.4   59.8   58.1   56.0         0.0       0.0  \n",
            "1   57.4   59.7   58.0   56.0         0.0       0.0  \n",
            "2   57.2   59.4   57.8   56.0         0.0       0.0  \n",
            "3   57.1   59.3   57.7   56.0         0.0       0.0  \n",
            "4   56.8   59.2   57.6   56.0         0.0       0.0  \n",
            "\n",
            "[5 rows x 50 columns]\n",
            "\n",
            "Columns kept in BMC dataset:\n",
            "['_time', '_measurement', 'device_id', 'Chipset2_Temp', 'Chipset_Temp', 'Cpu1_Temp', 'Cpu2_Temp', 'FAN1', 'FAN2', 'FAN3', 'FAN4', 'IO_Outlet_Temp', 'Inlet_Temp', 'Outlet_Temp', 'PSU1_CIN', 'PSU1_FAN', 'PSU1_Inlet', 'PSU1_Total_Power', 'PSU1_VIN', 'PSU2_CIN', 'PSU2_FAN', 'PSU2_Inlet', 'PSU2_Total_Power', 'PSU2_VIN']\n",
            "\n",
            "Columns kept in host dataset:\n",
            "['_time', 'device_id', 'L1_dcache_load_misses', 'L1_dcache_loads', 'L1_dcache_store_misses', 'L1_dcache_stores', 'L1_icache_load_misses', 'L1_icache_loads', 'branch_misses', 'bus_cycles', 'cache_misses', 'cache_references', 'cpu_migrations', 'cpu_usage', 'ctx_switches', 'cycles', 'dTLB_load_misses', 'flag1', 'flag2', 'flag3', 'flag4', 'freq1', 'freq2', 'freq3', 'freq4', 'freq5', 'freq6', 'freq7', 'freq8', 'iTLB_load_misses', 'instructions', 'kvm_entry', 'kvm_exit', 'kvm_vcpu_wakeup', 'load_1', 'load_15', 'load_5', 'mem_usage', 'page_faults', 'temp1', 'temp2', 'temp3', 'temp4', 'temp5', 'temp6', 'temp7', 'temp8', 'temp9', 'total_vcpu', 'total_vm']\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1290 entries, 0 to 1289\n",
            "Data columns (total 24 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   _time             1290 non-null   object \n",
            " 1   _measurement      1290 non-null   object \n",
            " 2   device_id         1290 non-null   object \n",
            " 3   Chipset2_Temp     1290 non-null   float64\n",
            " 4   Chipset_Temp      1290 non-null   float64\n",
            " 5   Cpu1_Temp         1290 non-null   float64\n",
            " 6   Cpu2_Temp         1290 non-null   float64\n",
            " 7   FAN1              1290 non-null   int64  \n",
            " 8   FAN2              1290 non-null   int64  \n",
            " 9   FAN3              1290 non-null   int64  \n",
            " 10  FAN4              1290 non-null   int64  \n",
            " 11  IO_Outlet_Temp    1290 non-null   float64\n",
            " 12  Inlet_Temp        1290 non-null   float64\n",
            " 13  Outlet_Temp       1290 non-null   float64\n",
            " 14  PSU1_CIN          1290 non-null   float64\n",
            " 15  PSU1_FAN          1290 non-null   int64  \n",
            " 16  PSU1_Inlet        1290 non-null   int64  \n",
            " 17  PSU1_Total_Power  1290 non-null   int64  \n",
            " 18  PSU1_VIN          1290 non-null   float64\n",
            " 19  PSU2_CIN          1290 non-null   float64\n",
            " 20  PSU2_FAN          1290 non-null   int64  \n",
            " 21  PSU2_Inlet        1290 non-null   int64  \n",
            " 22  PSU2_Total_Power  1290 non-null   int64  \n",
            " 23  PSU2_VIN          1290 non-null   float64\n",
            "dtypes: float64(11), int64(10), object(3)\n",
            "memory usage: 242.0+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1718 entries, 0 to 1717\n",
            "Data columns (total 50 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   _time                   1718 non-null   object \n",
            " 1   device_id               1718 non-null   object \n",
            " 2   L1_dcache_load_misses   1718 non-null   object \n",
            " 3   L1_dcache_loads         1718 non-null   object \n",
            " 4   L1_dcache_store_misses  1718 non-null   object \n",
            " 5   L1_dcache_stores        1718 non-null   object \n",
            " 6   L1_icache_load_misses   1718 non-null   object \n",
            " 7   L1_icache_loads         1718 non-null   object \n",
            " 8   branch_misses           1718 non-null   object \n",
            " 9   bus_cycles              1718 non-null   object \n",
            " 10  cache_misses            1718 non-null   object \n",
            " 11  cache_references        1718 non-null   object \n",
            " 12  cpu_migrations          1717 non-null   float64\n",
            " 13  cpu_usage               1717 non-null   float64\n",
            " 14  ctx_switches            1717 non-null   float64\n",
            " 15  cycles                  1717 non-null   float64\n",
            " 16  dTLB_load_misses        1717 non-null   float64\n",
            " 17  flag1                   1717 non-null   float64\n",
            " 18  flag2                   1717 non-null   float64\n",
            " 19  flag3                   1717 non-null   float64\n",
            " 20  flag4                   1717 non-null   float64\n",
            " 21  freq1                   1717 non-null   float64\n",
            " 22  freq2                   1717 non-null   float64\n",
            " 23  freq3                   1717 non-null   float64\n",
            " 24  freq4                   1717 non-null   float64\n",
            " 25  freq5                   1717 non-null   float64\n",
            " 26  freq6                   1717 non-null   float64\n",
            " 27  freq7                   1717 non-null   float64\n",
            " 28  freq8                   1717 non-null   float64\n",
            " 29  iTLB_load_misses        1717 non-null   float64\n",
            " 30  instructions            1717 non-null   float64\n",
            " 31  kvm_entry               1717 non-null   float64\n",
            " 32  kvm_exit                1717 non-null   float64\n",
            " 33  kvm_vcpu_wakeup         1717 non-null   float64\n",
            " 34  load_1                  1717 non-null   float64\n",
            " 35  load_15                 1717 non-null   float64\n",
            " 36  load_5                  1717 non-null   float64\n",
            " 37  mem_usage               1717 non-null   float64\n",
            " 38  page_faults             1717 non-null   float64\n",
            " 39  temp1                   1717 non-null   float64\n",
            " 40  temp2                   1717 non-null   float64\n",
            " 41  temp3                   1717 non-null   float64\n",
            " 42  temp4                   1717 non-null   float64\n",
            " 43  temp5                   1717 non-null   float64\n",
            " 44  temp6                   1717 non-null   float64\n",
            " 45  temp7                   1717 non-null   float64\n",
            " 46  temp8                   1717 non-null   float64\n",
            " 47  temp9                   1717 non-null   float64\n",
            " 48  total_vcpu              1717 non-null   float64\n",
            " 49  total_vm                1717 non-null   float64\n",
            "dtypes: float64(38), object(12)\n",
            "memory usage: 671.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set display options to see more columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Load the reduced datasets\n",
        "print(\"Loading datasets...\")\n",
        "dfBMC = pd.read_csv('BMCsensor_reduced.csv')\n",
        "dfhost = pd.read_csv('host_reduced.csv')\n",
        "\n",
        "# Display initial information\n",
        "print(\"\\nInitial BMC dataset info:\")\n",
        "print(dfBMC.info())\n",
        "print(\"\\nBMC dataset sample:\")\n",
        "print(dfBMC.head())\n",
        "\n",
        "print(\"\\nInitial host dataset info:\")\n",
        "print(dfhost.info())\n",
        "print(\"\\nHost dataset sample:\")\n",
        "print(dfhost.head())\n",
        "\n",
        "# ===== 1. IMPROVED TIME CONVERSION =====\n",
        "print(\"\\n=== Converting time columns to datetime ===\")\n",
        "\n",
        "# Function to convert time columns consistently with better handling for various formats\n",
        "def convert_time_column(df, time_col='_time'):\n",
        "    if time_col in df.columns:\n",
        "        # Check current format\n",
        "        print(f\"Sample time value before conversion: {df[time_col].iloc[0]}\")\n",
        "        print(f\"Current time column type: {df[time_col].dtype}\")\n",
        "\n",
        "        # Make a copy of original values to preserve them\n",
        "        original_times = df[time_col].copy()\n",
        "\n",
        "        # Convert to string first to handle any format issues\n",
        "        df[time_col] = df[time_col].astype(str)\n",
        "\n",
        "        # Method 1: Handle UTC 'Z' suffix explicitly\n",
        "        temp_times = df[time_col].str.replace('Z', '')\n",
        "        converted = pd.to_datetime(temp_times, errors='coerce')\n",
        "\n",
        "        # Check if we got good conversions\n",
        "        if converted.notna().sum() > 0:\n",
        "            df[time_col] = converted\n",
        "            print(f\"Successfully converted using Method 1 (Z suffix removal)\")\n",
        "        else:\n",
        "            # Method 2: Try with format specification\n",
        "            converted = pd.to_datetime(df[time_col], format='%Y-%m-%dT%H:%M:%S', errors='coerce')\n",
        "            if converted.notna().sum() > 0:\n",
        "                df[time_col] = converted\n",
        "                print(f\"Successfully converted using Method 2 (format specification)\")\n",
        "            else:\n",
        "                # Method 3: Most permissive approach\n",
        "                converted = pd.to_datetime(df[time_col], infer_datetime_format=True, errors='coerce')\n",
        "                if converted.notna().sum() > 0:\n",
        "                    df[time_col] = converted\n",
        "                    print(f\"Successfully converted using Method 3 (infer_datetime_format)\")\n",
        "                else:\n",
        "                    # If all methods fail, keep original strings but warn\n",
        "                    print(\"All conversion methods failed! Keeping original values.\")\n",
        "                    df[time_col] = original_times\n",
        "\n",
        "        # Report on results\n",
        "        invalid_count = df[time_col].isna().sum()\n",
        "        if invalid_count > 0:\n",
        "            print(f\"  Warning: {invalid_count} timestamps couldn't be converted ({invalid_count/len(df)*100:.1f}%)\")\n",
        "            if invalid_count < len(df):\n",
        "                # Show examples of successful and failed conversions\n",
        "                good_example = df.loc[df[time_col].notna(), time_col].iloc[0] if df[time_col].notna().any() else \"None\"\n",
        "                bad_idx = df[time_col].isna().idxmax() if df[time_col].isna().any() else None\n",
        "                bad_example = original_times.loc[bad_idx] if bad_idx is not None else \"None\"\n",
        "                print(f\"  Successful conversion example: {good_example}\")\n",
        "                print(f\"  Failed conversion example: {bad_example}\")\n",
        "        else:\n",
        "            print(f\"  All {len(df)} timestamps successfully converted\")\n",
        "\n",
        "        print(f\"Sample time value after conversion: {df[time_col].iloc[0]}\")\n",
        "        print(f\"New time column type: {df[time_col].dtype}\")\n",
        "    else:\n",
        "        print(f\"Time column '{time_col}' not found\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Convert time columns in both datasets\n",
        "dfBMC = convert_time_column(dfBMC)\n",
        "dfhost = convert_time_column(dfhost)\n",
        "\n",
        "# ===== 2. IMPROVED TYPE CONVERSION FOR NUMERIC COLUMNS =====\n",
        "print(\"\\n=== Converting string columns to numeric ===\")\n",
        "\n",
        "# Function to convert string columns to numeric with better error handling\n",
        "def convert_string_to_numeric(df, exclude_cols=None):\n",
        "    if exclude_cols is None:\n",
        "        exclude_cols = ['_time', 'device_id']\n",
        "\n",
        "    # Find string columns that aren't in the exclude list\n",
        "    string_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    string_cols = [col for col in string_cols if col not in exclude_cols]\n",
        "\n",
        "    if not string_cols:\n",
        "        print(f\"No string columns found that need conversion\")\n",
        "        return df, {}\n",
        "\n",
        "    print(f\"Converting {len(string_cols)} string columns to numeric\")\n",
        "    conversion_report = {}\n",
        "\n",
        "    for col in string_cols:\n",
        "        # Skip empty columns\n",
        "        if df[col].isna().all():\n",
        "            print(f\"  Skipping '{col}' - all values are NaN\")\n",
        "            continue\n",
        "\n",
        "        # Save original values for comparison\n",
        "        original_sample = df[col].head(2).tolist()\n",
        "        original_dtype = df[col].dtype\n",
        "\n",
        "        # Try to convert to numeric\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        # Report conversion results\n",
        "        na_count = df[col].isna().sum()\n",
        "        na_pct = na_count / len(df) * 100 if len(df) > 0 else 0\n",
        "\n",
        "        conversion_report[col] = {\n",
        "            'original_dtype': original_dtype,\n",
        "            'new_dtype': df[col].dtype,\n",
        "            'original_sample': original_sample,\n",
        "            'converted_sample': df[col].head(2).tolist(),\n",
        "            'na_count': na_count,\n",
        "            'na_percent': na_pct\n",
        "        }\n",
        "\n",
        "        print(f\"  Converted '{col}': {original_sample} -> {df[col].head(2).tolist()}\")\n",
        "        if na_count > 0:\n",
        "            print(f\"    Warning: {na_count} values ({na_pct:.1f}%) couldn't be converted to numeric\")\n",
        "\n",
        "            # If more than 50% failed, consider it might not be a numeric column\n",
        "            if na_pct > 50:\n",
        "                print(f\"    High failure rate for '{col}' - might not be a numeric column\")\n",
        "\n",
        "    return df, conversion_report\n",
        "\n",
        "# Convert string columns in both datasets\n",
        "dfBMC, bmc_conv_report = convert_string_to_numeric(dfBMC)\n",
        "dfhost, host_conv_report = convert_string_to_numeric(dfhost)\n",
        "\n",
        "# ===== 3. IMPROVED MISSING VALUE HANDLING =====\n",
        "print(\"\\n=== Handling missing values with improved approach ===\")\n",
        "\n",
        "# Function to handle missing values with more sophisticated methods\n",
        "def handle_missing_values(df, time_col='_time', strategy='mixed'):\n",
        "    missing_counts = df.isna().sum()\n",
        "    cols_with_missing = missing_counts[missing_counts > 0]\n",
        "\n",
        "    if len(cols_with_missing) == 0:\n",
        "        print(\"No missing values found\")\n",
        "        return df\n",
        "\n",
        "    print(f\"Found {len(cols_with_missing)} columns with missing values:\")\n",
        "    for col, count in cols_with_missing.items():\n",
        "        pct_missing = count / len(df) * 100\n",
        "        print(f\"  {col}: {count} missing values ({pct_missing:.2f}%)\")\n",
        "\n",
        "        # Skip time column - we've already handled it\n",
        "        if col == time_col:\n",
        "            continue\n",
        "\n",
        "        # For columns with extremely high missing value percentage, consider dropping\n",
        "        if pct_missing > 75:\n",
        "            print(f\"  Warning: {col} has {pct_missing:.2f}% missing values - consider dropping\")\n",
        "\n",
        "        # Apply selected imputation strategy\n",
        "        if strategy == 'mixed':\n",
        "            # For categorical columns (object type)\n",
        "            if df[col].dtype == 'object':\n",
        "                mode_val = df[col].mode()[0] if not df[col].mode().empty else \"UNKNOWN\"\n",
        "                df[col] = df[col].fillna(mode_val)\n",
        "                print(f\"  Filled '{col}' missing values with mode: {mode_val}\")\n",
        "\n",
        "            # For numeric columns, use more context-aware methods\n",
        "            else:\n",
        "                # If time-series data, consider forward/backward fill for temporal consistency\n",
        "                if '_time' in df.columns and pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
        "                    # Sort by time first\n",
        "                    df = df.sort_values(by=time_col)\n",
        "\n",
        "                    # Try forward fill first, then backward fill for any remaining NAs\n",
        "                    df[col] = df[col].fillna(method='ffill').fillna(method='bfill')\n",
        "                    remaining = df[col].isna().sum()\n",
        "\n",
        "                    # If we still have NAs, use median\n",
        "                    if remaining > 0:\n",
        "                        df[col] = df[col].fillna(df[col].median())\n",
        "                        print(f\"  Filled '{col}' using time-based methods + median for {remaining} values\")\n",
        "                    else:\n",
        "                        print(f\"  Filled '{col}' using time-based methods (ffill/bfill)\")\n",
        "                else:\n",
        "                    # Without time context, use median\n",
        "                    if df[col].count() > 0:  # Ensure we have some non-NaN values\n",
        "                        df[col] = df[col].fillna(df[col].median())\n",
        "                        print(f\"  Filled '{col}' with median: {df[col].median()}\")\n",
        "                    else:\n",
        "                        print(f\"  Warning: '{col}' has all NaN values, filling with 0\")\n",
        "                        df[col] = df[col].fillna(0)\n",
        "\n",
        "        elif strategy == 'median':\n",
        "            if df[col].dtype != 'object':\n",
        "                if df[col].count() > 0:\n",
        "                    df[col] = df[col].fillna(df[col].median())\n",
        "                    print(f\"  Filled '{col}' with median: {df[col].median()}\")\n",
        "                else:\n",
        "                    df[col] = df[col].fillna(0)\n",
        "                    print(f\"  Filled '{col}' with 0 (all values were NaN)\")\n",
        "            else:\n",
        "                mode_val = df[col].mode()[0] if not df[col].mode().empty else \"UNKNOWN\"\n",
        "                df[col] = df[col].fillna(mode_val)\n",
        "                print(f\"  Filled '{col}' with mode: {mode_val}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply improved missing value handling to both datasets\n",
        "dfBMC = handle_missing_values(dfBMC, strategy='mixed')\n",
        "dfhost = handle_missing_values(dfhost, strategy='mixed')\n",
        "\n",
        "# ===== 4. OUTLIER DETECTION AND HANDLING =====\n",
        "print(\"\\n=== Detecting and handling outliers ===\")\n",
        "\n",
        "# Function to detect and handle outliers\n",
        "def handle_outliers(df, exclude_cols=None, method='zscore', threshold=3):\n",
        "    if exclude_cols is None:\n",
        "        exclude_cols = ['_time', 'device_id']\n",
        "\n",
        "    # Only process numeric columns\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "    numeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
        "\n",
        "    if not numeric_cols:\n",
        "        print(\"No numeric columns found for outlier detection\")\n",
        "        return df, {}\n",
        "\n",
        "    outlier_stats = {}\n",
        "    print(f\"Checking {len(numeric_cols)} numeric columns for outliers\")\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        # Skip columns with no variance\n",
        "        if df[col].std() == 0:\n",
        "            print(f\"  Skipping '{col}' - zero variance\")\n",
        "            continue\n",
        "\n",
        "        if method == 'zscore':\n",
        "            # Z-score method: values beyond +/- threshold standard deviations\n",
        "            z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
        "            outliers = z_scores > threshold\n",
        "            outlier_count = outliers.sum()\n",
        "\n",
        "            if outlier_count > 0:\n",
        "                outlier_pct = outlier_count / len(df) * 100\n",
        "                outlier_stats[col] = {\n",
        "                    'count': outlier_count,\n",
        "                    'percentage': outlier_pct,\n",
        "                    'min': df.loc[outliers, col].min(),\n",
        "                    'max': df.loc[outliers, col].max()\n",
        "                }\n",
        "\n",
        "                print(f\"  Detected {outlier_count} outliers ({outlier_pct:.2f}%) in '{col}' using Z-score > {threshold}\")\n",
        "\n",
        "                # Only cap outliers if they're not too prevalent (< 10%)\n",
        "                if outlier_pct < 10:\n",
        "                    # Cap the outliers instead of removing\n",
        "                    lower_bound = df[col].mean() - threshold * df[col].std()\n",
        "                    upper_bound = df[col].mean() + threshold * df[col].std()\n",
        "\n",
        "                    # Store original values before capping\n",
        "                    original_min = df[col].min()\n",
        "                    original_max = df[col].max()\n",
        "\n",
        "                    # Cap outliers\n",
        "                    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "                    print(f\"    Capped values from [{original_min}, {original_max}] to [{df[col].min()}, {df[col].max()}]\")\n",
        "                else:\n",
        "                    print(f\"    Warning: Not capping outliers in '{col}' as they represent {outlier_pct:.2f}% of the data\")\n",
        "\n",
        "        elif method == 'iqr':\n",
        "            # IQR method: values beyond 1.5*IQR from Q1/Q3\n",
        "            Q1 = df[col].quantile(0.25)\n",
        "            Q3 = df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "\n",
        "            if IQR == 0:\n",
        "                print(f\"  Skipping '{col}' - IQR is zero\")\n",
        "                continue\n",
        "\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "            outliers = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
        "            outlier_count = outliers.sum()\n",
        "\n",
        "            if outlier_count > 0:\n",
        "                outlier_pct = outlier_count / len(df) * 100\n",
        "                outlier_stats[col] = {\n",
        "                    'count': outlier_count,\n",
        "                    'percentage': outlier_pct,\n",
        "                    'min': df.loc[outliers, col].min(),\n",
        "                    'max': df.loc[outliers, col].max(),\n",
        "                    'bounds': [lower_bound, upper_bound]\n",
        "                }\n",
        "\n",
        "                print(f\"  Detected {outlier_count} outliers ({outlier_pct:.2f}%) in '{col}' using IQR method\")\n",
        "\n",
        "                # Only cap outliers if they're not too prevalent (< 10%)\n",
        "                if outlier_pct < 10:\n",
        "                    # Store original values before capping\n",
        "                    original_min = df[col].min()\n",
        "                    original_max = df[col].max()\n",
        "\n",
        "                    # Cap outliers\n",
        "                    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "                    print(f\"    Capped values from [{original_min}, {original_max}] to [{df[col].min()}, {df[col].max()}]\")\n",
        "                else:\n",
        "                    print(f\"    Warning: Not capping outliers in '{col}' as they represent {outlier_pct:.2f}% of the data\")\n",
        "\n",
        "    return df, outlier_stats\n",
        "\n",
        "# Apply outlier handling to both datasets\n",
        "dfBMC, bmc_outlier_stats = handle_outliers(dfBMC, method='iqr')\n",
        "dfhost, host_outlier_stats = handle_outliers(dfhost, method='iqr')\n",
        "\n",
        "# ===== 5. FEATURE SELECTION BASED ON VARIANCE AND CORRELATION =====\n",
        "print(\"\\n=== Feature selection based on variance and correlation ===\")\n",
        "\n",
        "# Function to select features based on variance and correlation\n",
        "def select_features(df, exclude_cols=None, variance_threshold=0.01, correlation_threshold=0.95):\n",
        "    if exclude_cols is None:\n",
        "        exclude_cols = ['_time', 'device_id']\n",
        "\n",
        "    # Only analyze numeric columns\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "    numeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
        "\n",
        "    if not numeric_cols:\n",
        "        print(\"No numeric columns found for feature selection\")\n",
        "        return {'low_variance': [], 'high_correlation': []}\n",
        "\n",
        "    # Check for low variance features\n",
        "    variance = df[numeric_cols].var()\n",
        "    low_var_features = variance[variance < variance_threshold].index.tolist()\n",
        "\n",
        "    if low_var_features:\n",
        "        print(f\"Low variance features (< {variance_threshold}):\")\n",
        "        for feature in low_var_features:\n",
        "            print(f\"  {feature}: variance = {variance[feature]:.6f}\")\n",
        "    else:\n",
        "        print(f\"No low variance features found (threshold: {variance_threshold})\")\n",
        "\n",
        "    # Check for highly correlated features\n",
        "    try:\n",
        "        correlation_matrix = df[numeric_cols].corr().abs()\n",
        "\n",
        "        # Create a mask for the upper triangle\n",
        "        upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "        # Find features with correlation greater than threshold\n",
        "        high_corr_features = [column for column in upper.columns if any(upper[column] > correlation_threshold)]\n",
        "\n",
        "        if high_corr_features:\n",
        "            print(f\"\\nHighly correlated features (> {correlation_threshold}):\")\n",
        "            for column in high_corr_features:\n",
        "                correlated_with = upper[column][upper[column] > correlation_threshold].index.tolist()\n",
        "                for other_col in correlated_with:\n",
        "                    corr_value = correlation_matrix.loc[column, other_col]\n",
        "                    print(f\"  {column} and {other_col}: correlation = {corr_value:.4f}\")\n",
        "        else:\n",
        "            print(f\"No highly correlated features found (threshold: {correlation_threshold})\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during correlation analysis: {e}\")\n",
        "        high_corr_features = []\n",
        "\n",
        "    # Return list of features to drop\n",
        "    return {\n",
        "        'low_variance': low_var_features,\n",
        "        'high_correlation': high_corr_features\n",
        "    }\n",
        "\n",
        "# Apply feature selection to both datasets\n",
        "bmc_feature_selection = select_features(dfBMC)\n",
        "host_feature_selection = select_features(dfhost)\n",
        "\n",
        "# ===== 6. SAVE PROCESSED DATASETS =====\n",
        "print(\"\\n=== Saving processed datasets ===\")\n",
        "\n",
        "# Save the fully processed datasets\n",
        "dfBMC.to_csv('BMC_processed.csv', index=False)\n",
        "dfhost.to_csv('host_processed.csv', index=False)\n",
        "\n",
        "print(\"\\nProcessing complete. Datasets saved as BMC_processed.csv and host_processed.csv\")\n",
        "\n",
        "# ===== 7. DATASET SUMMARY =====\n",
        "print(\"\\n=== Final Dataset Summary ===\")\n",
        "print(f\"BMC dataset: {dfBMC.shape[0]} rows, {dfBMC.shape[1]} columns\")\n",
        "print(f\"Host dataset: {dfhost.shape[0]} rows, {dfhost.shape[1]} columns\")\n",
        "\n",
        "print(\"\\nBMC Columns:\")\n",
        "for col in dfBMC.columns:\n",
        "    print(f\"  {col} ({dfBMC[col].dtype})\")\n",
        "\n",
        "print(\"\\nHost Columns:\")\n",
        "for col in dfhost.columns:\n",
        "    print(f\"  {col} ({dfhost[col].dtype})\")\n",
        "\n",
        "# Create visual summary of the data if available\n",
        "try:\n",
        "    # Plot a correlation heatmap for BMC data\n",
        "    plt.figure(figsize=(14, 12))\n",
        "    numeric_cols = dfBMC.select_dtypes(include=['number']).columns\n",
        "    if len(numeric_cols) > 1:  # Need at least 2 columns for correlation\n",
        "        sns.heatmap(dfBMC[numeric_cols].corr(), annot=False, cmap='coolwarm')\n",
        "        plt.title('Correlation Heatmap - BMC Data')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('bmc_correlation_heatmap.png')\n",
        "        plt.close()\n",
        "        print(\"\\nSaved BMC correlation heatmap as bmc_correlation_heatmap.png\")\n",
        "\n",
        "    # Plot a correlation heatmap for host data\n",
        "    plt.figure(figsize=(14, 12))\n",
        "    numeric_cols = dfhost.select_dtypes(include=['number']).columns\n",
        "    if len(numeric_cols) > 1:  # Need at least 2 columns for correlation\n",
        "        sns.heatmap(dfhost[numeric_cols].corr(), annot=False, cmap='coolwarm')\n",
        "        plt.title('Correlation Heatmap - Host Data')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('host_correlation_heatmap.png')\n",
        "        plt.close()\n",
        "        print(\"Saved host correlation heatmap as host_correlation_heatmap.png\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating correlation heatmaps: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ar9YMfbM8ED9",
        "outputId": "dc2a46e5-a841-453b-b3ba-54c336d4e398"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "\n",
            "Initial BMC dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1290 entries, 0 to 1289\n",
            "Data columns (total 24 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   _time             1290 non-null   object \n",
            " 1   _measurement      1290 non-null   object \n",
            " 2   device_id         1290 non-null   object \n",
            " 3   Chipset2_Temp     1290 non-null   float64\n",
            " 4   Chipset_Temp      1290 non-null   float64\n",
            " 5   Cpu1_Temp         1290 non-null   float64\n",
            " 6   Cpu2_Temp         1290 non-null   float64\n",
            " 7   FAN1              1290 non-null   int64  \n",
            " 8   FAN2              1290 non-null   int64  \n",
            " 9   FAN3              1290 non-null   int64  \n",
            " 10  FAN4              1290 non-null   int64  \n",
            " 11  IO_Outlet_Temp    1290 non-null   float64\n",
            " 12  Inlet_Temp        1290 non-null   float64\n",
            " 13  Outlet_Temp       1290 non-null   float64\n",
            " 14  PSU1_CIN          1290 non-null   float64\n",
            " 15  PSU1_FAN          1290 non-null   int64  \n",
            " 16  PSU1_Inlet        1290 non-null   int64  \n",
            " 17  PSU1_Total_Power  1290 non-null   int64  \n",
            " 18  PSU1_VIN          1290 non-null   float64\n",
            " 19  PSU2_CIN          1290 non-null   float64\n",
            " 20  PSU2_FAN          1290 non-null   int64  \n",
            " 21  PSU2_Inlet        1290 non-null   int64  \n",
            " 22  PSU2_Total_Power  1290 non-null   int64  \n",
            " 23  PSU2_VIN          1290 non-null   float64\n",
            "dtypes: float64(11), int64(10), object(3)\n",
            "memory usage: 242.0+ KB\n",
            "None\n",
            "\n",
            "BMC dataset sample:\n",
            "                  _time _measurement device_id  Chipset2_Temp  Chipset_Temp  \\\n",
            "0  2024-11-01T10:01:08Z         sdgp       bmc           53.0          49.5   \n",
            "1  2024-11-01T10:01:20Z         sdgp       bmc           53.0          49.0   \n",
            "2  2024-11-01T10:01:30Z         sdgp       bmc           53.0          49.0   \n",
            "3  2024-11-01T10:01:41Z         sdgp       bmc           53.0          49.5   \n",
            "4  2024-11-01T10:01:53Z         sdgp       bmc           53.0          49.0   \n",
            "\n",
            "   Cpu1_Temp  Cpu2_Temp  FAN1  FAN2  FAN3  FAN4  IO_Outlet_Temp  Inlet_Temp  \\\n",
            "0       49.0       43.5  1706  1702  1682  1684            45.0        39.0   \n",
            "1       48.5       43.5  1674  1692  1701  1689            45.0        39.0   \n",
            "2       48.5       43.5  1695  1684  1698  1677            45.0        39.0   \n",
            "3       48.5       43.5  1696  1692  1691  1684            45.0        39.0   \n",
            "4       48.5       43.5  1701  1684  1697  1691            45.0        39.0   \n",
            "\n",
            "   Outlet_Temp  PSU1_CIN  PSU1_FAN  PSU1_Inlet  PSU1_Total_Power  PSU1_VIN  \\\n",
            "0         39.5      0.53      6264          31               120     230.5   \n",
            "1         39.5      0.53      6264          31               120     230.0   \n",
            "2         39.5      0.53      6264          31               120     230.0   \n",
            "3         39.5      0.53      6264          31               120     230.5   \n",
            "4         39.5      0.52      6264          31               119     230.5   \n",
            "\n",
            "   PSU2_CIN  PSU2_FAN  PSU2_Inlet  PSU2_Total_Power  PSU2_VIN  \n",
            "0      0.56      4920          24               126    230.25  \n",
            "1      0.56      4920          24               126    230.25  \n",
            "2      0.56      4920          24               126    230.00  \n",
            "3      0.56      4888          24               127    230.75  \n",
            "4      0.57      4888          24               128    230.75  \n",
            "\n",
            "Initial host dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1718 entries, 0 to 1717\n",
            "Data columns (total 50 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   _time                   1718 non-null   object \n",
            " 1   device_id               1718 non-null   object \n",
            " 2   L1_dcache_load_misses   1718 non-null   object \n",
            " 3   L1_dcache_loads         1718 non-null   object \n",
            " 4   L1_dcache_store_misses  1718 non-null   object \n",
            " 5   L1_dcache_stores        1718 non-null   object \n",
            " 6   L1_icache_load_misses   1718 non-null   object \n",
            " 7   L1_icache_loads         1718 non-null   object \n",
            " 8   branch_misses           1718 non-null   object \n",
            " 9   bus_cycles              1718 non-null   object \n",
            " 10  cache_misses            1718 non-null   object \n",
            " 11  cache_references        1718 non-null   object \n",
            " 12  cpu_migrations          1717 non-null   float64\n",
            " 13  cpu_usage               1717 non-null   float64\n",
            " 14  ctx_switches            1717 non-null   float64\n",
            " 15  cycles                  1717 non-null   float64\n",
            " 16  dTLB_load_misses        1717 non-null   float64\n",
            " 17  flag1                   1717 non-null   float64\n",
            " 18  flag2                   1717 non-null   float64\n",
            " 19  flag3                   1717 non-null   float64\n",
            " 20  flag4                   1717 non-null   float64\n",
            " 21  freq1                   1717 non-null   float64\n",
            " 22  freq2                   1717 non-null   float64\n",
            " 23  freq3                   1717 non-null   float64\n",
            " 24  freq4                   1717 non-null   float64\n",
            " 25  freq5                   1717 non-null   float64\n",
            " 26  freq6                   1717 non-null   float64\n",
            " 27  freq7                   1717 non-null   float64\n",
            " 28  freq8                   1717 non-null   float64\n",
            " 29  iTLB_load_misses        1717 non-null   float64\n",
            " 30  instructions            1717 non-null   float64\n",
            " 31  kvm_entry               1717 non-null   float64\n",
            " 32  kvm_exit                1717 non-null   float64\n",
            " 33  kvm_vcpu_wakeup         1717 non-null   float64\n",
            " 34  load_1                  1717 non-null   float64\n",
            " 35  load_15                 1717 non-null   float64\n",
            " 36  load_5                  1717 non-null   float64\n",
            " 37  mem_usage               1717 non-null   float64\n",
            " 38  page_faults             1717 non-null   float64\n",
            " 39  temp1                   1717 non-null   float64\n",
            " 40  temp2                   1717 non-null   float64\n",
            " 41  temp3                   1717 non-null   float64\n",
            " 42  temp4                   1717 non-null   float64\n",
            " 43  temp5                   1717 non-null   float64\n",
            " 44  temp6                   1717 non-null   float64\n",
            " 45  temp7                   1717 non-null   float64\n",
            " 46  temp8                   1717 non-null   float64\n",
            " 47  temp9                   1717 non-null   float64\n",
            " 48  total_vcpu              1717 non-null   float64\n",
            " 49  total_vm                1717 non-null   float64\n",
            "dtypes: float64(38), object(12)\n",
            "memory usage: 671.2+ KB\n",
            "None\n",
            "\n",
            "Host dataset sample:\n",
            "                  _time device_id L1_dcache_load_misses L1_dcache_loads  \\\n",
            "0  2024-11-01T10:01:08Z      host               6148582       131532464   \n",
            "1  2024-11-01T10:01:15Z      host               5567586       128338497   \n",
            "2  2024-11-01T10:01:22Z      host              25500334       705701972   \n",
            "3  2024-11-01T10:01:29Z      host               4796724       103290280   \n",
            "4  2024-11-01T10:01:36Z      host               6815037       154150807   \n",
            "\n",
            "  L1_dcache_store_misses L1_dcache_stores L1_icache_load_misses  \\\n",
            "0                4982528        108385704               3823946   \n",
            "1                6579029        173691566               5894640   \n",
            "2               27288695        734316864              35784141   \n",
            "3                3920736         97063966               3260699   \n",
            "4                6296245        143639750               4093086   \n",
            "\n",
            "  L1_icache_loads branch_misses  bus_cycles cache_misses cache_references  \\\n",
            "0       134540963       1904839  1261213451      5772986        131903099   \n",
            "1       189556586       1691909  1645591668      5637587        136737209   \n",
            "2      1036933082      16030986  4716820185     25998172        754605092   \n",
            "3       131283052       1450661  1094396064      4675731        104685326   \n",
            "4       142953526       1501766  1416467078      5865295        148082616   \n",
            "\n",
            "   cpu_migrations  cpu_usage  ctx_switches        cycles  dTLB_load_misses  \\\n",
            "0            23.0        0.6        6672.0  1.331380e+09          965327.0   \n",
            "1            44.0        0.4        6516.0  1.680316e+09         1668849.0   \n",
            "2           190.0        0.4       19896.0  4.675802e+09        10243404.0   \n",
            "3            23.0        1.3        4597.0  1.216232e+09          870151.0   \n",
            "4            28.0        0.4        6337.0  1.469586e+09          871928.0   \n",
            "\n",
            "   flag1  flag2  flag3  flag4  freq1  freq2  freq3  freq4  freq5  freq6  \\\n",
            "0    0.0    0.0    0.0    0.0    2.1    2.1    2.1    2.1    2.1    2.1   \n",
            "1    0.0    0.0    0.0    0.0    2.1    2.1    2.1    2.1    2.1    2.1   \n",
            "2    0.0    0.0    0.0    0.0    2.1    2.1    2.1    2.1    2.1    2.1   \n",
            "3    0.0    0.0    0.0    0.0    2.1    2.1    2.1    2.1    2.1    2.1   \n",
            "4    0.0    0.0    0.0    0.0    2.1    2.1    2.1    2.1    2.1    2.1   \n",
            "\n",
            "   freq7  freq8  iTLB_load_misses  instructions  kvm_entry  kvm_exit  \\\n",
            "0    2.1    2.1           81238.0  2.586487e+08        0.0       0.0   \n",
            "1    2.1    2.1          131669.0  3.375201e+08        0.0       0.0   \n",
            "2    2.1    2.1          639848.0  2.045661e+09        0.0       0.0   \n",
            "3    2.1    2.1           81536.0  2.196238e+08        0.0       0.0   \n",
            "4    2.1    2.1           80745.0  2.179312e+08        0.0       0.0   \n",
            "\n",
            "   kvm_vcpu_wakeup  load_1  load_15  load_5  mem_usage  page_faults  temp1  \\\n",
            "0              0.0    6.27    16.95   15.57       4.73          4.0   59.3   \n",
            "1              0.0    5.47    16.78   15.09       4.73         41.0   59.0   \n",
            "2              0.0    5.03    16.68   14.83       4.73       2051.0   59.0   \n",
            "3              0.0    4.71    16.60   14.60       4.74         75.0   58.8   \n",
            "4              0.0    4.06    16.43   14.14       4.74         10.0   58.7   \n",
            "\n",
            "   temp2  temp3  temp4  temp5  temp6  temp7  temp8  temp9  total_vcpu  \\\n",
            "0   57.4   59.8   57.4   59.2   57.4   59.8   58.1   56.0         0.0   \n",
            "1   57.3   59.8   57.4   59.0   57.4   59.7   58.0   56.0         0.0   \n",
            "2   57.2   59.5   57.3   58.8   57.2   59.4   57.8   56.0         0.0   \n",
            "3   56.9   59.4   57.2   58.8   57.1   59.3   57.7   56.0         0.0   \n",
            "4   57.0   59.3   57.1   58.7   56.8   59.2   57.6   56.0         0.0   \n",
            "\n",
            "   total_vm  \n",
            "0       0.0  \n",
            "1       0.0  \n",
            "2       0.0  \n",
            "3       0.0  \n",
            "4       0.0  \n",
            "\n",
            "=== Converting time columns to datetime ===\n",
            "Sample time value before conversion: 2024-11-01T10:01:08Z\n",
            "Current time column type: object\n",
            "Successfully converted using Method 1 (Z suffix removal)\n",
            "  All 1290 timestamps successfully converted\n",
            "Sample time value after conversion: 2024-11-01 10:01:08\n",
            "New time column type: datetime64[ns]\n",
            "Sample time value before conversion: 2024-11-01T10:01:08Z\n",
            "Current time column type: object\n",
            "Successfully converted using Method 1 (Z suffix removal)\n",
            "  Warning: 1 timestamps couldn't be converted (0.1%)\n",
            "  Successful conversion example: 2024-11-01 10:01:08\n",
            "  Failed conversion example: false\n",
            "Sample time value after conversion: 2024-11-01 10:01:08\n",
            "New time column type: datetime64[ns]\n",
            "\n",
            "=== Converting string columns to numeric ===\n",
            "Converting 1 string columns to numeric\n",
            "  Converted '_measurement': ['sdgp', 'sdgp'] -> [nan, nan]\n",
            "    Warning: 1290 values (100.0%) couldn't be converted to numeric\n",
            "    High failure rate for '_measurement' - might not be a numeric column\n",
            "Converting 10 string columns to numeric\n",
            "  Converted 'L1_dcache_load_misses': ['6148582', '5567586'] -> [6148582.0, 5567586.0]\n",
            "    Warning: 1 values (0.1%) couldn't be converted to numeric\n",
            "  Converted 'L1_dcache_loads': ['131532464', '128338497'] -> [131532464.0, 128338497.0]\n",
            "    Warning: 1 values (0.1%) couldn't be converted to numeric\n",
            "  Converted 'L1_dcache_store_misses': ['4982528', '6579029'] -> [4982528.0, 6579029.0]\n",
            "    Warning: 1 values (0.1%) couldn't be converted to numeric\n",
            "  Converted 'L1_dcache_stores': ['108385704', '173691566'] -> [108385704.0, 173691566.0]\n",
            "    Warning: 1 values (0.1%) couldn't be converted to numeric\n",
            "  Converted 'L1_icache_load_misses': ['3823946', '5894640'] -> [3823946.0, 5894640.0]\n",
            "    Warning: 1 values (0.1%) couldn't be converted to numeric\n",
            "  Converted 'L1_icache_loads': ['134540963', '189556586'] -> [134540963.0, 189556586.0]\n",
            "    Warning: 1 values (0.1%) couldn't be converted to numeric\n",
            "  Converted 'branch_misses': ['1904839', '1691909'] -> [1904839.0, 1691909.0]\n",
            "    Warning: 1 values (0.1%) couldn't be converted to numeric\n",
            "  Converted 'bus_cycles': ['1261213451', '1645591668'] -> [1261213451.0, 1645591668.0]\n",
            "    Warning: 1 values (0.1%) couldn't be converted to numeric\n",
            "  Converted 'cache_misses': ['5772986', '5637587'] -> [5772986.0, 5637587.0]\n",
            "    Warning: 1 values (0.1%) couldn't be converted to numeric\n",
            "  Converted 'cache_references': ['131903099', '136737209'] -> [131903099.0, 136737209.0]\n",
            "    Warning: 1 values (0.1%) couldn't be converted to numeric\n",
            "\n",
            "=== Handling missing values with improved approach ===\n",
            "Found 1 columns with missing values:\n",
            "  _measurement: 1290 missing values (100.00%)\n",
            "  Warning: _measurement has 100.00% missing values - consider dropping\n",
            "  Filled '_measurement' using time-based methods + median for 1290 values\n",
            "Found 49 columns with missing values:\n",
            "  _time: 1 missing values (0.06%)\n",
            "  L1_dcache_load_misses: 1 missing values (0.06%)\n",
            "  Filled 'L1_dcache_load_misses' using time-based methods (ffill/bfill)\n",
            "  L1_dcache_loads: 1 missing values (0.06%)\n",
            "  Filled 'L1_dcache_loads' using time-based methods (ffill/bfill)\n",
            "  L1_dcache_store_misses: 1 missing values (0.06%)\n",
            "  Filled 'L1_dcache_store_misses' using time-based methods (ffill/bfill)\n",
            "  L1_dcache_stores: 1 missing values (0.06%)\n",
            "  Filled 'L1_dcache_stores' using time-based methods (ffill/bfill)\n",
            "  L1_icache_load_misses: 1 missing values (0.06%)\n",
            "  Filled 'L1_icache_load_misses' using time-based methods (ffill/bfill)\n",
            "  L1_icache_loads: 1 missing values (0.06%)\n",
            "  Filled 'L1_icache_loads' using time-based methods (ffill/bfill)\n",
            "  branch_misses: 1 missing values (0.06%)\n",
            "  Filled 'branch_misses' using time-based methods (ffill/bfill)\n",
            "  bus_cycles: 1 missing values (0.06%)\n",
            "  Filled 'bus_cycles' using time-based methods (ffill/bfill)\n",
            "  cache_misses: 1 missing values (0.06%)\n",
            "  Filled 'cache_misses' using time-based methods (ffill/bfill)\n",
            "  cache_references: 1 missing values (0.06%)\n",
            "  Filled 'cache_references' using time-based methods (ffill/bfill)\n",
            "  cpu_migrations: 1 missing values (0.06%)\n",
            "  Filled 'cpu_migrations' using time-based methods (ffill/bfill)\n",
            "  cpu_usage: 1 missing values (0.06%)\n",
            "  Filled 'cpu_usage' using time-based methods (ffill/bfill)\n",
            "  ctx_switches: 1 missing values (0.06%)\n",
            "  Filled 'ctx_switches' using time-based methods (ffill/bfill)\n",
            "  cycles: 1 missing values (0.06%)\n",
            "  Filled 'cycles' using time-based methods (ffill/bfill)\n",
            "  dTLB_load_misses: 1 missing values (0.06%)\n",
            "  Filled 'dTLB_load_misses' using time-based methods (ffill/bfill)\n",
            "  flag1: 1 missing values (0.06%)\n",
            "  Filled 'flag1' using time-based methods (ffill/bfill)\n",
            "  flag2: 1 missing values (0.06%)\n",
            "  Filled 'flag2' using time-based methods (ffill/bfill)\n",
            "  flag3: 1 missing values (0.06%)\n",
            "  Filled 'flag3' using time-based methods (ffill/bfill)\n",
            "  flag4: 1 missing values (0.06%)\n",
            "  Filled 'flag4' using time-based methods (ffill/bfill)\n",
            "  freq1: 1 missing values (0.06%)\n",
            "  Filled 'freq1' using time-based methods (ffill/bfill)\n",
            "  freq2: 1 missing values (0.06%)\n",
            "  Filled 'freq2' using time-based methods (ffill/bfill)\n",
            "  freq3: 1 missing values (0.06%)\n",
            "  Filled 'freq3' using time-based methods (ffill/bfill)\n",
            "  freq4: 1 missing values (0.06%)\n",
            "  Filled 'freq4' using time-based methods (ffill/bfill)\n",
            "  freq5: 1 missing values (0.06%)\n",
            "  Filled 'freq5' using time-based methods (ffill/bfill)\n",
            "  freq6: 1 missing values (0.06%)\n",
            "  Filled 'freq6' using time-based methods (ffill/bfill)\n",
            "  freq7: 1 missing values (0.06%)\n",
            "  Filled 'freq7' using time-based methods (ffill/bfill)\n",
            "  freq8: 1 missing values (0.06%)\n",
            "  Filled 'freq8' using time-based methods (ffill/bfill)\n",
            "  iTLB_load_misses: 1 missing values (0.06%)\n",
            "  Filled 'iTLB_load_misses' using time-based methods (ffill/bfill)\n",
            "  instructions: 1 missing values (0.06%)\n",
            "  Filled 'instructions' using time-based methods (ffill/bfill)\n",
            "  kvm_entry: 1 missing values (0.06%)\n",
            "  Filled 'kvm_entry' using time-based methods (ffill/bfill)\n",
            "  kvm_exit: 1 missing values (0.06%)\n",
            "  Filled 'kvm_exit' using time-based methods (ffill/bfill)\n",
            "  kvm_vcpu_wakeup: 1 missing values (0.06%)\n",
            "  Filled 'kvm_vcpu_wakeup' using time-based methods (ffill/bfill)\n",
            "  load_1: 1 missing values (0.06%)\n",
            "  Filled 'load_1' using time-based methods (ffill/bfill)\n",
            "  load_15: 1 missing values (0.06%)\n",
            "  Filled 'load_15' using time-based methods (ffill/bfill)\n",
            "  load_5: 1 missing values (0.06%)\n",
            "  Filled 'load_5' using time-based methods (ffill/bfill)\n",
            "  mem_usage: 1 missing values (0.06%)\n",
            "  Filled 'mem_usage' using time-based methods (ffill/bfill)\n",
            "  page_faults: 1 missing values (0.06%)\n",
            "  Filled 'page_faults' using time-based methods (ffill/bfill)\n",
            "  temp1: 1 missing values (0.06%)\n",
            "  Filled 'temp1' using time-based methods (ffill/bfill)\n",
            "  temp2: 1 missing values (0.06%)\n",
            "  Filled 'temp2' using time-based methods (ffill/bfill)\n",
            "  temp3: 1 missing values (0.06%)\n",
            "  Filled 'temp3' using time-based methods (ffill/bfill)\n",
            "  temp4: 1 missing values (0.06%)\n",
            "  Filled 'temp4' using time-based methods (ffill/bfill)\n",
            "  temp5: 1 missing values (0.06%)\n",
            "  Filled 'temp5' using time-based methods (ffill/bfill)\n",
            "  temp6: 1 missing values (0.06%)\n",
            "  Filled 'temp6' using time-based methods (ffill/bfill)\n",
            "  temp7: 1 missing values (0.06%)\n",
            "  Filled 'temp7' using time-based methods (ffill/bfill)\n",
            "  temp8: 1 missing values (0.06%)\n",
            "  Filled 'temp8' using time-based methods (ffill/bfill)\n",
            "  temp9: 1 missing values (0.06%)\n",
            "  Filled 'temp9' using time-based methods (ffill/bfill)\n",
            "  total_vcpu: 1 missing values (0.06%)\n",
            "  Filled 'total_vcpu' using time-based methods (ffill/bfill)\n",
            "  total_vm: 1 missing values (0.06%)\n",
            "  Filled 'total_vm' using time-based methods (ffill/bfill)\n",
            "\n",
            "=== Detecting and handling outliers ===\n",
            "Checking 22 numeric columns for outliers\n",
            "  Detected 2 outliers (0.16%) in 'Chipset_Temp' using IQR method\n",
            "    Capped values from [48.0, 50.0] to [48.25, 50.0]\n",
            "  Detected 207 outliers (16.05%) in 'Cpu1_Temp' using IQR method\n",
            "    Warning: Not capping outliers in 'Cpu1_Temp' as they represent 16.05% of the data\n",
            "  Detected 1 outliers (0.08%) in 'FAN3' using IQR method\n",
            "    Capped values from [1679, 1711] to [1680, 1711]\n",
            "  Detected 175 outliers (13.57%) in 'PSU1_CIN' using IQR method\n",
            "    Warning: Not capping outliers in 'PSU1_CIN' as they represent 13.57% of the data\n",
            "  Detected 233 outliers (18.06%) in 'PSU1_FAN' using IQR method\n",
            "    Warning: Not capping outliers in 'PSU1_FAN' as they represent 18.06% of the data\n",
            "  Skipping 'PSU1_Inlet' - IQR is zero\n",
            "  Detected 64 outliers (4.96%) in 'PSU1_Total_Power' using IQR method\n",
            "    Capped values from [116, 133] to [118.5, 130.5]\n",
            "  Detected 4 outliers (0.31%) in 'PSU1_VIN' using IQR method\n",
            "    Capped values from [229.5, 233.25] to [229.75, 233.25]\n",
            "  Detected 233 outliers (18.06%) in 'PSU2_CIN' using IQR method\n",
            "    Warning: Not capping outliers in 'PSU2_CIN' as they represent 18.06% of the data\n",
            "  Detected 75 outliers (5.81%) in 'PSU2_Total_Power' using IQR method\n",
            "    Capped values from [122, 147] to [132, 147]\n",
            "  Detected 87 outliers (6.74%) in 'PSU2_VIN' using IQR method\n",
            "    Capped values from [229.5, 233.25] to [230.375, 233.25]\n",
            "Checking 48 numeric columns for outliers\n",
            "  Detected 19 outliers (1.11%) in 'L1_dcache_load_misses' using IQR method\n",
            "    Capped values from [3958692.0, 124480422.0] to [3958692.0, 40178767.25]\n",
            "  Detected 28 outliers (1.63%) in 'L1_dcache_loads' using IQR method\n",
            "    Capped values from [76407000.0, 2938219741.0] to [76407000.0, 993737595.25]\n",
            "  Detected 22 outliers (1.28%) in 'L1_dcache_store_misses' using IQR method\n",
            "    Capped values from [3472051.0, 128374693.0] to [3472051.0, 39158677.625]\n",
            "  Detected 26 outliers (1.51%) in 'L1_dcache_stores' using IQR method\n",
            "    Capped values from [69437978.0, 3070048872.0] to [69437978.0, 973470851.75]\n",
            "  Detected 65 outliers (3.78%) in 'L1_icache_load_misses' using IQR method\n",
            "    Capped values from [2644511.0, 196907308.0] to [2644511.0, 51271635.75]\n",
            "  Detected 34 outliers (1.98%) in 'L1_icache_loads' using IQR method\n",
            "    Capped values from [102177988.0, 3732664222.0] to [102177988.0, 1201122285.0]\n",
            "  Detected 22 outliers (1.28%) in 'branch_misses' using IQR method\n",
            "    Capped values from [1103808.0, 50222382.0] to [1103808.0, 19219085.25]\n",
            "  Detected 40 outliers (2.33%) in 'bus_cycles' using IQR method\n",
            "    Capped values from [1094396064.0, 46377015445.0] to [1094396064.0, 6970128473.75]\n",
            "  Detected 19 outliers (1.11%) in 'cache_misses' using IQR method\n",
            "    Capped values from [3991851.0, 124362397.0] to [3991851.0, 39709687.875]\n",
            "  Detected 31 outliers (1.80%) in 'cache_references' using IQR method\n",
            "    Capped values from [84057488.0, 2878377739.0] to [84057488.0, 975977505.5]\n",
            "  Detected 174 outliers (10.13%) in 'cpu_migrations' using IQR method\n",
            "    Warning: Not capping outliers in 'cpu_migrations' as they represent 10.13% of the data\n",
            "  Detected 333 outliers (19.38%) in 'cpu_usage' using IQR method\n",
            "    Warning: Not capping outliers in 'cpu_usage' as they represent 19.38% of the data\n",
            "  Detected 287 outliers (16.71%) in 'ctx_switches' using IQR method\n",
            "    Warning: Not capping outliers in 'ctx_switches' as they represent 16.71% of the data\n",
            "  Detected 36 outliers (2.10%) in 'cycles' using IQR method\n",
            "    Capped values from [1216232267.0, 45982710309.0] to [1216232267.0, 7051260632.75]\n",
            "  Detected 16 outliers (0.93%) in 'dTLB_load_misses' using IQR method\n",
            "    Capped values from [560027.0, 42696763.0] to [560027.0, 13932966.125]\n",
            "  Skipping 'flag1' - zero variance\n",
            "  Skipping 'flag2' - zero variance\n",
            "  Skipping 'flag3' - zero variance\n",
            "  Skipping 'flag4' - zero variance\n",
            "  Skipping 'freq1' - IQR is zero\n",
            "  Skipping 'freq2' - IQR is zero\n",
            "  Skipping 'freq3' - IQR is zero\n",
            "  Skipping 'freq4' - IQR is zero\n",
            "  Skipping 'freq5' - IQR is zero\n",
            "  Skipping 'freq6' - IQR is zero\n",
            "  Skipping 'freq7' - IQR is zero\n",
            "  Skipping 'freq8' - IQR is zero\n",
            "  Detected 98 outliers (5.70%) in 'iTLB_load_misses' using IQR method\n",
            "    Capped values from [74939.0, 2554155.0] to [74939.0, 840689.875]\n",
            "  Detected 28 outliers (1.63%) in 'instructions' using IQR method\n",
            "    Capped values from [193134480.0, 6737396438.0] to [193134480.0, 2352397084.625]\n",
            "  Detected 372 outliers (21.65%) in 'kvm_entry' using IQR method\n",
            "    Warning: Not capping outliers in 'kvm_entry' as they represent 21.65% of the data\n",
            "  Detected 371 outliers (21.59%) in 'kvm_exit' using IQR method\n",
            "    Warning: Not capping outliers in 'kvm_exit' as they represent 21.59% of the data\n",
            "  Detected 97 outliers (5.65%) in 'kvm_vcpu_wakeup' using IQR method\n",
            "    Capped values from [0.0, 9473.0] to [106.375, 441.375]\n",
            "  Detected 326 outliers (18.98%) in 'load_1' using IQR method\n",
            "    Warning: Not capping outliers in 'load_1' as they represent 18.98% of the data\n",
            "  Detected 369 outliers (21.48%) in 'load_15' using IQR method\n",
            "    Warning: Not capping outliers in 'load_15' as they represent 21.48% of the data\n",
            "  Detected 343 outliers (19.97%) in 'load_5' using IQR method\n",
            "    Warning: Not capping outliers in 'load_5' as they represent 19.97% of the data\n",
            "  Detected 105 outliers (6.11%) in 'mem_usage' using IQR method\n",
            "    Capped values from [4.73, 32.77] to [32.510000000000005, 32.749999999999986]\n",
            "  Detected 233 outliers (13.56%) in 'page_faults' using IQR method\n",
            "    Warning: Not capping outliers in 'page_faults' as they represent 13.56% of the data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-2ec1480bad37>:193: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[col] = df[col].fillna(method='ffill').fillna(method='bfill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Detected 232 outliers (13.50%) in 'temp1' using IQR method\n",
            "    Warning: Not capping outliers in 'temp1' as they represent 13.50% of the data\n",
            "  Detected 152 outliers (8.85%) in 'temp2' using IQR method\n",
            "    Capped values from [54.2, 60.9] to [55.349999999999994, 60.55000000000001]\n",
            "  Detected 210 outliers (12.22%) in 'temp3' using IQR method\n",
            "    Warning: Not capping outliers in 'temp3' as they represent 12.22% of the data\n",
            "  Detected 157 outliers (9.14%) in 'temp4' using IQR method\n",
            "    Capped values from [54.3, 60.6] to [55.44999999999999, 60.6]\n",
            "  Detected 179 outliers (10.42%) in 'temp5' using IQR method\n",
            "    Warning: Not capping outliers in 'temp5' as they represent 10.42% of the data\n",
            "  Detected 89 outliers (5.18%) in 'temp6' using IQR method\n",
            "    Capped values from [54.3, 60.9] to [55.3, 60.89999999999999]\n",
            "  Detected 229 outliers (13.33%) in 'temp7' using IQR method\n",
            "    Warning: Not capping outliers in 'temp7' as they represent 13.33% of the data\n",
            "  Detected 65 outliers (3.78%) in 'temp8' using IQR method\n",
            "    Capped values from [54.9, 61.6] to [55.65, 61.6]\n",
            "  Skipping 'temp9' - IQR is zero\n",
            "  Skipping 'total_vcpu' - IQR is zero\n",
            "  Skipping 'total_vm' - IQR is zero\n",
            "\n",
            "=== Feature selection based on variance and correlation ===\n",
            "Low variance features (< 0.01):\n",
            "  PSU1_CIN: variance = 0.000176\n",
            "  PSU2_CIN: variance = 0.000341\n",
            "\n",
            "Highly correlated features (> 0.95):\n",
            "  PSU1_Total_Power and PSU1_CIN: correlation = 0.9595\n",
            "  PSU2_VIN and PSU1_VIN: correlation = 0.9662\n",
            "Low variance features (< 0.01):\n",
            "  flag1: variance = 0.000000\n",
            "  flag2: variance = 0.000000\n",
            "  flag3: variance = 0.000000\n",
            "  flag4: variance = 0.000000\n",
            "  freq1: variance = 0.000000\n",
            "  freq2: variance = 0.000000\n",
            "  freq3: variance = 0.000000\n",
            "  freq4: variance = 0.000000\n",
            "  freq5: variance = 0.000000\n",
            "  freq6: variance = 0.000000\n",
            "  freq7: variance = 0.000000\n",
            "  freq8: variance = 0.000000\n",
            "  mem_usage: variance = 0.002769\n",
            "\n",
            "Highly correlated features (> 0.95):\n",
            "  L1_dcache_loads and L1_dcache_load_misses: correlation = 0.9555\n",
            "  L1_dcache_store_misses and L1_dcache_load_misses: correlation = 0.9713\n",
            "  L1_dcache_stores and L1_dcache_loads: correlation = 0.9764\n",
            "  L1_dcache_stores and L1_dcache_store_misses: correlation = 0.9512\n",
            "  L1_icache_loads and L1_dcache_stores: correlation = 0.9766\n",
            "  branch_misses and L1_dcache_load_misses: correlation = 0.9537\n",
            "  cache_misses and L1_dcache_load_misses: correlation = 0.9816\n",
            "  cache_misses and branch_misses: correlation = 0.9736\n",
            "  cache_references and L1_dcache_load_misses: correlation = 0.9538\n",
            "  cache_references and L1_dcache_loads: correlation = 0.9849\n",
            "  cache_references and branch_misses: correlation = 0.9579\n",
            "  cache_references and cache_misses: correlation = 0.9545\n",
            "  cycles and bus_cycles: correlation = 0.9941\n",
            "  iTLB_load_misses and L1_icache_load_misses: correlation = 0.9628\n",
            "  instructions and cache_references: correlation = 0.9624\n",
            "  kvm_exit and kvm_entry: correlation = 1.0000\n",
            "  load_1 and cpu_usage: correlation = 0.9584\n",
            "  load_15 and load_1: correlation = 0.9530\n",
            "  load_5 and load_1: correlation = 0.9740\n",
            "  load_5 and load_15: correlation = 0.9932\n",
            "  temp2 and temp1: correlation = 0.9553\n",
            "  temp3 and temp1: correlation = 0.9694\n",
            "  temp4 and temp3: correlation = 0.9625\n",
            "  temp5 and temp1: correlation = 0.9781\n",
            "  temp5 and temp3: correlation = 0.9660\n",
            "  temp6 and temp5: correlation = 0.9549\n",
            "  temp7 and temp1: correlation = 0.9734\n",
            "  temp7 and temp3: correlation = 0.9767\n",
            "  temp7 and temp5: correlation = 0.9750\n",
            "  temp8 and temp5: correlation = 0.9508\n",
            "  temp8 and temp7: correlation = 0.9612\n",
            "  total_vm and total_vcpu: correlation = 1.0000\n",
            "\n",
            "=== Saving processed datasets ===\n",
            "\n",
            "Processing complete. Datasets saved as BMC_processed.csv and host_processed.csv\n",
            "\n",
            "=== Final Dataset Summary ===\n",
            "BMC dataset: 1290 rows, 24 columns\n",
            "Host dataset: 1718 rows, 50 columns\n",
            "\n",
            "BMC Columns:\n",
            "  _time (datetime64[ns])\n",
            "  _measurement (float64)\n",
            "  device_id (object)\n",
            "  Chipset2_Temp (float64)\n",
            "  Chipset_Temp (float64)\n",
            "  Cpu1_Temp (float64)\n",
            "  Cpu2_Temp (float64)\n",
            "  FAN1 (int64)\n",
            "  FAN2 (int64)\n",
            "  FAN3 (int64)\n",
            "  FAN4 (int64)\n",
            "  IO_Outlet_Temp (float64)\n",
            "  Inlet_Temp (float64)\n",
            "  Outlet_Temp (float64)\n",
            "  PSU1_CIN (float64)\n",
            "  PSU1_FAN (int64)\n",
            "  PSU1_Inlet (int64)\n",
            "  PSU1_Total_Power (float64)\n",
            "  PSU1_VIN (float64)\n",
            "  PSU2_CIN (float64)\n",
            "  PSU2_FAN (int64)\n",
            "  PSU2_Inlet (int64)\n",
            "  PSU2_Total_Power (int64)\n",
            "  PSU2_VIN (float64)\n",
            "\n",
            "Host Columns:\n",
            "  _time (datetime64[ns])\n",
            "  device_id (object)\n",
            "  L1_dcache_load_misses (float64)\n",
            "  L1_dcache_loads (float64)\n",
            "  L1_dcache_store_misses (float64)\n",
            "  L1_dcache_stores (float64)\n",
            "  L1_icache_load_misses (float64)\n",
            "  L1_icache_loads (float64)\n",
            "  branch_misses (float64)\n",
            "  bus_cycles (float64)\n",
            "  cache_misses (float64)\n",
            "  cache_references (float64)\n",
            "  cpu_migrations (float64)\n",
            "  cpu_usage (float64)\n",
            "  ctx_switches (float64)\n",
            "  cycles (float64)\n",
            "  dTLB_load_misses (float64)\n",
            "  flag1 (float64)\n",
            "  flag2 (float64)\n",
            "  flag3 (float64)\n",
            "  flag4 (float64)\n",
            "  freq1 (float64)\n",
            "  freq2 (float64)\n",
            "  freq3 (float64)\n",
            "  freq4 (float64)\n",
            "  freq5 (float64)\n",
            "  freq6 (float64)\n",
            "  freq7 (float64)\n",
            "  freq8 (float64)\n",
            "  iTLB_load_misses (float64)\n",
            "  instructions (float64)\n",
            "  kvm_entry (float64)\n",
            "  kvm_exit (float64)\n",
            "  kvm_vcpu_wakeup (float64)\n",
            "  load_1 (float64)\n",
            "  load_15 (float64)\n",
            "  load_5 (float64)\n",
            "  mem_usage (float64)\n",
            "  page_faults (float64)\n",
            "  temp1 (float64)\n",
            "  temp2 (float64)\n",
            "  temp3 (float64)\n",
            "  temp4 (float64)\n",
            "  temp5 (float64)\n",
            "  temp6 (float64)\n",
            "  temp7 (float64)\n",
            "  temp8 (float64)\n",
            "  temp9 (float64)\n",
            "  total_vcpu (float64)\n",
            "  total_vm (float64)\n",
            "\n",
            "Saved BMC correlation heatmap as bmc_correlation_heatmap.png\n",
            "Saved host correlation heatmap as host_correlation_heatmap.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load processed datasets\n",
        "print(\"Loading processed datasets...\")\n",
        "dfBMC = pd.read_csv('BMC_processed.csv')\n",
        "dfhost = pd.read_csv('host_processed.csv')\n",
        "\n",
        "# Ensure time columns are datetime\n",
        "dfBMC['_time'] = pd.to_datetime(dfBMC['_time'])\n",
        "dfhost['_time'] = pd.to_datetime(dfhost['_time'])\n",
        "\n",
        "# ===== 1. TEMPORAL FEATURE EXTRACTION =====\n",
        "print(\"\\n=== Creating temporal features ===\")\n",
        "\n",
        "# Function to create time-based features\n",
        "def create_temporal_features(df):\n",
        "    # Ensure we're working with datetime\n",
        "    if '_time' not in df.columns:\n",
        "        print(\"Error: No time column found\")\n",
        "        return df\n",
        "\n",
        "    # Make sure the time column is datetime\n",
        "    df['_time'] = pd.to_datetime(df['_time'])\n",
        "\n",
        "    # Extract basic time components\n",
        "    df['hour'] = df['_time'].dt.hour\n",
        "    df['day'] = df['_time'].dt.day\n",
        "    df['weekday'] = df['_time'].dt.weekday\n",
        "    df['is_weekend'] = df['weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "    # Sort by time to ensure proper sequence\n",
        "    df = df.sort_values('_time')\n",
        "\n",
        "    # Add a row identifier for sequential operations\n",
        "    df['row_id'] = range(len(df))\n",
        "\n",
        "    # Create time difference features (how much time has passed since the last reading)\n",
        "    df['time_diff'] = df['_time'].diff().dt.total_seconds().fillna(0)\n",
        "\n",
        "    print(f\"Created temporal features: hour, day, weekday, is_weekend, time_diff\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply temporal feature creation to both datasets\n",
        "dfBMC = create_temporal_features(dfBMC)\n",
        "dfhost = create_temporal_features(dfhost)\n",
        "\n",
        "# ===== 2. ROLLING WINDOW STATISTICS =====\n",
        "print(\"\\n=== Creating rolling window features ===\")\n",
        "\n",
        "# Function to create rolling window features for numeric columns\n",
        "def create_rolling_features(df, window_sizes=[5, 10], exclude_cols=None):\n",
        "    if exclude_cols is None:\n",
        "        exclude_cols = ['_time', 'device_id', 'hour', 'day', 'weekday', 'is_weekend', 'row_id', 'time_diff']\n",
        "\n",
        "    # Ensure data is sorted by time\n",
        "    df = df.sort_values('_time')\n",
        "\n",
        "    # Get numeric columns\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "    numeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
        "\n",
        "    # Generate rolling window features for each numeric column\n",
        "    for col in numeric_cols:\n",
        "        for window in window_sizes:\n",
        "            # Create rolling mean\n",
        "            df[f'{col}_roll_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "            # Create rolling std\n",
        "            df[f'{col}_roll_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n",
        "\n",
        "            # Create z-score relative to rolling window (how many std deviations from rolling mean)\n",
        "            mean_col = f'{col}_roll_mean_{window}'\n",
        "            std_col = f'{col}_roll_std_{window}'\n",
        "            df[f'{col}_zscore_{window}'] = (df[col] - df[mean_col]) / df[std_col].replace(0, 1)  # Avoid div by zero\n",
        "\n",
        "        print(f\"Created rolling features for {col} with windows {window_sizes}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Create rolling features for key metrics\n",
        "# Select only the most important columns to avoid feature explosion\n",
        "bmc_key_metrics = [\n",
        "    'Cpu1_Temp', 'Cpu2_Temp', 'FAN1', 'FAN2', 'FAN3', 'FAN4',\n",
        "    'PSU1_Total_Power', 'PSU2_Total_Power'\n",
        "]\n",
        "\n",
        "host_key_metrics = [\n",
        "    'cpu_usage', 'mem_usage', 'load_1', 'load_5', 'load_15',\n",
        "    'cache_misses', 'L1_dcache_load_misses'\n",
        "]\n",
        "\n",
        "# Apply rolling features to selected columns\n",
        "for col in bmc_key_metrics:\n",
        "    if col in dfBMC.columns:\n",
        "        # Create features for just this column\n",
        "        exclude_cols = ['_time', 'device_id', 'hour', 'day', 'weekday', 'is_weekend', 'row_id', 'time_diff']\n",
        "        temp_cols = [c for c in dfBMC.columns if c != col and c not in exclude_cols]\n",
        "        exclude_cols.extend(temp_cols)\n",
        "\n",
        "        dfBMC = create_rolling_features(dfBMC, window_sizes=[5, 10], exclude_cols=exclude_cols)\n",
        "\n",
        "for col in host_key_metrics:\n",
        "    if col in dfhost.columns:\n",
        "        # Create features for just this column\n",
        "        exclude_cols = ['_time', 'device_id', 'hour', 'day', 'weekday', 'is_weekend', 'row_id', 'time_diff']\n",
        "        temp_cols = [c for c in dfhost.columns if c != col and c not in exclude_cols]\n",
        "        exclude_cols.extend(temp_cols)\n",
        "\n",
        "        dfhost = create_rolling_features(dfhost, window_sizes=[5, 10], exclude_cols=exclude_cols)\n",
        "\n",
        "# ===== 3. RATE OF CHANGE FEATURES =====\n",
        "print(\"\\n=== Creating rate of change features ===\")\n",
        "\n",
        "# Function to create rate of change features\n",
        "def create_rate_of_change(df, metrics, periods=[1, 3]):\n",
        "    for col in metrics:\n",
        "        if col in df.columns:\n",
        "            for period in periods:\n",
        "                # Percentage change\n",
        "                df[f'{col}_pct_change_{period}'] = df[col].pct_change(periods=period).fillna(0)\n",
        "\n",
        "                # Absolute diff (can be useful when the value can be negative)\n",
        "                df[f'{col}_diff_{period}'] = df[col].diff(periods=period).fillna(0)\n",
        "\n",
        "            print(f\"Created rate of change features for {col}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply rate of change to key metrics\n",
        "dfBMC = create_rate_of_change(dfBMC, bmc_key_metrics)\n",
        "dfhost = create_rate_of_change(dfhost, host_key_metrics)\n",
        "\n",
        "# ===== 4. DATA SCALING =====\n",
        "print(\"\\n=== Scaling features ===\")\n",
        "\n",
        "# Function to scale numeric features\n",
        "def scale_features(df, scaler_type='standard', exclude_cols=None):\n",
        "    if exclude_cols is None:\n",
        "        exclude_cols = ['_time', 'device_id', 'hour', 'day', 'weekday', 'is_weekend', 'row_id']\n",
        "\n",
        "    # Select numeric columns for scaling\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "    numeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
        "\n",
        "    # Create a copy of the dataframe for scaled values\n",
        "    df_scaled = df.copy()\n",
        "\n",
        "    # Initialize the appropriate scaler\n",
        "    if scaler_type == 'standard':\n",
        "        scaler = StandardScaler()\n",
        "    elif scaler_type == 'minmax':\n",
        "        scaler = MinMaxScaler()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown scaler type: {scaler_type}\")\n",
        "\n",
        "    # Fit and transform the selected columns\n",
        "    if numeric_cols:\n",
        "        df_scaled[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "        print(f\"Scaled {len(numeric_cols)} numeric columns using {scaler_type} scaling\")\n",
        "    else:\n",
        "        print(\"No numeric columns found for scaling\")\n",
        "\n",
        "    return df_scaled, scaler\n",
        "\n",
        "# Scale features in both datasets\n",
        "dfBMC_scaled, bmc_scaler = scale_features(dfBMC)\n",
        "dfhost_scaled, host_scaler = scale_features(dfhost)\n",
        "\n",
        "# ===== 5. FEATURE CORRELATION ANALYSIS =====\n",
        "print(\"\\n=== Analyzing feature correlations ===\")\n",
        "\n",
        "# Function to find and visualize highly correlated features\n",
        "def analyze_correlations(df, threshold=0.9, exclude_cols=None, title=\"Feature Correlation Analysis\"):\n",
        "    if exclude_cols is None:\n",
        "        exclude_cols = ['_time', 'device_id', 'row_id']\n",
        "\n",
        "    # Select numeric columns\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "    numeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
        "\n",
        "    # Calculate correlation matrix\n",
        "    corr = df[numeric_cols].corr()\n",
        "\n",
        "    # Create a mask for the upper triangle\n",
        "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "    # Find highly correlated pairs\n",
        "    pairs = []\n",
        "    for i in range(len(corr.columns)):\n",
        "        for j in range(i+1, len(corr.columns)):\n",
        "            if abs(corr.iloc[i, j]) >= threshold:\n",
        "                pairs.append({\n",
        "                    'feature1': corr.columns[i],\n",
        "                    'feature2': corr.columns[j],\n",
        "                    'correlation': corr.iloc[i, j]\n",
        "                })\n",
        "\n",
        "    # Sort by absolute correlation (highest first)\n",
        "    pairs = sorted(pairs, key=lambda x: abs(x['correlation']), reverse=True)\n",
        "\n",
        "    # Print top highly correlated pairs\n",
        "    print(f\"\\nHighly correlated features (|correlation| >= {threshold}):\")\n",
        "    for pair in pairs[:10]:  # Show top 10\n",
        "        print(f\"  {pair['feature1']} & {pair['feature2']}: {pair['correlation']:.4f}\")\n",
        "\n",
        "    # Return pairs for further processing\n",
        "    return pairs\n",
        "\n",
        "# Analyze correlations in both datasets\n",
        "bmc_correlations = analyze_correlations(dfBMC_scaled, title=\"BMC Feature Correlations\")\n",
        "host_correlations = analyze_correlations(dfhost_scaled, title=\"Host Feature Correlations\")\n",
        "\n",
        "# ===== 6. MERGE DATASETS (OPTIONAL) =====\n",
        "print(\"\\n=== Merging datasets (optional) ===\")\n",
        "\n",
        "# Function to merge BMC and host datasets on time\n",
        "def merge_datasets(df_bmc, df_host, time_tolerance=pd.Timedelta(minutes=1)):\n",
        "    # Ensure time columns are datetime\n",
        "    df_bmc['_time'] = pd.to_datetime(df_bmc['_time'])\n",
        "    df_host['_time'] = pd.to_datetime(df_host['_time'])\n",
        "\n",
        "    # Get list of columns with same name in both datasets (aside from _time and device_id)\n",
        "    common_cols = set(df_bmc.columns) & set(df_host.columns) - {'_time', 'device_id'}\n",
        "\n",
        "    if common_cols:\n",
        "        print(f\"Found common columns in both datasets: {common_cols}\")\n",
        "        # Rename common columns in the host dataset to avoid conflicts\n",
        "        rename_dict = {col: f'host_{col}' for col in common_cols}\n",
        "        df_host = df_host.rename(columns=rename_dict)\n",
        "\n",
        "    # Option 1: Merge on exact timestamp (if available)\n",
        "    if len(set(df_bmc['_time']) & set(df_host['_time'])) > 0:\n",
        "        print(\"Merging on exact timestamps\")\n",
        "        merged_df = pd.merge(df_bmc, df_host, on='_time', suffixes=('_bmc', '_host'))\n",
        "\n",
        "    # Option 2: Use nearest time approach\n",
        "    else:\n",
        "        print(f\"Using nearest time merge with tolerance {time_tolerance}\")\n",
        "        # Sort both dataframes by time\n",
        "        df_bmc = df_bmc.sort_values('_time')\n",
        "        df_host = df_host.sort_values('_time')\n",
        "\n",
        "        # Create a list to store merged rows\n",
        "        merged_rows = []\n",
        "\n",
        "        # For each row in BMC, find the closest host row\n",
        "        for _, bmc_row in df_bmc.iterrows():\n",
        "            bmc_time = bmc_row['_time']\n",
        "\n",
        "            # Find closest host row within tolerance\n",
        "            time_diffs = (df_host['_time'] - bmc_time).abs()\n",
        "            closest_idx = time_diffs.idxmin()\n",
        "\n",
        "            if time_diffs[closest_idx] <= time_tolerance:\n",
        "                host_row = df_host.loc[closest_idx]\n",
        "\n",
        "                # Combine the rows\n",
        "                merged_row = {**bmc_row.to_dict(), **host_row.to_dict()}\n",
        "                # Fix the _time (use BMC time)\n",
        "                merged_row['_time'] = bmc_time\n",
        "                # Rename device_id from host\n",
        "                if 'device_id' in host_row:\n",
        "                    merged_row['device_id_host'] = host_row['device_id']\n",
        "\n",
        "                merged_rows.append(merged_row)\n",
        "\n",
        "        if merged_rows:\n",
        "            merged_df = pd.DataFrame(merged_rows)\n",
        "        else:\n",
        "            print(\"Warning: No matching rows found within the specified time tolerance\")\n",
        "            merged_df = None\n",
        "\n",
        "    if merged_df is not None:\n",
        "        print(f\"Successfully merged datasets: {merged_df.shape[0]} rows, {merged_df.shape[1]} columns\")\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "# Try to merge datasets (if they have overlapping time periods)\n",
        "try:\n",
        "    merged_df = merge_datasets(dfBMC, dfhost)\n",
        "\n",
        "    if merged_df is not None:\n",
        "        # Save merged dataset\n",
        "        merged_df.to_csv('merged_dataset.csv', index=False)\n",
        "        print(\"Saved merged dataset to merged_dataset.csv\")\n",
        "except Exception as e:\n",
        "    print(f\"Error merging datasets: {e}\")\n",
        "    print(\"Continuing with separate datasets for analysis\")\n",
        "    merged_df = None\n",
        "\n",
        "# ===== 7. SAVE ENGINEERED DATASETS =====\n",
        "print(\"\\n=== Saving engineered datasets ===\")\n",
        "\n",
        "# Save the engineered datasets\n",
        "dfBMC_scaled.to_csv('BMC_engineered.csv', index=False)\n",
        "dfhost_scaled.to_csv('host_engineered.csv', index=False)\n",
        "\n",
        "print(\"\\nFeature engineering complete.\")\n",
        "print(f\"BMC dataset: {dfBMC_scaled.shape[0]} rows, {dfBMC_scaled.shape[1]} columns\")\n",
        "print(f\"Host dataset: {dfhost_scaled.shape[0]} rows, {dfhost_scaled.shape[1]} columns\")\n",
        "if merged_df is not None:\n",
        "    print(f\"Merged dataset: {merged_df.shape[0]} rows, {merged_df.shape[1]} columns\")\n",
        "\n",
        "# Display feature counts by category\n",
        "bmc_feature_counts = {\n",
        "    'Original': len(set(dfBMC.columns) - set(['hour', 'day', 'weekday', 'is_weekend', 'row_id', 'time_diff'])),\n",
        "    'Temporal': 5,  # hour, day, weekday, is_weekend, time_diff\n",
        "    'Rolling': sum(1 for col in dfBMC_scaled.columns if ('roll_mean' in col or 'roll_std' in col or 'zscore' in col)),\n",
        "    'Rate of Change': sum(1 for col in dfBMC_scaled.columns if ('pct_change' in col or '_diff_' in col)),\n",
        "    'Total': dfBMC_scaled.shape[1]\n",
        "}\n",
        "\n",
        "host_feature_counts = {\n",
        "    'Original': len(set(dfhost.columns) - set(['hour', 'day', 'weekday', 'is_weekend', 'row_id', 'time_diff'])),\n",
        "    'Temporal': 5,  # hour, day, weekday, is_weekend, time_diff\n",
        "    'Rolling': sum(1 for col in dfhost_scaled.columns if ('roll_mean' in col or 'roll_std' in col or 'zscore' in col)),\n",
        "    'Rate of Change': sum(1 for col in dfhost_scaled.columns if ('pct_change' in col or '_diff_' in col)),\n",
        "    'Total': dfhost_scaled.shape[1]\n",
        "}\n",
        "\n",
        "print(\"\\nBMC Feature Counts:\")\n",
        "for category, count in bmc_feature_counts.items():\n",
        "    print(f\"  {category}: {count}\")\n",
        "\n",
        "print(\"\\nHost Feature Counts:\")\n",
        "for category, count in host_feature_counts.items():\n",
        "    print(f\"  {category}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlQfKbQBpcou",
        "outputId": "cc248e3a-14e4-489e-f8b4-5c8b94627283"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading processed datasets...\n",
            "\n",
            "=== Creating temporal features ===\n",
            "Created temporal features: hour, day, weekday, is_weekend, time_diff\n",
            "Created temporal features: hour, day, weekday, is_weekend, time_diff\n",
            "\n",
            "=== Creating rolling window features ===\n",
            "Created rolling features for Cpu1_Temp with windows [5, 10]\n",
            "Created rolling features for Cpu2_Temp with windows [5, 10]\n",
            "Created rolling features for FAN1 with windows [5, 10]\n",
            "Created rolling features for FAN2 with windows [5, 10]\n",
            "Created rolling features for FAN3 with windows [5, 10]\n",
            "Created rolling features for FAN4 with windows [5, 10]\n",
            "Created rolling features for PSU1_Total_Power with windows [5, 10]\n",
            "Created rolling features for PSU2_Total_Power with windows [5, 10]\n",
            "Created rolling features for cpu_usage with windows [5, 10]\n",
            "Created rolling features for mem_usage with windows [5, 10]\n",
            "Created rolling features for load_1 with windows [5, 10]\n",
            "Created rolling features for load_5 with windows [5, 10]\n",
            "Created rolling features for load_15 with windows [5, 10]\n",
            "Created rolling features for cache_misses with windows [5, 10]\n",
            "Created rolling features for L1_dcache_load_misses with windows [5, 10]\n",
            "\n",
            "=== Creating rate of change features ===\n",
            "Created rate of change features for Cpu1_Temp\n",
            "Created rate of change features for Cpu2_Temp\n",
            "Created rate of change features for FAN1\n",
            "Created rate of change features for FAN2\n",
            "Created rate of change features for FAN3\n",
            "Created rate of change features for FAN4\n",
            "Created rate of change features for PSU1_Total_Power\n",
            "Created rate of change features for PSU2_Total_Power\n",
            "Created rate of change features for cpu_usage\n",
            "Created rate of change features for mem_usage\n",
            "Created rate of change features for load_1\n",
            "Created rate of change features for load_5\n",
            "Created rate of change features for load_15\n",
            "Created rate of change features for cache_misses\n",
            "Created rate of change features for L1_dcache_load_misses\n",
            "\n",
            "=== Scaling features ===\n",
            "Scaled 103 numeric columns using standard scaling\n",
            "Scaled 119 numeric columns using standard scaling\n",
            "\n",
            "=== Analyzing feature correlations ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
            "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
            "  T = new_sum / new_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
            "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Highly correlated features (|correlation| >= 0.9):\n",
            "  FAN3_pct_change_1 & FAN3_diff_1: 1.0000\n",
            "  FAN2_pct_change_1 & FAN2_diff_1: 1.0000\n",
            "  FAN3_pct_change_3 & FAN3_diff_3: 1.0000\n",
            "  FAN2_pct_change_3 & FAN2_diff_3: 1.0000\n",
            "  FAN4_pct_change_1 & FAN4_diff_1: 1.0000\n",
            "  FAN4_pct_change_3 & FAN4_diff_3: 1.0000\n",
            "  FAN1_pct_change_3 & FAN1_diff_3: 1.0000\n",
            "  FAN1_pct_change_1 & FAN1_diff_1: 1.0000\n",
            "  Cpu2_Temp_pct_change_1 & Cpu2_Temp_diff_1: 0.9999\n",
            "  Cpu2_Temp_pct_change_3 & Cpu2_Temp_diff_3: 0.9999\n",
            "\n",
            "Highly correlated features (|correlation| >= 0.9):\n",
            "  total_vcpu & total_vm: 1.0000\n",
            "  mem_usage_pct_change_1 & mem_usage_diff_1: 1.0000\n",
            "  mem_usage_pct_change_3 & mem_usage_diff_3: 1.0000\n",
            "  kvm_entry & kvm_exit: 1.0000\n",
            "  load_15 & load_15_roll_mean_5: 1.0000\n",
            "  load_15_roll_mean_5 & load_15_roll_mean_10: 0.9999\n",
            "  load_15 & load_15_roll_mean_10: 0.9998\n",
            "  load_5 & load_5_roll_mean_5: 0.9998\n",
            "  load_5_roll_mean_5 & load_5_roll_mean_10: 0.9997\n",
            "  load_5 & load_5_roll_mean_10: 0.9992\n",
            "\n",
            "=== Merging datasets (optional) ===\n",
            "Found common columns in both datasets: {'hour', 'time_diff', 'day', 'weekday', 'row_id', 'is_weekend'}\n",
            "Merging on exact timestamps\n",
            "Successfully merged datasets: 153 rows, 235 columns\n",
            "Saved merged dataset to merged_dataset.csv\n",
            "\n",
            "=== Saving engineered datasets ===\n",
            "\n",
            "Feature engineering complete.\n",
            "BMC dataset: 1290 rows, 110 columns\n",
            "Host dataset: 1718 rows, 126 columns\n",
            "Merged dataset: 153 rows, 235 columns\n",
            "\n",
            "BMC Feature Counts:\n",
            "  Original: 104\n",
            "  Temporal: 5\n",
            "  Rolling: 48\n",
            "  Rate of Change: 32\n",
            "  Total: 110\n",
            "\n",
            "Host Feature Counts:\n",
            "  Original: 120\n",
            "  Temporal: 5\n",
            "  Rolling: 42\n",
            "  Rate of Change: 28\n",
            "  Total: 126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Load engineered datasets\n",
        "print(\"Loading engineered datasets...\")\n",
        "try:\n",
        "    # Try to load the datasets - adjust paths if needed\n",
        "    dfBMC = pd.read_csv('BMC_engineered.csv')\n",
        "    dfhost = pd.read_csv('host_engineered.csv')\n",
        "    print(\"Loaded engineered datasets\")\n",
        "\n",
        "    # Also check if merged dataset exists\n",
        "    try:\n",
        "        merged_df = pd.read_csv('merged_dataset.csv')\n",
        "        print(f\"Loaded merged dataset: {merged_df.shape[0]} rows, {merged_df.shape[1]} columns\")\n",
        "        use_merged = True\n",
        "    except:\n",
        "        print(\"No merged dataset found\")\n",
        "        use_merged = False\n",
        "except:\n",
        "    print(\"Engineered datasets not found, loading processed datasets...\")\n",
        "    try:\n",
        "        dfBMC = pd.read_csv('BMC_processed.csv')\n",
        "        dfhost = pd.read_csv('host_processed.csv')\n",
        "        print(\"Loaded processed datasets\")\n",
        "        use_merged = False\n",
        "    except:\n",
        "        print(\"ERROR: Could not load datasets. Please run preprocessing first.\")\n",
        "        raise\n",
        "\n",
        "# ===== 1. PREPARE DATA FOR MODELING - WITH IMPROVED NAN HANDLING =====\n",
        "print(\"\\n=== Preparing data for modeling ===\")\n",
        "\n",
        "def prepare_data_for_modeling(df, exclude_cols=None):\n",
        "    \"\"\"Prepare data for modeling with thorough NaN handling\"\"\"\n",
        "    if exclude_cols is None:\n",
        "        exclude_cols = ['_time', 'device_id', 'row_id']\n",
        "\n",
        "    # Drop non-feature columns\n",
        "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "    # Ensure all columns are numeric\n",
        "    X = df[feature_cols].select_dtypes(include=['number'])\n",
        "\n",
        "    # Report on NaN values before handling\n",
        "    nan_count_before = X.isna().sum().sum()\n",
        "    if nan_count_before > 0:\n",
        "        print(f\"Found {nan_count_before} NaN values across {X.shape[1]} features\")\n",
        "\n",
        "    # Handle missing values by column\n",
        "    for col in X.columns:\n",
        "        nan_count = X[col].isna().sum()\n",
        "        if nan_count > 0:\n",
        "            # If many NaN values, consider dropping the column\n",
        "            if nan_count / len(X) > 0.5:  # More than 50% missing\n",
        "                print(f\"  Column '{col}' has {nan_count} NaNs ({nan_count/len(X)*100:.1f}%) - consider dropping\")\n",
        "\n",
        "            # Fill missing values with column mean\n",
        "            col_mean = X[col].mean()\n",
        "            if pd.isna(col_mean):  # If mean is also NaN (all values are NaN)\n",
        "                print(f\"  Column '{col}' has all NaN values - filling with 0\")\n",
        "                X[col] = X[col].fillna(0)\n",
        "            else:\n",
        "                X[col] = X[col].fillna(col_mean)\n",
        "                print(f\"  Filled {nan_count} NaNs in '{col}' with mean: {col_mean:.4f}\")\n",
        "\n",
        "    # Verify no NaNs remain and report\n",
        "    nan_count_after = X.isna().sum().sum()\n",
        "    if nan_count_after > 0:\n",
        "        print(f\"WARNING: {nan_count_after} NaN values remain - replacing with 0\")\n",
        "        X = X.fillna(0)  # Last resort\n",
        "    else:\n",
        "        print(f\"Successfully handled all NaN values. Dataset shape: {X.shape}\")\n",
        "\n",
        "    return X, X.columns.tolist()\n",
        "\n",
        "# Prepare each dataset with improved NaN handling\n",
        "X_bmc, bmc_features = prepare_data_for_modeling(dfBMC)\n",
        "X_host, host_features = prepare_data_for_modeling(dfhost)\n",
        "\n",
        "if use_merged:\n",
        "    X_merged, merged_features = prepare_data_for_modeling(merged_df)\n",
        "\n",
        "# ===== 2. ISOLATION FOREST MODEL =====\n",
        "print(\"\\n=== Isolation Forest Model ===\")\n",
        "\n",
        "def train_isolation_forest(X, df, contamination=0.05, n_estimators=100, random_state=42):\n",
        "    \"\"\"Train Isolation Forest with error handling\"\"\"\n",
        "    try:\n",
        "        if X.shape[0] < 10:\n",
        "            print(\"Too few samples for Isolation Forest\")\n",
        "            return None, df, None, np.zeros(len(df))\n",
        "\n",
        "        # Double check for NaNs\n",
        "        if X.isna().sum().sum() > 0:\n",
        "            print(f\"WARNING: {X.isna().sum().sum()} NaN values detected. Filling with 0.\")\n",
        "            X = X.fillna(0)\n",
        "\n",
        "        # Train model\n",
        "        model = IsolationForest(\n",
        "            n_estimators=n_estimators,\n",
        "            contamination=contamination,\n",
        "            random_state=random_state\n",
        "        )\n",
        "\n",
        "        # Fit and predict\n",
        "        model.fit(X)\n",
        "        scores = model.decision_function(X)\n",
        "        predictions = model.predict(X)\n",
        "\n",
        "        # Convert predictions to binary (1 = normal, -1 = anomaly)\n",
        "        # Also convert to 0 = normal, 1 = anomaly for easier interpretation\n",
        "        anomaly_labels = np.where(predictions == -1, 1, 0)\n",
        "\n",
        "        # Add results to dataframe\n",
        "        results_df = df.copy()\n",
        "        results_df['anomaly_score'] = scores\n",
        "        results_df['is_anomaly'] = anomaly_labels\n",
        "\n",
        "        # Get anomaly statistics\n",
        "        anomaly_count = np.sum(anomaly_labels)\n",
        "        anomaly_percent = (anomaly_count / len(anomaly_labels)) * 100\n",
        "\n",
        "        print(f\"Isolation Forest detected {anomaly_count} anomalies ({anomaly_percent:.2f}%)\")\n",
        "\n",
        "        return model, results_df, scores, anomaly_labels\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Isolation Forest: {e}\")\n",
        "        return None, df, None, np.zeros(len(df))\n",
        "\n",
        "# Train Isolation Forest on BMC dataset\n",
        "iforest_bmc, results_bmc_if, scores_bmc_if, anomalies_bmc_if = train_isolation_forest(X_bmc, dfBMC)\n",
        "\n",
        "# Train Isolation Forest on host dataset\n",
        "iforest_host, results_host_if, scores_host_if, anomalies_host_if = train_isolation_forest(X_host, dfhost)\n",
        "\n",
        "if use_merged:\n",
        "    # Train Isolation Forest on merged dataset\n",
        "    iforest_merged, results_merged_if, scores_merged_if, anomalies_merged_if = train_isolation_forest(X_merged, merged_df)\n",
        "\n",
        "# ===== 3. ONE-CLASS SVM MODEL =====\n",
        "print(\"\\n=== One-Class SVM Model ===\")\n",
        "\n",
        "def train_ocsvm(X, df, nu=0.05, kernel='rbf'):\n",
        "    \"\"\"Train One-Class SVM with enhanced error handling\"\"\"\n",
        "    try:\n",
        "        if X.shape[0] < 10:\n",
        "            print(\"Too few samples for One-Class SVM\")\n",
        "            return None, df, None, np.zeros(len(df))\n",
        "\n",
        "        # Double check for NaNs\n",
        "        if X.isna().sum().sum() > 0:\n",
        "            print(f\"WARNING: {X.isna().sum().sum()} NaN values detected. Filling with 0.\")\n",
        "            X = X.fillna(0)\n",
        "\n",
        "        # Train model\n",
        "        model = OneClassSVM(\n",
        "            nu=nu,  # Similar to contamination\n",
        "            kernel=kernel\n",
        "        )\n",
        "\n",
        "        # Fit and predict\n",
        "        model.fit(X)\n",
        "        predictions = model.predict(X)\n",
        "        scores = model.decision_function(X)\n",
        "\n",
        "        # Convert predictions to binary (1 = normal, -1 = anomaly)\n",
        "        # Also convert to 0 = normal, 1 = anomaly for easier interpretation\n",
        "        anomaly_labels = np.where(predictions == -1, 1, 0)\n",
        "\n",
        "        # Add results to dataframe\n",
        "        results_df = df.copy()\n",
        "        results_df['anomaly_score'] = scores\n",
        "        results_df['is_anomaly'] = anomaly_labels\n",
        "\n",
        "        # Get anomaly statistics\n",
        "        anomaly_count = np.sum(anomaly_labels)\n",
        "        anomaly_percent = (anomaly_count / len(anomaly_labels)) * 100\n",
        "\n",
        "        print(f\"One-Class SVM detected {anomaly_count} anomalies ({anomaly_percent:.2f}%)\")\n",
        "\n",
        "        return model, results_df, scores, anomaly_labels\n",
        "    except Exception as e:\n",
        "        print(f\"Error training One-Class SVM: {e}\")\n",
        "        return None, df, None, np.zeros(len(df))\n",
        "\n",
        "# Train One-Class SVM on BMC dataset\n",
        "ocsvm_bmc, results_bmc_ocsvm, scores_bmc_ocsvm, anomalies_bmc_ocsvm = train_ocsvm(X_bmc, dfBMC)\n",
        "\n",
        "# Train One-Class SVM on host dataset\n",
        "ocsvm_host, results_host_ocsvm, scores_host_ocsvm, anomalies_host_ocsvm = train_ocsvm(X_host, dfhost)\n",
        "\n",
        "if use_merged:\n",
        "    # Train One-Class SVM on merged dataset\n",
        "    ocsvm_merged, results_merged_ocsvm, scores_merged_ocsvm, anomalies_merged_ocsvm = train_ocsvm(X_merged, merged_df)\n",
        "\n",
        "# ===== 4. LOCAL OUTLIER FACTOR MODEL =====\n",
        "print(\"\\n=== Local Outlier Factor Model ===\")\n",
        "\n",
        "def train_lof(X, df, n_neighbors=20, contamination=0.05):\n",
        "    \"\"\"Train Local Outlier Factor with enhanced error handling\"\"\"\n",
        "    try:\n",
        "        # Handle small dataset\n",
        "        if X.shape[0] < n_neighbors + 1:\n",
        "            print(f\"Warning: Sample size ({X.shape[0]}) too small for {n_neighbors} neighbors\")\n",
        "            n_neighbors = max(1, X.shape[0] // 2)\n",
        "            print(f\"Reducing to {n_neighbors} neighbors\")\n",
        "\n",
        "        # Double check for NaNs\n",
        "        if X.isna().sum().sum() > 0:\n",
        "            print(f\"WARNING: {X.isna().sum().sum()} NaN values detected. Filling with 0.\")\n",
        "            X = X.fillna(0)\n",
        "\n",
        "        # Train model\n",
        "        model = LocalOutlierFactor(\n",
        "            n_neighbors=n_neighbors,\n",
        "            contamination=contamination,\n",
        "            novelty=True\n",
        "        )\n",
        "\n",
        "        # Fit and predict\n",
        "        model.fit(X)\n",
        "        scores = -model.decision_function(X)  # Negative because higher means more anomalous\n",
        "        predictions = model.predict(X)\n",
        "\n",
        "        # Convert predictions to binary (1 = normal, -1 = anomaly)\n",
        "        # Also convert to 0 = normal, 1 = anomaly for easier interpretation\n",
        "        anomaly_labels = np.where(predictions == -1, 1, 0)\n",
        "\n",
        "        # Add results to dataframe\n",
        "        results_df = df.copy()\n",
        "        results_df['anomaly_score'] = scores\n",
        "        results_df['is_anomaly'] = anomaly_labels\n",
        "\n",
        "        # Get anomaly statistics\n",
        "        anomaly_count = np.sum(anomaly_labels)\n",
        "        anomaly_percent = (anomaly_count / len(anomaly_labels)) * 100\n",
        "\n",
        "        print(f\"Local Outlier Factor detected {anomaly_count} anomalies ({anomaly_percent:.2f}%)\")\n",
        "\n",
        "        return model, results_df, scores, anomaly_labels\n",
        "    except Exception as e:\n",
        "        print(f\"Error in LOF: {e}\")\n",
        "        return None, df, None, np.zeros(len(df))\n",
        "\n",
        "# Train LOF on BMC dataset\n",
        "lof_bmc, results_bmc_lof, scores_bmc_lof, anomalies_bmc_lof = train_lof(X_bmc, dfBMC)\n",
        "\n",
        "# Train LOF on host dataset\n",
        "lof_host, results_host_lof, scores_host_lof, anomalies_host_lof = train_lof(X_host, dfhost)\n",
        "\n",
        "if use_merged:\n",
        "    # Train LOF on merged dataset\n",
        "    lof_merged, results_merged_lof, scores_merged_lof, anomalies_merged_lof = train_lof(X_merged, merged_df)\n",
        "\n",
        "# ===== 5. DBSCAN CLUSTERING MODEL =====\n",
        "print(\"\\n=== DBSCAN Clustering Model ===\")\n",
        "\n",
        "def train_dbscan(X, df, eps=0.5, min_samples=5):\n",
        "    \"\"\"Train DBSCAN with enhanced error handling\"\"\"\n",
        "    try:\n",
        "        # Handle small dataset\n",
        "        if X.shape[0] < min_samples * 2:\n",
        "            print(f\"Warning: Sample size ({X.shape[0]}) too small for min_samples={min_samples}\")\n",
        "            min_samples = max(2, X.shape[0] // 5)\n",
        "            print(f\"Reducing to min_samples={min_samples}\")\n",
        "\n",
        "        # Double check for NaNs\n",
        "        if X.isna().sum().sum() > 0:\n",
        "            print(f\"WARNING: {X.isna().sum().sum()} NaN values detected. Filling with 0.\")\n",
        "            X = X.fillna(0)\n",
        "\n",
        "        # Train model\n",
        "        model = DBSCAN(\n",
        "            eps=eps,\n",
        "            min_samples=min_samples\n",
        "        )\n",
        "\n",
        "        # Fit and predict\n",
        "        clusters = model.fit_predict(X)\n",
        "\n",
        "        # In DBSCAN, -1 cluster is noise/anomalies\n",
        "        anomaly_labels = np.where(clusters == -1, 1, 0)\n",
        "\n",
        "        # Calculate anomaly scores (distance to nearest core point)\n",
        "        # This is a simplified version, as DBSCAN doesn't provide scores directly\n",
        "        scores = np.zeros(len(X))\n",
        "        for i, is_anomaly in enumerate(anomaly_labels):\n",
        "            if is_anomaly:\n",
        "                # For anomalies, use a high score\n",
        "                scores[i] = 1.0\n",
        "            else:\n",
        "                # For normal points, score is 0\n",
        "                scores[i] = 0.0\n",
        "\n",
        "        # Add results to dataframe\n",
        "        results_df = df.copy()\n",
        "        results_df['cluster'] = clusters\n",
        "        results_df['anomaly_score'] = scores\n",
        "        results_df['is_anomaly'] = anomaly_labels\n",
        "\n",
        "        # Get anomaly statistics\n",
        "        anomaly_count = np.sum(anomaly_labels)\n",
        "        anomaly_percent = (anomaly_count / len(anomaly_labels)) * 100\n",
        "\n",
        "        print(f\"DBSCAN detected {anomaly_count} anomalies ({anomaly_percent:.2f}%)\")\n",
        "\n",
        "        return model, results_df, scores, anomaly_labels\n",
        "    except Exception as e:\n",
        "        print(f\"Error in DBSCAN: {e}\")\n",
        "        return None, df, None, np.zeros(len(df))\n",
        "\n",
        "# Train DBSCAN on BMC dataset\n",
        "dbscan_bmc, results_bmc_dbscan, scores_bmc_dbscan, anomalies_bmc_dbscan = train_dbscan(X_bmc, dfBMC)\n",
        "\n",
        "# Train DBSCAN on host dataset\n",
        "dbscan_host, results_host_dbscan, scores_host_dbscan, anomalies_host_dbscan = train_dbscan(X_host, dfhost)\n",
        "\n",
        "if use_merged:\n",
        "    # Train DBSCAN on merged dataset\n",
        "    dbscan_merged, results_merged_dbscan, scores_merged_dbscan, anomalies_merged_dbscan = train_dbscan(X_merged, merged_df)\n",
        "\n",
        "# ===== 6. ENSEMBLE MODEL (VOTING) =====\n",
        "print(\"\\n=== Ensemble Model (Voting) ===\")\n",
        "\n",
        "def create_ensemble(df, anomaly_predictions_list, threshold=0.5):\n",
        "    \"\"\"Create ensemble model with enhanced error handling\"\"\"\n",
        "    try:\n",
        "        # Validate input\n",
        "        valid_predictions = []\n",
        "        for i, pred in enumerate(anomaly_predictions_list):\n",
        "            if pred is not None and len(pred) == len(df):\n",
        "                valid_predictions.append(pred)\n",
        "\n",
        "        if not valid_predictions:\n",
        "            print(\"Error: No valid predictions for ensemble\")\n",
        "            return df, np.zeros(len(df)), np.zeros(len(df))\n",
        "\n",
        "        # Create a matrix of all model predictions\n",
        "        all_predictions = np.column_stack(valid_predictions)\n",
        "\n",
        "        # Calculate the fraction of models that predict an anomaly\n",
        "        ensemble_scores = np.mean(all_predictions, axis=1)\n",
        "\n",
        "        # Apply threshold to get final prediction\n",
        "        ensemble_predictions = np.where(ensemble_scores >= threshold, 1, 0)\n",
        "\n",
        "        # Add results to dataframe\n",
        "        results_df = df.copy()\n",
        "        results_df['ensemble_score'] = ensemble_scores\n",
        "        results_df['is_anomaly'] = ensemble_predictions\n",
        "\n",
        "        # Get anomaly statistics\n",
        "        anomaly_count = np.sum(ensemble_predictions)\n",
        "        anomaly_percent = (anomaly_count / len(ensemble_predictions)) * 100\n",
        "\n",
        "        print(f\"Ensemble model detected {anomaly_count} anomalies ({anomaly_percent:.2f}%)\")\n",
        "        print(f\"Used {len(valid_predictions)} models for ensemble voting\")\n",
        "\n",
        "        return results_df, ensemble_scores, ensemble_predictions\n",
        "    except Exception as e:\n",
        "        print(f\"Error in ensemble creation: {e}\")\n",
        "        return df, np.zeros(len(df)), np.zeros(len(df))\n",
        "\n",
        "# Create ensemble for BMC dataset\n",
        "bmc_anomaly_predictions = []\n",
        "if 'anomalies_bmc_if' in locals() and anomalies_bmc_if is not None:\n",
        "    bmc_anomaly_predictions.append(anomalies_bmc_if)\n",
        "if 'anomalies_bmc_ocsvm' in locals() and anomalies_bmc_ocsvm is not None:\n",
        "    bmc_anomaly_predictions.append(anomalies_bmc_ocsvm)\n",
        "if 'anomalies_bmc_lof' in locals() and anomalies_bmc_lof is not None:\n",
        "    bmc_anomaly_predictions.append(anomalies_bmc_lof)\n",
        "if 'anomalies_bmc_dbscan' in locals() and anomalies_bmc_dbscan is not None:\n",
        "    bmc_anomaly_predictions.append(anomalies_bmc_dbscan)\n",
        "\n",
        "results_bmc_ensemble, scores_bmc_ensemble, anomalies_bmc_ensemble = create_ensemble(\n",
        "    dfBMC, bmc_anomaly_predictions, threshold=0.5)\n",
        "\n",
        "# Create ensemble for host dataset\n",
        "host_anomaly_predictions = []\n",
        "if 'anomalies_host_if' in locals() and anomalies_host_if is not None:\n",
        "    host_anomaly_predictions.append(anomalies_host_if)\n",
        "if 'anomalies_host_ocsvm' in locals() and anomalies_host_ocsvm is not None:\n",
        "    host_anomaly_predictions.append(anomalies_host_ocsvm)\n",
        "if 'anomalies_host_lof' in locals() and anomalies_host_lof is not None:\n",
        "    host_anomaly_predictions.append(anomalies_host_lof)\n",
        "if 'anomalies_host_dbscan' in locals() and anomalies_host_dbscan is not None:\n",
        "    host_anomaly_predictions.append(anomalies_host_dbscan)\n",
        "\n",
        "results_host_ensemble, scores_host_ensemble, anomalies_host_ensemble = create_ensemble(\n",
        "    dfhost, host_anomaly_predictions, threshold=0.5)\n",
        "\n",
        "if use_merged:\n",
        "    # Create ensemble for merged dataset\n",
        "    merged_anomaly_predictions = []\n",
        "    if 'anomalies_merged_if' in locals() and anomalies_merged_if is not None:\n",
        "        merged_anomaly_predictions.append(anomalies_merged_if)\n",
        "    if 'anomalies_merged_ocsvm' in locals() and anomalies_merged_ocsvm is not None:\n",
        "        merged_anomaly_predictions.append(anomalies_merged_ocsvm)\n",
        "    if 'anomalies_merged_lof' in locals() and anomalies_merged_lof is not None:\n",
        "        merged_anomaly_predictions.append(anomalies_merged_lof)\n",
        "    if 'anomalies_merged_dbscan' in locals() and anomalies_merged_dbscan is not None:\n",
        "        merged_anomaly_predictions.append(anomalies_merged_dbscan)\n",
        "\n",
        "    results_merged_ensemble, scores_merged_ensemble, anomalies_merged_ensemble = create_ensemble(\n",
        "        merged_df, merged_anomaly_predictions, threshold=0.5)\n",
        "\n",
        "# ===== 7. ANALYZE ANOMALIES =====\n",
        "print(\"\\n=== Analyzing Detected Anomalies ===\")\n",
        "\n",
        "def analyze_anomalies(df, features, device_type=\"\"):\n",
        "    \"\"\"Analyze detected anomalies with enhanced error handling\"\"\"\n",
        "    try:\n",
        "        # Get anomaly rows\n",
        "        anomalies = df[df['is_anomaly'] == 1]\n",
        "\n",
        "        if len(anomalies) == 0:\n",
        "            print(f\"No anomalies found in {device_type} data\")\n",
        "            return None\n",
        "\n",
        "        print(f\"Analyzing {len(anomalies)} anomalies in {device_type} data\")\n",
        "\n",
        "        # Get feature values for anomalies vs normal\n",
        "        normal = df[df['is_anomaly'] == 0]\n",
        "\n",
        "        if len(normal) == 0:\n",
        "            print(f\"Warning: All {device_type} points classified as anomalies!\")\n",
        "            return None\n",
        "\n",
        "        # Calculate statistics\n",
        "        anomaly_stats = anomalies[features].mean()\n",
        "        normal_stats = normal[features].mean()\n",
        "\n",
        "        # Calculate percent difference\n",
        "        # Avoid division by zero by adding a small epsilon\n",
        "        epsilon = 1e-10\n",
        "        percent_diff = (anomaly_stats - normal_stats) / (normal_stats.abs() + epsilon) * 100\n",
        "\n",
        "        # Create a dataframe of the comparison\n",
        "        comparison = pd.DataFrame({\n",
        "            'normal_avg': normal_stats,\n",
        "            'anomaly_avg': anomaly_stats,\n",
        "            'percent_diff': percent_diff\n",
        "        })\n",
        "\n",
        "        # Sort by absolute percent difference\n",
        "        comparison = comparison.sort_values(by='percent_diff', key=abs, ascending=False)\n",
        "\n",
        "        # Print top differentiating features\n",
        "        print(f\"\\nTop differentiating features between normal and anomalous points:\")\n",
        "        top_features = comparison.head(10)\n",
        "        for feature, row in top_features.iterrows():\n",
        "            print(f\"  {feature}: Normal={row['normal_avg']:.4f}, Anomaly={row['anomaly_avg']:.4f}, Diff={row['percent_diff']:.2f}%\")\n",
        "\n",
        "        # Save full comparison\n",
        "        comparison.to_csv(f\"{device_type}_anomaly_comparison.csv\")\n",
        "        print(f\"Saved detailed comparison to {device_type}_anomaly_comparison.csv\")\n",
        "\n",
        "        return comparison, anomalies, normal\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing anomalies: {e}\")\n",
        "        return None\n",
        "\n",
        "# Analyze BMC anomalies\n",
        "bmc_analysis = analyze_anomalies(results_bmc_ensemble, bmc_features, \"BMC\")\n",
        "\n",
        "# Analyze host anomalies\n",
        "host_analysis = analyze_anomalies(results_host_ensemble, host_features, \"Host\")\n",
        "\n",
        "if use_merged:\n",
        "    # Analyze merged dataset anomalies\n",
        "    merged_analysis = analyze_anomalies(results_merged_ensemble, merged_features, \"Merged\")\n",
        "\n",
        "# ===== 8. VISUALIZE ANOMALIES =====\n",
        "print(\"\\n=== Creating Anomaly Visualizations ===\")\n",
        "\n",
        "def visualize_anomalies(results_df, device_type, features, top_n=5):\n",
        "    \"\"\"Create visualizations for detected anomalies\"\"\"\n",
        "    try:\n",
        "        # Get anomalies and normal points\n",
        "        anomalies = results_df[results_df['is_anomaly'] == 1]\n",
        "        normal = results_df[results_df['is_anomaly'] == 0]\n",
        "\n",
        "        if len(anomalies) == 0:\n",
        "            print(f\"No anomalies to visualize for {device_type}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Visualizing {len(anomalies)} anomalies for {device_type}\")\n",
        "\n",
        "        # 1. Plot anomaly scores\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.hist(results_df['ensemble_score'], bins=50, alpha=0.7)\n",
        "        plt.axvline(x=0.5, color='r', linestyle='--', label='Threshold')\n",
        "        plt.title(f'{device_type} Anomaly Score Distribution')\n",
        "        plt.xlabel('Ensemble Anomaly Score')\n",
        "        plt.ylabel('Count')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{device_type}_anomaly_scores.png')\n",
        "        plt.close()\n",
        "\n",
        "        # 2. Plot top differentiating features\n",
        "        # Get feature comparison\n",
        "        if 'normal_avg' not in results_df.columns:\n",
        "            # Calculate statistics\n",
        "            anomaly_stats = anomalies[features].mean()\n",
        "            normal_stats = normal[features].mean()\n",
        "\n",
        "            # Calculate percent difference with protection against division by zero\n",
        "            epsilon = 1e-10\n",
        "            percent_diff = (anomaly_stats - normal_stats) / (normal_stats.abs() + epsilon) * 100\n",
        "\n",
        "            # Create a dataframe of the comparison\n",
        "            comparison = pd.DataFrame({\n",
        "                'normal_avg': normal_stats,\n",
        "                'anomaly_avg': anomaly_stats,\n",
        "                'percent_diff': percent_diff\n",
        "            })\n",
        "\n",
        "            # Sort by absolute percent difference\n",
        "            comparison = comparison.sort_values(by='percent_diff', key=abs, ascending=False)\n",
        "        else:\n",
        "            comparison = results_df[['normal_avg', 'anomaly_avg', 'percent_diff']].sort_values(\n",
        "                by='percent_diff', key=abs, ascending=False)\n",
        "\n",
        "        # Get top N features\n",
        "        top_features = comparison.head(top_n).index.tolist()\n",
        "\n",
        "        # Plot boxplots for top features\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        for i, feature in enumerate(top_features):\n",
        "            if feature in features:\n",
        "                plt.subplot(3, 2, i+1)\n",
        "\n",
        "                # Create boxplot data\n",
        "                data = [normal[feature].dropna(), anomalies[feature].dropna()]\n",
        "                labels = ['Normal', 'Anomaly']\n",
        "\n",
        "                # Create boxplot\n",
        "                plt.boxplot(data, labels=labels)\n",
        "                plt.title(feature)\n",
        "\n",
        "                # Add individual points for anomalies\n",
        "                x = np.random.normal(2, 0.04, size=len(anomalies))\n",
        "                plt.scatter(x, anomalies[feature], color='red', alpha=0.5)\n",
        "\n",
        "                if i >= 5:  # Limit to 6 plots\n",
        "                    break\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{device_type}_top_features.png')\n",
        "        plt.close()\n",
        "\n",
        "        # 3. Create time-based plot if time column exists\n",
        "        if '_time' in results_df.columns:\n",
        "            # Convert to datetime if needed\n",
        "            if not pd.api.types.is_datetime64_any_dtype(results_df['_time']):\n",
        "                results_df['_time'] = pd.to_datetime(results_df['_time'], errors='coerce')\n",
        "\n",
        "            # Plot anomaly scores over time\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.plot(results_df['_time'], results_df['ensemble_score'], 'b-', alpha=0.5)\n",
        "            plt.scatter(anomalies['_time'], anomalies['ensemble_score'],\n",
        "                        color='red', s=50, label='Anomalies')\n",
        "            plt.axhline(y=0.5, color='g', linestyle='--', label='Threshold')\n",
        "            plt.title(f'{device_type} Anomaly Scores Over Time')\n",
        "            plt.xlabel('Time')\n",
        "            plt.ylabel('Anomaly Score')\n",
        "            plt.legend()\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'{device_type}_time_anomalies.png')\n",
        "            plt.close()\n",
        "\n",
        "            # For each top feature, plot values over time\n",
        "            for feature in top_features[:3]:  # Limit to top 3 features\n",
        "                if feature in features:\n",
        "                    plt.figure(figsize=(12, 6))\n",
        "                    plt.plot(normal['_time'], normal[feature], 'b.', alpha=0.5, label='Normal')\n",
        "                    plt.plot(anomalies['_time'], anomalies[feature], 'r.', label='Anomaly')\n",
        "                    plt.title(f'{feature} Over Time')\n",
        "                    plt.xlabel('Time')\n",
        "                    plt.ylabel(feature)\n",
        "                    plt.legend()\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(f'{device_type}_{feature}_time.png')\n",
        "                    plt.close()\n",
        "\n",
        "        print(f\"Saved visualizations for {device_type}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating visualizations: {e}\")\n",
        "\n",
        "# Visualize BMC anomalies\n",
        "visualize_anomalies(results_bmc_ensemble, \"BMC\", bmc_features)\n",
        "\n",
        "# Visualize host anomalies\n",
        "visualize_anomalies(results_host_ensemble, \"Host\", host_features)\n",
        "\n",
        "if use_merged:\n",
        "    # Visualize merged dataset anomalies\n",
        "    visualize_anomalies(results_merged_ensemble, \"Merged\", merged_features)\n",
        "\n",
        "# ===== 9. SAVE MODELS AND RESULTS =====\n",
        "print(\"\\n=== Saving models and results ===\")\n",
        "\n",
        "def save_model_and_results(model, results_df, dataset_name, model_name):\n",
        "    \"\"\"Save model and results safely\"\"\"\n",
        "    try:\n",
        "        if model is None:\n",
        "            print(f\"Cannot save {model_name} model for {dataset_name} (model is None)\")\n",
        "            return\n",
        "\n",
        "        # Save model\n",
        "        model_filename = f\"{dataset_name}_{model_name}_model.joblib\"\n",
        "        joblib.dump(model, model_filename)\n",
        "\n",
        "        # Save results\n",
        "        results_filename = f\"{dataset_name}_{model_name}_results.csv\"\n",
        "        results_df.to_csv(results_filename, index=False)\n",
        "\n",
        "        print(f\"Saved {model_name} model and results for {dataset_name} dataset\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving {model_name} model for {dataset_name}: {e}\")\n",
        "\n",
        "# Save BMC models\n",
        "if iforest_bmc is not None:\n",
        "    save_model_and_results(iforest_bmc, results_bmc_if, \"BMC\", \"IsolationForest\")\n",
        "if ocsvm_bmc is not None:\n",
        "    save_model_and_results(ocsvm_bmc, results_bmc_ocsvm, \"BMC\", \"OneClassSVM\")\n",
        "if lof_bmc is not None:\n",
        "    save_model_and_results(lof_bmc, results_bmc_lof, \"BMC\", \"LOF\")\n",
        "if dbscan_bmc is not None:\n",
        "    save_model_and_results(dbscan_bmc, results_bmc_dbscan, \"BMC\", \"DBSCAN\")\n",
        "# Save ensemble results\n",
        "results_bmc_ensemble.to_csv(\"BMC_Ensemble_results.csv\", index=False)\n",
        "\n",
        "# Save host models\n",
        "if iforest_host is not None:\n",
        "    save_model_and_results(iforest_host, results_host_if, \"Host\", \"IsolationForest\")\n",
        "if ocsvm_host is not None:\n",
        "    save_model_and_results(ocsvm_host, results_host_ocsvm, \"Host\", \"OneClassSVM\")\n",
        "if lof_host is not None:\n",
        "    save_model_and_results(lof_host, results_host_lof, \"Host\", \"LOF\")\n",
        "if dbscan_host is not None:\n",
        "    save_model_and_results(dbscan_host, results_host_dbscan, \"Host\", \"DBSCAN\")\n",
        "# Save ensemble results\n",
        "results_host_ensemble.to_csv(\"Host_Ensemble_results.csv\", index=False)\n",
        "\n",
        "if use_merged:\n",
        "    # Save merged models\n",
        "    if iforest_merged is not None:\n",
        "        save_model_and_results(iforest_merged, results_merged_if, \"Merged\", \"IsolationForest\")\n",
        "    if ocsvm_merged is not None:\n",
        "        save_model_and_results(ocsvm_merged, results_merged_ocsvm, \"Merged\", \"OneClassSVM\")\n",
        "    if lof_merged is not None:\n",
        "        save_model_and_results(lof_merged, results_merged_lof, \"Merged\", \"LOF\")\n",
        "    if dbscan_merged is not None:\n",
        "        save_model_and_results(dbscan_merged, results_merged_dbscan, \"Merged\", \"DBSCAN\")\n",
        "    # Save ensemble results\n",
        "    results_merged_ensemble.to_csv(\"Merged_Ensemble_results.csv\", index=False)\n",
        "\n",
        "print(\"\\nAnomaly detection completed successfully!\")\n",
        "\n",
        "# ===== 10. EXPORT DETECTOR FUNCTION =====\n",
        "print(\"\\n=== Creating portable anomaly detector function ===\")\n",
        "\n",
        "# Write the anomaly detection function to a file\n",
        "with open('anomaly_detector.py', 'w') as f:\n",
        "    f.write(\"\"\"import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "def detect_anomalies(new_data, device_type='BMC', model_type='ensemble', time_col='_time'):\n",
        "    \\\"\\\"\\\"\n",
        "    Detect anomalies in server hardware data\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    new_data : DataFrame\n",
        "        New data to analyze for anomalies\n",
        "    device_type : str\n",
        "        'BMC' or 'Host'\n",
        "    model_type : str\n",
        "        'IsolationForest', 'OneClassSVM', 'LOF', 'DBSCAN', or 'ensemble'\n",
        "    time_col : str\n",
        "        Name of the time column\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    DataFrame with anomaly scores and flags\n",
        "    \\\"\\\"\\\"\n",
        "    try:\n",
        "        # Ensure we're working with a copy\n",
        "        data = new_data.copy()\n",
        "\n",
        "        # Handle time column if present\n",
        "        if time_col in data.columns:\n",
        "            if not pd.api.types.is_datetime64_any_dtype(data[time_col]):\n",
        "                # Handle 'Z' suffix in timestamps\n",
        "                data[time_col] = data[time_col].astype(str).str.replace('Z', '')\n",
        "                data[time_col] = pd.to_datetime(data[time_col], errors='coerce')\n",
        "\n",
        "        # Remove non-feature columns\n",
        "        exclude_cols = [time_col, 'device_id', 'row_id']\n",
        "        feature_cols = [col for col in data.columns if col not in exclude_cols]\n",
        "\n",
        "        # Ensure all feature columns are numeric\n",
        "        for col in feature_cols:\n",
        "            if data[col].dtype == 'object':\n",
        "                try:\n",
        "                    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "                except:\n",
        "                    # Remove non-numeric columns\n",
        "                    feature_cols.remove(col)\n",
        "\n",
        "        # Get only numeric columns\n",
        "        X = data[feature_cols].select_dtypes(include=['number'])\n",
        "\n",
        "        # Handle missing values - critical for algorithms like LOF and One-Class SVM\n",
        "        for col in X.columns:\n",
        "            if X[col].isna().any():\n",
        "                if X[col].count() > 0:  # If there are some non-NaN values\n",
        "                    X[col] = X[col].fillna(X[col].mean())\n",
        "                else:\n",
        "                    X[col] = X[col].fillna(0)  # If all values are NaN\n",
        "\n",
        "        # Final NaN check\n",
        "        if X.isna().sum().sum() > 0:\n",
        "            print(f\"Warning: {X.isna().sum().sum()} NaN values remain. Filling with 0.\")\n",
        "            X = X.fillna(0)\n",
        "\n",
        "        # Determine which models to load\n",
        "        models_to_load = []\n",
        "        if model_type.lower() == 'ensemble':\n",
        "            models_to_load = ['IsolationForest', 'LOF', 'DBSCAN', 'OneClassSVM']\n",
        "        else:\n",
        "            models_to_load = [model_type]\n",
        "\n",
        "        # Load models\n",
        "        loaded_models = {}\n",
        "        for model_name in models_to_load:\n",
        "            model_path = f\"{device_type}_{model_name}_model.joblib\"\n",
        "            if os.path.exists(model_path):\n",
        "                try:\n",
        "                    loaded_models[model_name] = joblib.load(model_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {model_name} model: {e}\")\n",
        "\n",
        "        if not loaded_models:\n",
        "            print(f\"No models could be loaded for {device_type}\")\n",
        "            return data\n",
        "\n",
        "        # Apply each model\n",
        "        all_predictions = []\n",
        "\n",
        "        for name, model in loaded_models.items():\n",
        "            try:\n",
        "                if name == 'IsolationForest' or name == 'OneClassSVM' or name == 'LOF':\n",
        "                    # These models have predict and decision_function methods\n",
        "                    predictions = model.predict(X)\n",
        "                    scores = model.decision_function(X)\n",
        "\n",
        "                    # Convert to 0 = normal, 1 = anomaly\n",
        "                    anomaly_labels = np.where(predictions == -1, 1, 0)\n",
        "\n",
        "                    # Store in results\n",
        "                    data[f'{name}_score'] = scores\n",
        "                    data[f'{name}_anomaly'] = anomaly_labels\n",
        "\n",
        "                    # Add to all predictions for ensemble\n",
        "                    all_predictions.append(anomaly_labels)\n",
        "\n",
        "                elif name == 'DBSCAN':\n",
        "                    # DBSCAN only has fit_predict\n",
        "                    clusters = model.fit_predict(X)\n",
        "\n",
        "                    # -1 cluster is anomalies\n",
        "                    anomaly_labels = np.where(clusters == -1, 1, 0)\n",
        "\n",
        "                    # Create dummy scores (1 for anomaly, 0 for normal)\n",
        "                    scores = anomaly_labels.astype(float)\n",
        "\n",
        "                    # Store in results\n",
        "                    data[f'{name}_cluster'] = clusters\n",
        "                    data[f'{name}_score'] = scores\n",
        "                    data[f'{name}_anomaly'] = anomaly_labels\n",
        "\n",
        "                    # Add to all predictions for ensemble\n",
        "                    all_predictions.append(anomaly_labels)\n",
        "            except Exception as e:\n",
        "                print(f\"Error applying {name} model: {e}\")\n",
        "\n",
        "        # If ensemble was requested or multiple models were loaded, create ensemble prediction\n",
        "        if model_type.lower() == 'ensemble' and len(all_predictions) > 0:\n",
        "            # Stack all predictions\n",
        "            all_predictions_array = np.column_stack(all_predictions)\n",
        "\n",
        "            # Calculate ensemble score (average of all model predictions)\n",
        "            ensemble_scores = np.mean(all_predictions_array, axis=1)\n",
        "\n",
        "            # Apply threshold to get final prediction\n",
        "            ensemble_predictions = np.where(ensemble_scores >= 0.5, 1, 0)\n",
        "\n",
        "            # Add to results\n",
        "            data['ensemble_score'] = ensemble_scores\n",
        "            data['is_anomaly'] = ensemble_predictions\n",
        "        elif len(all_predictions) == 1:\n",
        "            # If only one model, use its prediction as final result\n",
        "            data['is_anomaly'] = all_predictions[0]\n",
        "        else:\n",
        "            # No valid predictions\n",
        "            data['is_anomaly'] = 0\n",
        "            data['ensemble_score'] = 0\n",
        "\n",
        "        # Count anomalies\n",
        "        anomaly_count = data['is_anomaly'].sum()\n",
        "        total_count = len(data)\n",
        "        print(f\"Detected {anomaly_count} anomalies in {total_count} samples ({anomaly_count/total_count*100:.2f}%)\")\n",
        "\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in anomaly detection: {e}\")\n",
        "        return new_data\n",
        "\n",
        "def get_anomaly_explanation(anomalies, normal_data=None, top_n=5):\n",
        "    \\\"\\\"\\\"\n",
        "    Get explanation of what's unusual about the detected anomalies\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    anomalies : DataFrame\n",
        "        Data points flagged as anomalies\n",
        "    normal_data : DataFrame, optional\n",
        "        Data points considered normal for comparison\n",
        "    top_n : int\n",
        "        Number of most significant features to include in explanation\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Dictionary with explanation details\n",
        "    \\\"\\\"\\\"\n",
        "    try:\n",
        "        if len(anomalies) == 0:\n",
        "            return {\"explanation\": \"No anomalies detected\"}\n",
        "\n",
        "        # Get non-metadata columns\n",
        "        exclude_cols = ['_time', 'device_id', 'row_id', 'is_anomaly', 'ensemble_score']\n",
        "        exclude_cols.extend([col for col in anomalies.columns if 'anomaly' in col or 'score' in col or 'cluster' in col])\n",
        "\n",
        "        feature_cols = [col for col in anomalies.columns if col not in exclude_cols]\n",
        "\n",
        "        # If no normal data provided, compare to the average\n",
        "        if normal_data is None or len(normal_data) == 0:\n",
        "            # Create synthetic normal data with average values\n",
        "            normal_avg = {col: anomalies[col].mean() for col in feature_cols}\n",
        "\n",
        "            # Create explanation based on deviation from average\n",
        "            deviations = []\n",
        "            for col in feature_cols:\n",
        "                try:\n",
        "                    avg_value = normal_avg[col]\n",
        "                    for i, row in anomalies.iterrows():\n",
        "                        value = row[col]\n",
        "                        if pd.notna(value) and pd.notna(avg_value) and avg_value != 0:\n",
        "                            pct_diff = (value - avg_value) / avg_value * 100\n",
        "                            deviations.append({\n",
        "                                \"feature\": col,\n",
        "                                \"value\": value,\n",
        "                                \"avg_value\": avg_value,\n",
        "                                \"pct_diff\": pct_diff,\n",
        "                                \"abs_pct_diff\": abs(pct_diff)\n",
        "                            })\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # Sort by absolute percent difference\n",
        "            deviations.sort(key=lambda x: x['abs_pct_diff'], reverse=True)\n",
        "\n",
        "            return {\n",
        "                \"anomaly_count\": len(anomalies),\n",
        "                \"top_deviations\": deviations[:top_n],\n",
        "                \"explanation\": \"These data points show unusual values compared to average patterns\"\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            # Calculate statistics\n",
        "            anomaly_stats = anomalies[feature_cols].mean()\n",
        "            normal_stats = normal_data[feature_cols].mean()\n",
        "\n",
        "            # Calculate percent difference\n",
        "            percent_diff = {}\n",
        "            for col in feature_cols:\n",
        "                try:\n",
        "                    if pd.notna(normal_stats[col]) and normal_stats[col] != 0:\n",
        "                        percent_diff[col] = (anomaly_stats[col] - normal_stats[col]) / normal_stats[col] * 100\n",
        "                    else:\n",
        "                        percent_diff[col] = 0\n",
        "                except:\n",
        "                    percent_diff[col] = 0\n",
        "\n",
        "            # Create sorted list of deviations\n",
        "            deviations = [\n",
        "                {\n",
        "                    \"feature\": col,\n",
        "                    \"anomaly_avg\": anomaly_stats[col],\n",
        "                    \"normal_avg\": normal_stats[col],\n",
        "                    \"pct_diff\": percent_diff[col],\n",
        "                    \"abs_pct_diff\": abs(percent_diff[col])\n",
        "                }\n",
        "                for col in feature_cols if pd.notna(percent_diff[col])\n",
        "            ]\n",
        "\n",
        "            # Sort by absolute percent difference\n",
        "            deviations.sort(key=lambda x: x['abs_pct_diff'], reverse=True)\n",
        "\n",
        "            return {\n",
        "                \"anomaly_count\": len(anomalies),\n",
        "                \"normal_count\": len(normal_data),\n",
        "                \"top_deviations\": deviations[:top_n],\n",
        "                \"explanation\": \"These anomalies differ from normal patterns in the following ways\"\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"explanation\": f\"Error generating explanation: {e}\"}\n",
        "\"\"\")\n",
        "\n",
        "print(\"Successfully created anomaly detector function!\")\n",
        "print(\"\\nTo use this detector with new data:\")\n",
        "print(\"\"\"\n",
        "from anomaly_detector import detect_anomalies, get_anomaly_explanation\n",
        "\n",
        "# Load new data\n",
        "new_data = pd.read_csv('new_bmc_data.csv')\n",
        "\n",
        "# Detect anomalies\n",
        "results = detect_anomalies(new_data, device_type='BMC', model_type='ensemble')\n",
        "\n",
        "# Get anomalies\n",
        "anomalies = results[results['is_anomaly'] == 1]\n",
        "normal = results[results['is_anomaly'] == 0]\n",
        "\n",
        "# Get explanation for anomalies\n",
        "if len(anomalies) > 0:\n",
        "    explanation = get_anomaly_explanation(anomalies, normal)\n",
        "    print(explanation)\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tSaABllQ64kC",
        "outputId": "051c3993-fb8c-4002-e7ac-a024272b84a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading engineered datasets...\n",
            "Loaded engineered datasets\n",
            "Loaded merged dataset: 153 rows, 235 columns\n",
            "\n",
            "=== Preparing data for modeling ===\n",
            "Found 1322 NaN values across 107 features\n",
            "  Column '_measurement' has 1290 NaNs (100.0%) - consider dropping\n",
            "  Column '_measurement' has all NaN values - filling with 0\n",
            "  Filled 1 NaNs in 'Cpu1_Temp_roll_std_5' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'Cpu1_Temp_zscore_5' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'Cpu1_Temp_roll_std_10' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'Cpu1_Temp_zscore_10' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'Cpu2_Temp_roll_std_5' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'Cpu2_Temp_zscore_5' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'Cpu2_Temp_roll_std_10' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'Cpu2_Temp_zscore_10' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'FAN1_roll_std_5' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'FAN1_zscore_5' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'FAN1_roll_std_10' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'FAN1_zscore_10' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'FAN2_roll_std_5' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'FAN2_zscore_5' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'FAN2_roll_std_10' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'FAN2_zscore_10' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'FAN3_roll_std_5' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'FAN3_zscore_5' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'FAN3_roll_std_10' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'FAN3_zscore_10' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'FAN4_roll_std_5' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'FAN4_zscore_5' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'FAN4_roll_std_10' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'FAN4_zscore_10' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'PSU1_Total_Power_roll_std_5' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'PSU1_Total_Power_zscore_5' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'PSU1_Total_Power_roll_std_10' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'PSU1_Total_Power_zscore_10' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'PSU2_Total_Power_roll_std_5' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'PSU2_Total_Power_zscore_5' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'PSU2_Total_Power_roll_std_10' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'PSU2_Total_Power_zscore_10' with mean: 0.0000\n",
            "Successfully handled all NaN values. Dataset shape: (1290, 107)\n",
            "Found 31 NaN values across 123 features\n",
            "  Filled 1 NaNs in 'hour' with mean: 11.4881\n",
            "  Filled 1 NaNs in 'day' with mean: 1.0000\n",
            "  Filled 1 NaNs in 'weekday' with mean: 4.0000\n",
            "  Filled 1 NaNs in 'cpu_usage_roll_std_5' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'cpu_usage_zscore_5' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'cpu_usage_roll_std_10' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'cpu_usage_zscore_10' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'mem_usage_roll_std_5' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'mem_usage_zscore_5' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'mem_usage_roll_std_10' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'mem_usage_zscore_10' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'load_1_roll_std_5' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'load_1_zscore_5' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'load_1_roll_std_10' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'load_1_zscore_10' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'load_5_roll_std_5' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'load_5_zscore_5' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'load_5_roll_std_10' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'load_5_zscore_10' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'load_15_roll_std_5' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'load_15_zscore_5' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'load_15_roll_std_10' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'load_15_zscore_10' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'cache_misses_roll_std_5' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'cache_misses_zscore_5' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'cache_misses_roll_std_10' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'cache_misses_zscore_10' with mean: 0.0000\n",
            "  Filled 1 NaNs in 'L1_dcache_load_misses_roll_std_5' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'L1_dcache_load_misses_zscore_5' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'L1_dcache_load_misses_roll_std_10' with mean: -0.0000\n",
            "  Filled 1 NaNs in 'L1_dcache_load_misses_zscore_10' with mean: 0.0000\n",
            "Successfully handled all NaN values. Dataset shape: (1718, 123)\n",
            "Found 213 NaN values across 231 features\n",
            "  Column '_measurement' has 153 NaNs (100.0%) - consider dropping\n",
            "  Column '_measurement' has all NaN values - filling with 0\n",
            "  Filled 1 NaNs in 'Cpu1_Temp_roll_std_5' with mean: 0.0388\n",
            "  Filled 1 NaNs in 'Cpu1_Temp_zscore_5' with mean: -0.0297\n",
            "  Filled 1 NaNs in 'Cpu1_Temp_roll_std_10' with mean: 0.0496\n",
            "  Filled 1 NaNs in 'Cpu1_Temp_zscore_10' with mean: -0.0508\n",
            "  Filled 1 NaNs in 'Cpu2_Temp_roll_std_5' with mean: 0.0131\n",
            "  Filled 1 NaNs in 'Cpu2_Temp_zscore_5' with mean: 0.0008\n",
            "  Filled 1 NaNs in 'Cpu2_Temp_roll_std_10' with mean: 0.0255\n",
            "  Filled 1 NaNs in 'Cpu2_Temp_zscore_10' with mean: 0.0153\n",
            "  Filled 1 NaNs in 'FAN1_roll_std_5' with mean: 12.7526\n",
            "  Filled 1 NaNs in 'FAN1_zscore_5' with mean: 0.1159\n",
            "  Filled 1 NaNs in 'FAN1_roll_std_10' with mean: 13.6061\n",
            "  Filled 1 NaNs in 'FAN1_zscore_10' with mean: 0.0937\n",
            "  Filled 1 NaNs in 'FAN2_roll_std_5' with mean: 5.6499\n",
            "  Filled 1 NaNs in 'FAN2_zscore_5' with mean: -0.0478\n",
            "  Filled 1 NaNs in 'FAN2_roll_std_10' with mean: 5.8736\n",
            "  Filled 1 NaNs in 'FAN2_zscore_10' with mean: 0.0047\n",
            "  Filled 1 NaNs in 'FAN3_roll_std_5' with mean: 5.6232\n",
            "  Filled 1 NaNs in 'FAN3_zscore_5' with mean: -0.0618\n",
            "  Filled 1 NaNs in 'FAN3_roll_std_10' with mean: 5.6450\n",
            "  Filled 1 NaNs in 'FAN3_zscore_10' with mean: -0.0286\n",
            "  Filled 1 NaNs in 'FAN4_roll_std_5' with mean: 5.8435\n",
            "  Filled 1 NaNs in 'FAN4_zscore_5' with mean: 0.0724\n",
            "  Filled 1 NaNs in 'FAN4_roll_std_10' with mean: 5.9770\n",
            "  Filled 1 NaNs in 'FAN4_zscore_10' with mean: 0.0802\n",
            "  Filled 1 NaNs in 'PSU1_Total_Power_roll_std_5' with mean: 0.4912\n",
            "  Filled 1 NaNs in 'PSU1_Total_Power_zscore_5' with mean: 0.2487\n",
            "  Filled 1 NaNs in 'PSU1_Total_Power_roll_std_10' with mean: 0.6595\n",
            "  Filled 1 NaNs in 'PSU1_Total_Power_zscore_10' with mean: 0.2213\n",
            "  Filled 1 NaNs in 'PSU2_Total_Power_roll_std_5' with mean: 0.5294\n",
            "  Filled 1 NaNs in 'PSU2_Total_Power_zscore_5' with mean: 0.2622\n",
            "  Filled 1 NaNs in 'PSU2_Total_Power_roll_std_10' with mean: 0.6932\n",
            "  Filled 1 NaNs in 'PSU2_Total_Power_zscore_10' with mean: 0.2381\n",
            "  Filled 1 NaNs in 'cpu_usage_roll_std_5' with mean: 0.6359\n",
            "  Filled 1 NaNs in 'cpu_usage_zscore_5' with mean: 0.0279\n",
            "  Filled 1 NaNs in 'cpu_usage_roll_std_10' with mean: 0.7423\n",
            "  Filled 1 NaNs in 'cpu_usage_zscore_10' with mean: 0.0299\n",
            "  Filled 1 NaNs in 'mem_usage_roll_std_5' with mean: 0.0068\n",
            "  Filled 1 NaNs in 'mem_usage_zscore_5' with mean: 0.1088\n",
            "  Filled 1 NaNs in 'mem_usage_roll_std_10' with mean: 0.0079\n",
            "  Filled 1 NaNs in 'mem_usage_zscore_10' with mean: 0.1679\n",
            "  Filled 1 NaNs in 'load_1_roll_std_5' with mean: 0.1917\n",
            "  Filled 1 NaNs in 'load_1_zscore_5' with mean: -0.2283\n",
            "  Filled 1 NaNs in 'load_1_roll_std_10' with mean: 0.2833\n",
            "  Filled 1 NaNs in 'load_1_zscore_10' with mean: -0.2262\n",
            "  Filled 1 NaNs in 'load_5_roll_std_5' with mean: 0.0477\n",
            "  Filled 1 NaNs in 'load_5_zscore_5' with mean: -0.2549\n",
            "  Filled 1 NaNs in 'load_5_roll_std_10' with mean: 0.0802\n",
            "  Filled 1 NaNs in 'load_5_zscore_10' with mean: -0.3123\n",
            "  Filled 1 NaNs in 'load_15_roll_std_5' with mean: 0.0219\n",
            "  Filled 1 NaNs in 'load_15_zscore_5' with mean: -0.2893\n",
            "  Filled 1 NaNs in 'load_15_roll_std_10' with mean: 0.0381\n",
            "  Filled 1 NaNs in 'load_15_zscore_10' with mean: -0.3741\n",
            "  Filled 1 NaNs in 'cache_misses_roll_std_5' with mean: 7914806.6230\n",
            "  Filled 1 NaNs in 'cache_misses_zscore_5' with mean: 0.0496\n",
            "  Filled 1 NaNs in 'cache_misses_roll_std_10' with mean: 8032161.9758\n",
            "  Filled 1 NaNs in 'cache_misses_zscore_10' with mean: 0.0646\n",
            "  Filled 1 NaNs in 'L1_dcache_load_misses_roll_std_5' with mean: 8101558.9625\n",
            "  Filled 1 NaNs in 'L1_dcache_load_misses_zscore_5' with mean: 0.0762\n",
            "  Filled 1 NaNs in 'L1_dcache_load_misses_roll_std_10' with mean: 8109638.0573\n",
            "  Filled 1 NaNs in 'L1_dcache_load_misses_zscore_10' with mean: 0.0794\n",
            "Successfully handled all NaN values. Dataset shape: (153, 231)\n",
            "\n",
            "=== Isolation Forest Model ===\n",
            "Isolation Forest detected 65 anomalies (5.04%)\n",
            "Isolation Forest detected 86 anomalies (5.01%)\n",
            "Isolation Forest detected 8 anomalies (5.23%)\n",
            "\n",
            "=== One-Class SVM Model ===\n",
            "One-Class SVM detected 68 anomalies (5.27%)\n",
            "One-Class SVM detected 88 anomalies (5.12%)\n",
            "One-Class SVM detected 8 anomalies (5.23%)\n",
            "\n",
            "=== Local Outlier Factor Model ===\n",
            "Local Outlier Factor detected 58 anomalies (4.50%)\n",
            "Local Outlier Factor detected 81 anomalies (4.71%)\n",
            "Local Outlier Factor detected 6 anomalies (3.92%)\n",
            "\n",
            "=== DBSCAN Clustering Model ===\n",
            "DBSCAN detected 1290 anomalies (100.00%)\n",
            "DBSCAN detected 1718 anomalies (100.00%)\n",
            "DBSCAN detected 153 anomalies (100.00%)\n",
            "\n",
            "=== Ensemble Model (Voting) ===\n",
            "Ensemble model detected 123 anomalies (9.53%)\n",
            "Used 4 models for ensemble voting\n",
            "Ensemble model detected 134 anomalies (7.80%)\n",
            "Used 4 models for ensemble voting\n",
            "Ensemble model detected 15 anomalies (9.80%)\n",
            "Used 4 models for ensemble voting\n",
            "\n",
            "=== Analyzing Detected Anomalies ===\n",
            "Analyzing 123 anomalies in BMC data\n",
            "\n",
            "Top differentiating features between normal and anomalous points:\n",
            "  Cpu2_Temp_roll_std_5: Normal=-0.1249, Anomaly=1.1950, Diff=1056.56%\n",
            "  Cpu1_Temp_roll_std_5: Normal=-0.1000, Anomaly=0.9562, Diff=1056.56%\n",
            "  PSU1_Total_Power_roll_std_10: Normal=-0.0948, Anomaly=0.9069, Diff=1056.56%\n",
            "  Cpu1_Temp_roll_std_10: Normal=-0.0911, Anomaly=0.8714, Diff=1056.56%\n",
            "  Cpu2_Temp_roll_std_10: Normal=-0.0909, Anomaly=0.8693, Diff=1056.56%\n",
            "  PSU1_Total_Power_roll_std_5: Normal=-0.0839, Anomaly=0.8027, Diff=1056.56%\n",
            "  PSU2_Total_Power_roll_std_10: Normal=-0.0719, Anomaly=0.6876, Diff=1056.56%\n",
            "  PSU2_Total_Power_roll_std_5: Normal=-0.0663, Anomaly=0.6344, Diff=1056.56%\n",
            "  FAN2_roll_std_5: Normal=-0.0321, Anomaly=0.3071, Diff=1056.56%\n",
            "  FAN2_roll_std_10: Normal=-0.0268, Anomaly=0.2563, Diff=1056.56%\n",
            "Saved detailed comparison to BMC_anomaly_comparison.csv\n",
            "Analyzing 134 anomalies in Host data\n",
            "\n",
            "Top differentiating features between normal and anomalous points:\n",
            "  load_5_roll_std_10: Normal=-0.1226, Anomaly=1.4598, Diff=1290.98%\n",
            "  load_15_roll_std_10: Normal=-0.1170, Anomaly=1.3933, Diff=1290.98%\n",
            "  load_5_roll_std_5: Normal=-0.1161, Anomaly=1.3821, Diff=1290.98%\n",
            "  load_15_roll_std_5: Normal=-0.1104, Anomaly=1.3153, Diff=1290.98%\n",
            "  load_1_roll_std_10: Normal=-0.0983, Anomaly=1.1708, Diff=1290.98%\n",
            "  load_1_roll_std_5: Normal=-0.0979, Anomaly=1.1654, Diff=1290.98%\n",
            "  cpu_usage_roll_std_10: Normal=-0.0789, Anomaly=0.9398, Diff=1290.98%\n",
            "  L1_dcache_load_misses_roll_std_10: Normal=-0.0742, Anomaly=0.8837, Diff=1290.98%\n",
            "  cache_misses_roll_std_10: Normal=-0.0729, Anomaly=0.8681, Diff=1290.98%\n",
            "  cpu_usage_roll_std_5: Normal=-0.0661, Anomaly=0.7877, Diff=1290.98%\n",
            "Saved detailed comparison to Host_anomaly_comparison.csv\n",
            "Analyzing 15 anomalies in Merged data\n",
            "\n",
            "Top differentiating features between normal and anomalous points:\n",
            "  Cpu2_Temp_diff_3: Normal=0.0000, Anomaly=0.0667, Diff=66666666666.67%\n",
            "  Cpu1_Temp_diff_1: Normal=0.0000, Anomaly=-0.0667, Diff=-66666666666.67%\n",
            "  Cpu2_Temp_pct_change_3: Normal=0.0000, Anomaly=0.0015, Diff=166487.34%\n",
            "  Cpu1_Temp_pct_change_1: Normal=0.0000, Anomaly=-0.0014, Diff=-92667.33%\n",
            "  Cpu2_Temp_zscore_10: Normal=-0.0045, Anomaly=0.2102, Diff=4792.71%\n",
            "  cpu_usage_zscore_10: Normal=0.0067, Anomaly=0.2586, Diff=3737.57%\n",
            "  load_1_diff_3: Normal=-0.0342, Anomaly=0.9100, Diff=2760.59%\n",
            "  FAN3_zscore_10: Normal=-0.0081, Anomaly=-0.2303, Diff=-2740.28%\n",
            "  load_1_pct_change_3: Normal=-0.0063, Anomaly=0.1583, Diff=2599.84%\n",
            "  load_1_diff_1: Normal=-0.0176, Anomaly=0.3407, Diff=2034.65%\n",
            "Saved detailed comparison to Merged_anomaly_comparison.csv\n",
            "\n",
            "=== Creating Anomaly Visualizations ===\n",
            "Visualizing 123 anomalies for BMC\n",
            "Saved visualizations for BMC\n",
            "Visualizing 134 anomalies for Host\n",
            "Error creating visualizations: Failed to convert value(s) to axis units: 0       2024-11-01 10:01:08\n",
            "1       2024-11-01 10:01:15\n",
            "2       2024-11-01 10:01:22\n",
            "3       2024-11-01 10:01:29\n",
            "4       2024-11-01 10:01:36\n",
            "               ...         \n",
            "1713    2024-11-01 13:55:57\n",
            "1714    2024-11-01 13:56:05\n",
            "1715    2024-11-01 13:56:13\n",
            "1716    2024-11-01 13:56:21\n",
            "1717                    NaN\n",
            "Name: _time, Length: 134, dtype: object\n",
            "Visualizing 15 anomalies for Merged\n",
            "Saved visualizations for Merged\n",
            "\n",
            "=== Saving models and results ===\n",
            "Saved IsolationForest model and results for BMC dataset\n",
            "Saved OneClassSVM model and results for BMC dataset\n",
            "Saved LOF model and results for BMC dataset\n",
            "Saved DBSCAN model and results for BMC dataset\n",
            "Saved IsolationForest model and results for Host dataset\n",
            "Saved OneClassSVM model and results for Host dataset\n",
            "Saved LOF model and results for Host dataset\n",
            "Saved DBSCAN model and results for Host dataset\n",
            "Saved IsolationForest model and results for Merged dataset\n",
            "Saved OneClassSVM model and results for Merged dataset\n",
            "Saved LOF model and results for Merged dataset\n",
            "Saved DBSCAN model and results for Merged dataset\n",
            "\n",
            "Anomaly detection completed successfully!\n",
            "\n",
            "=== Creating portable anomaly detector function ===\n",
            "Successfully created anomaly detector function!\n",
            "\n",
            "To use this detector with new data:\n",
            "\n",
            "from anomaly_detector import detect_anomalies, get_anomaly_explanation\n",
            "\n",
            "# Load new data\n",
            "new_data = pd.read_csv('new_bmc_data.csv')\n",
            "\n",
            "# Detect anomalies\n",
            "results = detect_anomalies(new_data, device_type='BMC', model_type='ensemble')\n",
            "\n",
            "# Get anomalies\n",
            "anomalies = results[results['is_anomaly'] == 1]\n",
            "normal = results[results['is_anomaly'] == 0]\n",
            "\n",
            "# Get explanation for anomalies\n",
            "if len(anomalies) > 0:\n",
            "    explanation = get_anomaly_explanation(anomalies, normal)\n",
            "    print(explanation)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from anomaly_detector import detect_anomalies_in_new_data\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    filename='hardware_monitor.log'\n",
        ")\n",
        "logger = logging.getLogger('HardwareMonitor')\n",
        "\n",
        "# Create a console handler\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "console_handler.setFormatter(formatter)\n",
        "logger.addHandler(console_handler)\n",
        "\n",
        "class HardwareAnomalyMonitor:\n",
        "    \"\"\"\n",
        "    Real-time monitoring system for hardware anomaly detection\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config_file='monitor_config.json'):\n",
        "        \"\"\"\n",
        "        Initialize the monitoring system\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        config_file : str\n",
        "            Path to configuration file\n",
        "        \"\"\"\n",
        "        self.config = self._load_config(config_file)\n",
        "\n",
        "        # Initialize state\n",
        "        self.last_bmc_time = None\n",
        "        self.last_host_time = None\n",
        "        self.anomaly_history = []\n",
        "        self.alert_sent = False\n",
        "        self.alert_cooldown = datetime.datetime.now() - datetime.timedelta(hours=24)  # Start with no cooldown\n",
        "\n",
        "        # Initialize alert thresholds\n",
        "        self.anomaly_threshold = self.config.get('anomaly_threshold', 0.7)\n",
        "        self.alert_cooldown_hours = self.config.get('alert_cooldown_hours', 6)\n",
        "\n",
        "        logger.info(f\"Hardware Anomaly Monitor initialized with threshold {self.anomaly_threshold}\")\n",
        "\n",
        "    def _load_config(self, config_file):\n",
        "        \"\"\"Load configuration from JSON file\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(config_file):\n",
        "                with open(config_file, 'r') as f:\n",
        "                    config = json.load(f)\n",
        "                logger.info(f\"Loaded configuration from {config_file}\")\n",
        "                return config\n",
        "            else:\n",
        "                # Create default config\n",
        "                config = {\n",
        "                    'bmc_data_path': 'BMC_new_data.csv',\n",
        "                    'host_data_path': 'host_new_data.csv',\n",
        "                    'bmc_model_type': 'ensemble',\n",
        "                    'host_model_type': 'ensemble',\n",
        "                    'check_interval_seconds': 60,\n",
        "                    'anomaly_threshold': 0.7,\n",
        "                    'alert_cooldown_hours': 6,\n",
        "                    'email_alerts': False,\n",
        "                    'email_config': {\n",
        "                        'smtp_server': 'smtp.example.com',\n",
        "                        'smtp_port': 587,\n",
        "                        'username': 'alert@example.com',\n",
        "                        'password': 'your_password',\n",
        "                        'from_address': 'alert@example.com',\n",
        "                        'to_addresses': ['admin@example.com']\n",
        "                    },\n",
        "                    'log_to_file': True,\n",
        "                    'visualization': True,\n",
        "                    'max_history_size': 1000\n",
        "                }\n",
        "\n",
        "                # Save default config\n",
        "                with open(config_file, 'w') as f:\n",
        "                    json.dump(config, f, indent=4)\n",
        "\n",
        "                logger.info(f\"Created default configuration in {config_file}\")\n",
        "                return config\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading configuration: {e}\")\n",
        "            # Return minimal default config\n",
        "            return {\n",
        "                'bmc_data_path': 'BMC_new_data.csv',\n",
        "                'host_data_path': 'host_new_data.csv',\n",
        "                'check_interval_seconds': 60\n",
        "            }\n",
        "\n",
        "    def _load_new_data(self, device_type='BMC'):\n",
        "        \"\"\"\n",
        "        Load new data from CSV file\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        device_type : str\n",
        "            'BMC' or 'Host'\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        DataFrame with new data, or None if no new data\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if device_type.upper() == 'BMC':\n",
        "                path = self.config.get('bmc_data_path', 'BMC_new_data.csv')\n",
        "                last_time = self.last_bmc_time\n",
        "            else:\n",
        "                path = self.config.get('host_data_path', 'host_new_data.csv')\n",
        "                last_time = self.last_host_time\n",
        "\n",
        "            # Check if file exists\n",
        "            if not os.path.exists(path):\n",
        "                logger.warning(f\"{device_type} data file {path} not found\")\n",
        "                return None\n",
        "\n",
        "            # Load data\n",
        "            df = pd.read_csv(path)\n",
        "\n",
        "            # Convert time column to datetime\n",
        "            if '_time' in df.columns:\n",
        "                df['_time'] = pd.to_datetime(df['_time'])\n",
        "            else:\n",
        "                logger.warning(f\"No '_time' column in {device_type} data\")\n",
        "                return df  # Return all data without filtering\n",
        "\n",
        "            # Filter for new data only if we have a last time\n",
        "            if last_time is not None:\n",
        "                new_data = df[df['_time'] > last_time]\n",
        "                if len(new_data) == 0:\n",
        "                    logger.info(f\"No new {device_type} data since {last_time}\")\n",
        "                    return None\n",
        "\n",
        "                # Update last time\n",
        "                if device_type.upper() == 'BMC':\n",
        "                    self.last_bmc_time = new_data['_time'].max()\n",
        "                else:\n",
        "                    self.last_host_time = new_data['_time'].max()\n",
        "\n",
        "                logger.info(f\"Loaded {len(new_data)} new {device_type} records\")\n",
        "                return new_data\n",
        "            else:\n",
        "                # First time loading, take all data\n",
        "                if device_type.upper() == 'BMC':\n",
        "                    self.last_bmc_time = df['_time'].max()\n",
        "                else:\n",
        "                    self.last_host_time = df['_time'].max()\n",
        "\n",
        "                logger.info(f\"First load: {len(df)} {device_type} records\")\n",
        "                return df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading {device_type} data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _detect_anomalies(self, data, device_type='BMC'):\n",
        "        \"\"\"\n",
        "        Detect anomalies in the data\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : DataFrame\n",
        "            Data to analyze\n",
        "        device_type : str\n",
        "            'BMC' or 'Host'\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        DataFrame with anomaly scores and flags\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if device_type.upper() == 'BMC':\n",
        "                model_type = self.config.get('bmc_model_type', 'ensemble')\n",
        "            else:\n",
        "                model_type = self.config.get('host_model_type', 'ensemble')\n",
        "\n",
        "            results = detect_anomalies_in_new_data(data, model_type=model_type, device_type=device_type)\n",
        "\n",
        "            if results is not None:\n",
        "                # Check for anomalies\n",
        "                anomalies = results[results['is_anomaly'] == 1]\n",
        "                if len(anomalies) > 0:\n",
        "                    logger.info(f\"Found {len(anomalies)} anomalies in {len(results)} {device_type} samples\")\n",
        "\n",
        "                    # Store anomalies for historical tracking\n",
        "                    for _, row in anomalies.iterrows():\n",
        "                        anomaly_info = {\n",
        "                            'time': row['_time'] if '_time' in row else datetime.datetime.now(),\n",
        "                            'device_type': device_type,\n",
        "                            'device_id': row['device_id'] if 'device_id' in row else 'unknown',\n",
        "                            'score': row.get('ensemble_score', row.get('anomaly_score', 1.0)),\n",
        "                            'data': row.to_dict()\n",
        "                        }\n",
        "                        self.anomaly_history.append(anomaly_info)\n",
        "\n",
        "                    # Trim history if it gets too large\n",
        "                    max_history = self.config.get('max_history_size', 1000)\n",
        "                    if len(self.anomaly_history) > max_history:\n",
        "                        self.anomaly_history = self.anomaly_history[-max_history:]\n",
        "\n",
        "                    # Check if we need to send an alert\n",
        "                    self._check_alert_conditions(anomalies, device_type)\n",
        "\n",
        "                return results\n",
        "            else:\n",
        "                logger.warning(f\"No results from anomaly detection for {device_type}\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error detecting anomalies in {device_type} data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _check_alert_conditions(self, anomalies, device_type):\n",
        "        \"\"\"\n",
        "        Check if an alert should be sent\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        anomalies : DataFrame\n",
        "            Dataframe containing anomaly records\n",
        "        device_type : str\n",
        "            'BMC' or 'Host'\n",
        "        \"\"\"\n",
        "        now = datetime.datetime.now()\n",
        "\n",
        "        # Check if we're on cooldown\n",
        "        if now < self.alert_cooldown:\n",
        "            logger.info(f\"In alert cooldown period until {self.alert_cooldown}\")\n",
        "            return\n",
        "\n",
        "        # Check if any anomalies exceed the threshold\n",
        "        if 'ensemble_score' in anomalies.columns:\n",
        "            high_score_anomalies = anomalies[anomalies['ensemble_score'] >= self.anomaly_threshold]\n",
        "        else:\n",
        "            # Fall back to anomaly_score or just count all anomalies\n",
        "            score_col = next((col for col in anomalies.columns if 'score' in col.lower()), None)\n",
        "            if score_col:\n",
        "                high_score_anomalies = anomalies[anomalies[score_col] >= self.anomaly_threshold]\n",
        "            else:\n",
        "                high_score_anomalies = anomalies\n",
        "\n",
        "        if len(high_score_anomalies) > 0:\n",
        "            # Get the highest scoring anomaly\n",
        "            if 'ensemble_score' in high_score_anomalies.columns:\n",
        "                highest_anomaly = high_score_anomalies.loc[high_score_anomalies['ensemble_score'].idxmax()]\n",
        "                score = highest_anomaly['ensemble_score']\n",
        "            elif score_col:\n",
        "                highest_anomaly = high_score_anomalies.loc[high_score_anomalies[score_col].idxmax()]\n",
        "                score = highest_anomaly[score_col]\n",
        "            else:\n",
        "                highest_anomaly = high_score_anomalies.iloc[0]\n",
        "                score = 1.0\n",
        "\n",
        "            # Send alert\n",
        "            self._send_alert(highest_anomaly, score, device_type)\n",
        "\n",
        "            # Set cooldown\n",
        "            self.alert_cooldown = now + datetime.timedelta(hours=self.alert_cooldown_hours)\n",
        "            logger.info(f\"Alert cooldown set until {self.alert_cooldown}\")\n",
        "\n",
        "    def _send_alert(self, anomaly_record, score, device_type):\n",
        "        \"\"\"\n",
        "        Send an alert about the anomaly\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        anomaly_record : Series\n",
        "            Record with the anomaly data\n",
        "        score : float\n",
        "            Anomaly score\n",
        "        device_type : str\n",
        "            'BMC' or 'Host'\n",
        "        \"\"\"\n",
        "        # Format timestamp\n",
        "        if '_time' in anomaly_record:\n",
        "            timestamp = anomaly_record['_time']\n",
        "            if isinstance(timestamp, str):\n",
        "                try:\n",
        "                    timestamp = pd.to_datetime(timestamp)\n",
        "                except:\n",
        "                    timestamp = \"Unknown\"\n",
        "        else:\n",
        "            timestamp = datetime.datetime.now()\n",
        "\n",
        "        # Get device ID\n",
        "        device_id = anomaly_record.get('device_id', 'Unknown')\n",
        "\n",
        "        # Create alert message\n",
        "        alert_message = f\"HARDWARE ANOMALY DETECTED\\n\"\n",
        "        alert_message += f\"Time: {timestamp}\\n\"\n",
        "        alert_message += f\"Device Type: {device_type}\\n\"\n",
        "        alert_message += f\"Device ID: {device_id}\\n\"\n",
        "        alert_message += f\"Anomaly Score: {score:.4f}\\n\"\n",
        "        alert_message += f\"\\nKey Metrics:\\n\"\n",
        "\n",
        "        # Add key metrics based on device type\n",
        "        if device_type == 'BMC':\n",
        "            key_metrics = [\n",
        "                'Cpu1_Temp', 'Cpu2_Temp', 'FAN1', 'FAN2', 'FAN3', 'FAN4',\n",
        "                'PSU1_Total_Power', 'PSU2_Total_Power'\n",
        "            ]\n",
        "        else:  # Host\n",
        "            key_metrics = [\n",
        "                'cpu_usage', 'mem_usage', 'load_1', 'load_5', 'load_15',\n",
        "                'cache_misses', 'L1_dcache_load_misses'\n",
        "            ]\n",
        "\n",
        "        for metric in key_metrics:\n",
        "            if metric in anomaly_record:\n",
        "                alert_message += f\"  {metric}: {anomaly_record[metric]}\\n\"\n",
        "\n",
        "        # Log the alert\n",
        "        logger.warning(f\"ALERT: Hardware anomaly detected in {device_type} device {device_id}, score: {score:.4f}\")\n",
        "\n",
        "        # Send email alert if configured\n",
        "        if self.config.get('email_alerts', False):\n",
        "            try:\n",
        "                email_config = self.config.get('email_config', {})\n",
        "                smtp_server = email_config.get('smtp_server', '')\n",
        "                smtp_port = email_config.get('smtp_port', 587)\n",
        "                username = email_config.get('username', '')\n",
        "                password = email_config.get('password', '')\n",
        "                from_address = email_config.get('from_address', '')\n",
        "                to_addresses = email_config.get('to_addresses', [])\n",
        "\n",
        "                if not smtp_server or not username or not from_address or not to_addresses:\n",
        "                    logger.error(\"Incomplete email configuration\")\n",
        "                    return\n",
        "\n",
        "                # Create email\n",
        "                msg = MIMEMultipart()\n",
        "                msg['From'] = from_address\n",
        "                msg['To'] = ', '.join(to_addresses)\n",
        "                msg['Subject'] = f\"HARDWARE ANOMALY ALERT: {device_type} {device_id}\"\n",
        "\n",
        "                msg.attach(MIMEText(alert_message, 'plain'))\n",
        "\n",
        "                # Send email\n",
        "                with smtplib.SMTP(smtp_server, smtp_port) as server:\n",
        "                    server.starttls()\n",
        "                    server.login(username, password)\n",
        "                    server.send_message(msg)\n",
        "\n",
        "                logger.info(f\"Sent email alert to {to_addresses}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error sending email alert: {e}\")\n",
        "\n",
        "        # Create visualization if configured\n",
        "        if self.config.get('visualization', True):\n",
        "            self._create_anomaly_visualization(device_type)\n",
        "\n",
        "    def _create_anomaly_visualization(self, device_type):\n",
        "        \"\"\"\n",
        "        Create visualization of recent anomalies\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        device_type : str\n",
        "            'BMC' or 'Host'\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Filter anomaly history for this device type\n",
        "            device_anomalies = [a for a in self.anomaly_history if a['device_type'] == device_type]\n",
        "\n",
        "            if not device_anomalies:\n",
        "                logger.info(f\"No {device_type} anomalies for visualization\")\n",
        "                return\n",
        "\n",
        "            # Create dataframe from anomaly history\n",
        "            anomaly_df = pd.DataFrame(device_anomalies)\n",
        "\n",
        "            # Get timestamps\n",
        "            timestamps = [a['time'] for a in device_anomalies]\n",
        "\n",
        "            # Get scores\n",
        "            scores = [a['score'] for a in device_anomalies]\n",
        "\n",
        "            # Create plot\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.plot(timestamps, scores, 'r.-')\n",
        "            plt.axhline(y=self.anomaly_threshold, color='g', linestyle='--', label=f'Threshold ({self.anomaly_threshold})')\n",
        "            plt.title(f'{device_type} Anomaly Scores Over Time')\n",
        "            plt.xlabel('Time')\n",
        "            plt.ylabel('Anomaly Score')\n",
        "            plt.legend()\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save plot\n",
        "            plt.savefig(f'{device_type}_anomalies.png')\n",
        "            plt.close()\n",
        "\n",
        "            logger.info(f\"Created {device_type} anomaly visualization\")\n",
        "\n",
        "            # Create additional visualizations for BMC data\n",
        "            if device_type == 'BMC' and len(device_anomalies) >= 5:\n",
        "                self._create_bmc_visualizations(device_anomalies)\n",
        "\n",
        "            # Create additional visualizations for host data\n",
        "            if device_type == 'Host' and len(device_anomalies) >= 5:\n",
        "                self._create_host_visualizations(device_anomalies)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating visualization: {e}\")\n",
        "\n",
        "    def _create_bmc_visualizations(self, anomalies):\n",
        "        \"\"\"Create BMC-specific visualizations\"\"\"\n",
        "        try:\n",
        "            # Extract key metrics from anomalies\n",
        "            key_metrics = [\n",
        "                'Cpu1_Temp', 'Cpu2_Temp', 'FAN1', 'FAN2', 'FAN3', 'FAN4',\n",
        "                'PSU1_Total_Power', 'PSU2_Total_Power'\n",
        "            ]\n",
        "\n",
        "            # Create dataframe with timestamps and metrics\n",
        "            data = []\n",
        "            for anomaly in anomalies:\n",
        "                record = {'time': anomaly['time']}\n",
        "                for metric in key_metrics:\n",
        "                    if metric in anomaly['data']:\n",
        "                        record[metric] = anomaly['data'][metric]\n",
        "                data.append(record)\n",
        "\n",
        "            df = pd.DataFrame(data)\n",
        "\n",
        "            # Plot temperatures\n",
        "            temp_cols = [col for col in df.columns if 'Temp' in col]\n",
        "            if temp_cols:\n",
        "                plt.figure(figsize=(12, 6))\n",
        "                for col in temp_cols:\n",
        "                    if col in df.columns:\n",
        "                        plt.plot(df['time'], df[col], '.-', label=col)\n",
        "                plt.title('BMC Temperature Anomalies')\n",
        "                plt.xlabel('Time')\n",
        "                plt.ylabel('Temperature')\n",
        "                plt.legend()\n",
        "                plt.tight_layout()\n",
        "                plt.savefig('BMC_temperature_anomalies.png')\n",
        "                plt.close()\n",
        "\n",
        "            # Plot fan speeds\n",
        "            fan_cols = [col for col in df.columns if 'FAN' in col]\n",
        "            if fan_cols:\n",
        "                plt.figure(figsize=(12, 6))\n",
        "                for col in fan_cols:\n",
        "                    if col in df.columns:\n",
        "                        plt.plot(df['time'], df[col], '.-', label=col)\n",
        "                plt.title('BMC Fan Speed Anomalies')\n",
        "                plt.xlabel('Time')\n",
        "                plt.ylabel('Fan Speed')\n",
        "                plt.legend()\n",
        "                plt.tight_layout()\n",
        "                plt.savefig('BMC_fan_anomalies.png')\n",
        "                plt.close()\n",
        "\n",
        "            # Plot power\n",
        "            power_cols = [col for col in df.columns if 'Power' in col]\n",
        "            if power_cols:\n",
        "                plt.figure(figsize=(12, 6))\n",
        "                for col in power_cols:\n",
        "                    if col in df.columns:\n",
        "                        plt.plot(df['time'], df[col], '.-', label=col)\n",
        "                plt.title('BMC Power Anomalies')\n",
        "                plt.xlabel('Time')\n",
        "                plt.ylabel('Power')\n",
        "                plt.legend()\n",
        "                plt.tight_layout()\n",
        "                plt.savefig('BMC_power_anomalies.png')\n",
        "                plt.close()\n",
        "\n",
        "            logger.info(\"Created BMC-specific visualizations\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating BMC visualizations: {e}\")\n",
        "\n",
        "    def _create_host_visualizations(self, anomalies):\n",
        "        \"\"\"Create host-specific visualizations\"\"\"\n",
        "        try:\n",
        "            # Extract key metrics from anomalies\n",
        "            key_metrics = [\n",
        "                'cpu_usage', 'mem_usage', 'load_1', 'load_5', 'load_15',\n",
        "                'cache_misses', 'L1_dcache_load_misses'\n",
        "            ]\n",
        "\n",
        "            # Create dataframe with timestamps and metrics\n",
        "            data = []\n",
        "            for anomaly in anomalies:\n",
        "                record = {'time': anomaly['time']}\n",
        "                for metric in key_metrics:\n",
        "                    if metric in anomaly['data']:\n",
        "                        record[metric] = anomaly['data'][metric]\n",
        "                data.append(record)\n",
        "\n",
        "            df = pd.DataFrame(data)\n",
        "\n",
        "            # Plot CPU and memory usage\n",
        "            usage_cols = [col for col in df.columns if 'usage' in col]\n",
        "            if usage_cols:\n",
        "                plt.figure(figsize=(12, 6))\n",
        "                for col in usage_cols:\n",
        "                    if col in df.columns:\n",
        "                        plt.plot(df['time'], df[col], '.-', label=col)\n",
        "                plt.title('Host Usage Anomalies')\n",
        "                plt.xlabel('Time')\n",
        "                plt.ylabel('Usage (%)')\n",
        "                plt.legend()\n",
        "                plt.tight_layout()\n",
        "                plt.savefig('Host_usage_anomalies.png')\n",
        "                plt.close()\n",
        "\n",
        "            # Plot load averages\n",
        "            load_cols = [col for col in df.columns if 'load_' in col]\n",
        "            if load_cols:\n",
        "                plt.figure(figsize=(12, 6))\n",
        "                for col in load_cols:\n",
        "                    if col in df.columns:\n",
        "                        plt.plot(df['time'], df[col], '.-', label=col)\n",
        "                plt.title('Host Load Anomalies')\n",
        "                plt.xlabel('Time')\n",
        "                plt.ylabel('Load Average')\n",
        "                plt.legend()\n",
        "                plt.tight_layout()\n",
        "                plt.savefig('Host_load_anomalies.png')\n",
        "                plt.close()\n",
        "\n",
        "            # Plot cache metrics\n",
        "            cache_cols = [col for col in df.columns if 'cache' in col]\n",
        "            if cache_cols:\n",
        "                plt.figure(figsize=(12, 6))\n",
        "                for col in cache_cols:\n",
        "                    if col in df.columns:\n",
        "                        plt.plot(df['time'], df[col], '.-', label=col)\n",
        "                plt.title('Host Cache Anomalies')\n",
        "                plt.xlabel('Time')\n",
        "                plt.ylabel('Cache Metrics')\n",
        "                plt.legend()\n",
        "                plt.tight_layout()\n",
        "                plt.savefig('Host_cache_anomalies.png')\n",
        "                plt.close()\n",
        "\n",
        "            logger.info(\"Created host-specific visualizations\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating host visualizations: {e}\")\n",
        "\n",
        "    def run_once(self):\n",
        "        \"\"\"Run one iteration of the monitoring loop\"\"\"\n",
        "        # Check for new BMC data\n",
        "        bmc_data = self._load_new_data('BMC')\n",
        "        if bmc_data is not None:\n",
        "            self._detect_anomalies(bmc_data, 'BMC')\n",
        "\n",
        "        # Check for new host data\n",
        "        host_data = self._load_new_data('Host')\n",
        "        if host_data is not None:\n",
        "            self._detect_anomalies(host_data, 'Host')\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Run the monitoring loop continuously\"\"\"\n",
        "        logger.info(\"Starting hardware anomaly monitoring loop\")\n",
        "\n",
        "        try:\n",
        "            while True:\n",
        "                self.run_once()\n",
        "\n",
        "                # Sleep until next check\n",
        "                interval = self.config.get('check_interval_seconds', 60)\n",
        "                logger.info(f\"Sleeping for {interval} seconds until next check\")\n",
        "                time.sleep(interval)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            logger.info(\"Monitoring stopped by user\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in monitoring loop: {e}\")\n",
        "            raise\n",
        "        finally:\n",
        "            logger.info(\"Hardware anomaly monitoring stopped\")\n",
        "\n",
        "    def get_anomaly_summary(self):\n",
        "        \"\"\"Get summary of recent anomalies\"\"\"\n",
        "        if not self.anomaly_history:\n",
        "            return \"No anomalies detected yet\"\n",
        "\n",
        "        summary = f\"Recent Anomalies (last {len(self.anomaly_history)}):\\n\"\n",
        "\n",
        "        # Group by device type\n",
        "        bmc_anomalies = [a for a in self.anomaly_history if a['device_type'] == 'BMC']\n",
        "        host_anomalies = [a for a in self.anomaly_history if a['device_type'] == 'Host']\n",
        "\n",
        "        summary += f\"BMC Anomalies: {len(bmc_anomalies)}\\n\"\n",
        "        summary += f\"Host Anomalies: {len(host_anomalies)}\\n\\n\"\n",
        "\n",
        "        # Add most recent anomalies\n",
        "        recent = sorted(self.anomaly_history, key=lambda x: x['time'] if isinstance(x['time'], datetime.datetime) else pd.to_datetime(x['time']), reverse=True)\n",
        "\n",
        "        for i, anomaly in enumerate(recent[:5]):\n",
        "            if i > 0:\n",
        "                summary += \"\\n\"\n",
        "            summary += f\"Anomaly {i+1}:\\n\"\n",
        "            summary += f\"  Time: {anomaly['time']}\\n\"\n",
        "            summary += f\"  Device: {anomaly['device_type']} {anomaly['device_id']}\\n\"\n",
        "            summary += f\"  Score: {anomaly['score']:.4f}\\n\"\n",
        "\n",
        "        return summary\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create and run the monitor\n",
        "    monitor = HardwareAnomalyMonitor()\n",
        "\n",
        "    # Print initial information\n",
        "    print(\"\\nHardware Anomaly Monitor\")\n",
        "    print(\"------------------------\")\n",
        "    print(f\"BMC Data Path: {monitor.config.get('bmc_data_path')}\")\n",
        "    print(f\"Host Data Path: {monitor.config.get('host_data_path')}\")\n",
        "    print(f\"Anomaly Threshold: {monitor.anomaly_threshold}\")\n",
        "    print(f\"Check Interval: {monitor.config.get('check_interval_seconds')} seconds\")\n",
        "    print(f\"Alert Cooldown: {monitor.alert_cooldown_hours} hours\")\n",
        "    print(f\"Email Alerts: {'Enabled' if monitor.config.get('email_alerts', False) else 'Disabled'}\")\n",
        "    print(\"------------------------\")\n",
        "    print(\"Press Ctrl+C to stop monitoring\")\n",
        "    print(\"------------------------\\n\")\n",
        "\n",
        "    # Run the monitor\n",
        "    monitor.run()"
      ],
      "metadata": {
        "id": "dZpYy08v71-1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "ebde495a-13ef-48fe-8941-73af77458316"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'detect_anomalies_in_new_data' from 'anomaly_detector' (/content/anomaly_detector.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e287c071dfaf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0manomaly_detector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetect_anomalies_in_new_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Set up logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'detect_anomalies_in_new_data' from 'anomaly_detector' (/content/anomaly_detector.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}